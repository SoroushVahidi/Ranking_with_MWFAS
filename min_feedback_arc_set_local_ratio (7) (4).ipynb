{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60769118-7444-47ac-8af5-4dd96e7fbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_graph(file_path):\n",
    "    \"\"\"\n",
    "    Reads a weighted directed graph from a file. Each line contains three values:\n",
    "    start vertex, end vertex, and edge weight.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing the graph.\n",
    "\n",
    "    Returns:\n",
    "        edges (list): List of tuples representing directed edges (start, end, weight, edge_id).\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    edge_id = 0  # Unique identifier for each edge\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line:  # Skip empty lines\n",
    "                    start, end, weight = map(float, line.split())\n",
    "                    edges.append((int(start), int(end), weight, edge_id))\n",
    "                    edge_id += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading graph: {e}\")\n",
    "    return edges\n",
    "\n",
    "\n",
    "def initialize_graph(edges):\n",
    "    \"\"\"\n",
    "    Converts edge list to an adjacency list and weights dictionary.\n",
    "    Handles parallel edges by including edge_id as part of the structure.\n",
    "\n",
    "    Args:\n",
    "        edges (list): List of edges as (start, end, weight, edge_id).\n",
    "\n",
    "    Returns:\n",
    "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
    "        weights (dict): Dictionary mapping (start, end, edge_id) to weights.\n",
    "    \"\"\"\n",
    "    graph = defaultdict(list)\n",
    "    weights = {}\n",
    "    for u, v, w, edge_id in edges:\n",
    "        graph[u].append((v, edge_id))\n",
    "        weights[(u, v, edge_id)] = w\n",
    "    return graph, weights\n",
    "\n",
    "\n",
    "def find_cycles_and_reduce(graph, weights, n):\n",
    "    \"\"\"\n",
    "    Phase 1: Find cycles and reduce weights using a copy.\n",
    "    Handles graphs with parallel edges by considering edge identifiers.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
    "        weights (dict): Dictionary mapping (u, v, edge_id) to weights.\n",
    "        n (int): Number of vertices in the graph.\n",
    "\n",
    "    Returns:\n",
    "        removed_edges (set): Set of removed edges as (u, v, edge_id).\n",
    "        removed_weights (dict): Dictionary of removed edges with their original weights.\n",
    "    \"\"\"\n",
    "    weights_copy = weights.copy()  # Work with a copy of weights\n",
    "    removed_edges = set()\n",
    "    removed_weights = {}\n",
    "\n",
    "    while True:\n",
    "        cycle = find_cycle(graph, n)  # Modified `find_cycle` returns edges with edge_id\n",
    "        if not cycle:  # No cycle found\n",
    "            break\n",
    "\n",
    "        # Ensure all edges in the cycle exist in the weights dictionary\n",
    "        cycle = [(u, v, edge_id) for u, v, edge_id in cycle if (u, v, edge_id) in weights_copy]\n",
    "\n",
    "        if not cycle:  # If no valid cycle exists, continue\n",
    "            continue\n",
    "\n",
    "        # Find the minimum weight in the cycle\n",
    "        min_weight = min(weights_copy[(u, v, edge_id)] for u, v, edge_id in cycle)\n",
    "\n",
    "        for u, v, edge_id in cycle:\n",
    "            weights_copy[(u, v, edge_id)] -= min_weight\n",
    "            if weights_copy[(u, v, edge_id)] <= 0:\n",
    "                # Ensure the edge is in the graph before removing\n",
    "                if (v, edge_id) in graph[u]:\n",
    "                    graph[u].remove((v, edge_id))\n",
    "                    removed_edges.add((u, v, edge_id))\n",
    "                    removed_weights[(u, v, edge_id)] = weights[(u, v, edge_id)]\n",
    "\n",
    "    return removed_edges, removed_weights\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def find_cycle(graph, n):\n",
    "    \"\"\"\n",
    "    Detect a cycle in the graph using DFS and return the cycle as a list of edges.\n",
    "    Handles parallel edges and cycles of length 2 caused by reverse edges.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
    "        n (int): Number of vertices in the graph.\n",
    "\n",
    "    Returns:\n",
    "        cycle (list): List of edges forming the cycle, or None if no cycle is found.\n",
    "    \"\"\"\n",
    "    visited = [False] * n\n",
    "    stack = [False] * n\n",
    "    parent = [-1] * n\n",
    "    edge_to_parent = {}  # Map to track edge_id for cycle reconstruction\n",
    "\n",
    "    def dfs(v):\n",
    "        visited[v] = True\n",
    "        stack[v] = True\n",
    "        for neighbor, edge_id in graph[v]:\n",
    "            if not visited[neighbor]:\n",
    "                parent[neighbor] = v\n",
    "                edge_to_parent[neighbor] = edge_id\n",
    "                cycle = dfs(neighbor)\n",
    "                if cycle:\n",
    "                    return cycle\n",
    "            elif stack[neighbor]:\n",
    "                # Found a cycle, reconstruct it\n",
    "                cycle = []\n",
    "                current = v\n",
    "                while current != neighbor:\n",
    "                    if current not in edge_to_parent:\n",
    "                        break  # Avoid KeyError if edge metadata is missing\n",
    "                    cycle.append((parent[current], current, edge_to_parent[current]))\n",
    "                    current = parent[current]\n",
    "\n",
    "                # Handle the root of the cycle\n",
    "                if neighbor in edge_to_parent and parent[neighbor] != -1:\n",
    "                    cycle.append((parent[neighbor], neighbor, edge_to_parent[neighbor]))\n",
    "                return cycle\n",
    "\n",
    "        stack[v] = False\n",
    "        return None\n",
    "\n",
    "    # Detect length-2 cycles caused by reverse edges\n",
    "    for u in list(graph):  # Use list(graph) to iterate over a static copy of keys\n",
    "        for neighbor, edge_id1 in graph[u]:\n",
    "            for neighbor_of_neighbor, edge_id2 in graph[neighbor]:\n",
    "                if neighbor_of_neighbor == u and edge_id1 != edge_id2:\n",
    "                    # Found a length-2 cycle\n",
    "                    return [(u, neighbor, edge_id1), (neighbor, u, edge_id2)]\n",
    "\n",
    "    # Run DFS for longer cycles\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            cycle = dfs(i)\n",
    "            if cycle:\n",
    "                return cycle\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def find_minimum_weight_cycle(graph, weights, n):\n",
    "    \"\"\"\n",
    "    Find the minimum weight cycle in the graph using Floyd-Warshall.\n",
    "\n",
    "    :param graph: Adjacency list representation of the graph.\n",
    "    :param weights: Dictionary of edge weights.\n",
    "    :param n: Total number of vertices in the graph.\n",
    "    :return: List of edges representing the minimum weight cycle.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize distance and predecessor matrices\n",
    "    dist = np.full((n, n), float('inf'))\n",
    "    pred = [[-1 for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "    # Fill in the distances based on edge weights\n",
    "    for u in range(n):\n",
    "        dist[u][u] = 0\n",
    "        if u in graph:\n",
    "            for v in graph[u]:\n",
    "                dist[u][v] = weights.get((u, v), float('inf'))\n",
    "                pred[u][v] = u\n",
    "\n",
    "    # Step 2: Run Floyd-Warshall algorithm\n",
    "    for k in range(n):\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if dist[i][j] > dist[i][k] + dist[k][j]:\n",
    "                    dist[i][j] = dist[i][k] + dist[k][j]\n",
    "                    pred[i][j] = pred[k][j]\n",
    "\n",
    "    # Step 3: Find the minimum weight cycle\n",
    "    min_cycle_weight = float('inf')\n",
    "    cycle = []\n",
    "\n",
    "    for u in range(n):\n",
    "        for v in range(n):\n",
    "            if u != v and dist[u][v] < float('inf') and dist[v][u] < float('inf'):\n",
    "                cycle_weight = dist[u][v] + dist[v][u]\n",
    "                if cycle_weight < min_cycle_weight:\n",
    "                    min_cycle_weight = cycle_weight\n",
    "                    # Reconstruct the cycle\n",
    "                    cycle = []\n",
    "                    # Trace path from u to v\n",
    "                    current = v\n",
    "                    while current != u:\n",
    "                        cycle.append((pred[u][current], current))\n",
    "                        current = pred[u][current]\n",
    "                    # Trace path from v back to u\n",
    "                    current = u\n",
    "                    while current != v:\n",
    "                        cycle.append((pred[v][current], current))\n",
    "                        current = pred[v][current]\n",
    "\n",
    "    # Return the minimum weight cycle if found\n",
    "    if min_cycle_weight == float('inf'):\n",
    "        return None\n",
    "    else:\n",
    "        return cycle\n",
    "\n",
    "\n",
    "def check_and_readd_edges(graph, removed_edges, n):\n",
    "    \"\"\"\n",
    "    Phase 2: Check and re-add edges if they do not create a cycle.\n",
    "    Handles graphs with parallel edges using edge identifiers.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
    "        removed_edges (set): Set of removed edges as (u, v, edge_id).\n",
    "        n (int): Number of vertices in the graph.\n",
    "\n",
    "    Returns:\n",
    "        readded_edges (set): Set of edges that were successfully re-added.\n",
    "        remaining_removed_edges (set): Set of edges that could not be re-added.\n",
    "    \"\"\"\n",
    "    \n",
    "    def has_path(start, end, graph):\n",
    "        \"\"\"\n",
    "        Helper function to check if there is a path from start to end using DFS.\n",
    "        Avoids cycles when re-adding edges.\n",
    "        \"\"\"\n",
    "        visited = [False] * n\n",
    "        stack = [start]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            if node == end:\n",
    "                return True\n",
    "            if not visited[node]:\n",
    "                visited[node] = True\n",
    "                stack.extend(neighbor for neighbor, _ in graph[node])  # Add only neighbors\n",
    "        return False\n",
    "\n",
    "    readded_edges = set()\n",
    "    removed_edges_list = sorted(list(removed_edges), reverse=True)\n",
    "\n",
    "    for u, v, edge_id in removed_edges_list:\n",
    "        if not has_path(v, u, graph):  # Only re-add if it doesn't create a cycle\n",
    "            graph[u].append((v, edge_id))\n",
    "            readded_edges.add((u, v, edge_id))\n",
    "\n",
    "    remaining_removed_edges = removed_edges - readded_edges\n",
    "    return readded_edges, remaining_removed_edges\n",
    "\n",
    "def mwfas(file_path):\n",
    "    \"\"\"\n",
    "    Main function to find Minimum Weighted Feedback Arc Set (MWFAS) in a graph with parallel edges.\n",
    "    \n",
    "    :param file_path: Path to the file containing the graph.\n",
    "    :return: A dictionary with metrics, updated graph, removed edges, and their weights.\n",
    "    \"\"\"\n",
    "    # Read the graph and initialize its structure\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    graph, weights = initialize_graph(edges)\n",
    "\n",
    "    # Original graph statistics\n",
    "    total_edges = len(edges)\n",
    "    total_weight = sum(w for _, _, w, _ in edges)\n",
    "\n",
    "    # Phase 1: Reduce cycles\n",
    "    removed_edges, removed_weights = find_cycles_and_reduce(graph, weights, n)\n",
    "\n",
    "    # Phase 2: Re-add edges (if applicable)\n",
    "    readded_edges, remaining_removed_edges = check_and_readd_edges(graph, removed_edges, n)\n",
    "\n",
    "    # Compute final metrics\n",
    "    num_removed_edges = len(remaining_removed_edges)\n",
    "    total_removed_weight = sum(removed_weights.get(edge, 0) for edge in remaining_removed_edges)\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        \"total_edges\": total_edges,\n",
    "        \"total_weight\": total_weight,\n",
    "        \"num_removed_edges\": num_removed_edges,\n",
    "        \"removed_weight\": total_removed_weight,\n",
    "        \"final_graph\": graph,\n",
    "        \"removed_edges\": remaining_removed_edges,\n",
    "        \"removed_weights\": {edge: removed_weights.get(edge, 0) for edge in remaining_removed_edges},\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff99c0-7497-4c1b-b8a0-261ae33cbcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830eebee-e905-4ffa-958d-bc4f22a7a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "class DirectedGraph:\n",
    "    def __init__(self, vertices):\n",
    "        self.graph = defaultdict(list)\n",
    "        self.vertices = vertices\n",
    "\n",
    "    def add_edge(self, src, dest, weight=1):\n",
    "        self.graph[src].append((dest, weight))\n",
    "\n",
    "    def get_edges(self):\n",
    "        edges = []\n",
    "        for src in self.graph:\n",
    "            for dest, weight in self.graph[src]:\n",
    "                edges.append((src, dest, weight))\n",
    "        return edges\n",
    "\n",
    "    def remove_edge(self, src, dest):\n",
    "        self.graph[src] = [(d, w) for d, w in self.graph[src] if d != dest]\n",
    "\n",
    "    def get_indegree(self):\n",
    "        indegree = {v: 0 for v in range(self.vertices)}\n",
    "        for src in self.graph:\n",
    "            for dest, _ in self.graph[src]:\n",
    "                indegree[dest] += 1\n",
    "        return indegree\n",
    "\n",
    "    def get_outdegree(self):\n",
    "        outdegree = {v: 0 for v in range(self.vertices)}\n",
    "        for src in self.graph:\n",
    "            for dest, _ in self.graph[src]:\n",
    "                outdegree[src] += 1\n",
    "        return outdegree\n",
    "\n",
    "    def eliminate_parallel_arcs(self):\n",
    "        for src in self.graph:\n",
    "            seen = {}\n",
    "            for dest, weight in self.graph[src]:\n",
    "                if dest in seen:\n",
    "                    seen[dest] += weight\n",
    "                else:\n",
    "                    seen[dest] = weight\n",
    "            self.graph[src] = [(dest, weight) for dest, weight in seen.items()]\n",
    "\n",
    "    def eliminate_two_cycles(self):\n",
    "        for src in list(self.graph.keys()):\n",
    "            for dest, weight in self.graph[src]:\n",
    "                for back_dest, back_weight in self.graph[dest]:\n",
    "                    if back_dest == src:\n",
    "                        if weight > back_weight:\n",
    "                            self.graph[src] = [(d, w) for d, w in self.graph[src] if d != dest]\n",
    "                            self.graph[dest] = [(d, w) for d, w in self.graph[dest] if d != src]\n",
    "                        elif weight == back_weight:\n",
    "                            self.graph[src] = [(d, w) for d, w in self.graph[src] if d != dest]\n",
    "                            self.graph[dest] = [(d, w) for d, w in self.graph[dest] if d != src]\n",
    "\n",
    "    def strongly_connected_components(self):\n",
    "        index = 0\n",
    "        stack = []\n",
    "        indices = [-1] * self.vertices\n",
    "        lowlinks = [-1] * self.vertices\n",
    "        on_stack = [False] * self.vertices\n",
    "        sccs = []\n",
    "\n",
    "        def strongconnect(v):\n",
    "            nonlocal index\n",
    "            indices[v] = index\n",
    "            lowlinks[v] = index\n",
    "            index += 1\n",
    "            stack.append(v)\n",
    "            on_stack[v] = True\n",
    "\n",
    "            for neighbor, _ in self.graph[v]:\n",
    "                if indices[neighbor] == -1:\n",
    "                    strongconnect(neighbor)\n",
    "                    lowlinks[v] = min(lowlinks[v], lowlinks[neighbor])\n",
    "                elif on_stack[neighbor]:\n",
    "                    lowlinks[v] = min(lowlinks[v], indices[neighbor])\n",
    "\n",
    "            if lowlinks[v] == indices[v]:\n",
    "                scc = []\n",
    "                while True:\n",
    "                    node = stack.pop()\n",
    "                    on_stack[node] = False\n",
    "                    scc.append(node)\n",
    "                    if node == v:\n",
    "                        break\n",
    "                sccs.append(scc)\n",
    "\n",
    "        for v in range(self.vertices):\n",
    "            if indices[v] == -1:\n",
    "                strongconnect(v)\n",
    "\n",
    "        return sccs\n",
    "\n",
    "class HCSReduction:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "\n",
    "    def reduce(self):\n",
    "        self.graph.eliminate_parallel_arcs()\n",
    "        self.graph.eliminate_two_cycles()\n",
    "        sccs = self.graph.strongly_connected_components()\n",
    "        for scc in sccs:\n",
    "            if len(scc) > 1:\n",
    "                self.contract_scc(scc)\n",
    "\n",
    "    def contract_scc(self, scc):\n",
    "        super_node = scc[0]\n",
    "        for node in scc[1:]:\n",
    "            for neighbor, weight in self.graph.graph[node]:\n",
    "                self.graph.add_edge(super_node, neighbor, weight)\n",
    "            self.graph.graph.pop(node, None)\n",
    "\n",
    "class GreedyABS:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "\n",
    "    def solve(self):\n",
    "        indegree = self.graph.get_indegree()\n",
    "        outdegree = self.graph.get_outdegree()\n",
    "\n",
    "        sources = deque([v for v in indegree if indegree[v] == 0])\n",
    "        sinks = deque([v for v in outdegree if outdegree[v] == 0])\n",
    "\n",
    "        ordering = []\n",
    "\n",
    "        while sources or sinks:\n",
    "            while sources:\n",
    "                v = sources.popleft()\n",
    "                ordering.append(v)\n",
    "                for neighbor, _ in self.graph.graph[v]:\n",
    "                    indegree[neighbor] -= 1\n",
    "                    if indegree[neighbor] == 0:\n",
    "                        sources.append(neighbor)\n",
    "\n",
    "            while sinks:\n",
    "                v = sinks.popleft()\n",
    "                ordering.append(v)\n",
    "                for src in list(self.graph.graph.keys()):\n",
    "                    self.graph.graph[src] = [(d, w) for d, w in self.graph.graph[src] if d != v]\n",
    "                    if not self.graph.graph[src]:\n",
    "                        sinks.append(src)\n",
    "\n",
    "        if len(ordering) < self.graph.vertices:\n",
    "            remaining = [v for v in range(self.graph.vertices) if v not in ordering]\n",
    "            ordering.extend(remaining)\n",
    "\n",
    "        return ordering\n",
    "\n",
    "def mwfas2(file_path):\n",
    "    \"\"\"\n",
    "    Main function to find Minimum Weighted Feedback Arc Set (MWFAS) in a graph with parallel edges.\n",
    "\n",
    "    :param file_path: Path to the file containing the graph.\n",
    "    :return: A dictionary with metrics, updated graph, removed edges, and their weights.\n",
    "    \"\"\"\n",
    "    # Step 1: Read edges from file\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "\n",
    "    # Step 2: Construct DirectedGraph as adjacency list with edge IDs\n",
    "    graph = defaultdict(list)\n",
    "    weights = {}\n",
    "    for u, v, weight, edge_id in edges:\n",
    "        edge_key = (u, v, edge_id)\n",
    "        graph[u].append((v, edge_id))\n",
    "        weights[edge_key] = weight\n",
    "\n",
    "    print(\"Graph loaded successfully.\")\n",
    "    print(\"Weights dictionary:\", weights)\n",
    "\n",
    "    # Original graph statistics\n",
    "    total_edges = len(edges)\n",
    "    total_weight = sum(weight for _, _, weight, _ in edges)\n",
    "\n",
    "    # Phase 1: Compute topological ordering dynamically\n",
    "    in_degree = {i: 0 for i in range(n)}\n",
    "    for u in graph:\n",
    "        for v, _ in graph[u]:\n",
    "            in_degree[v] += 1\n",
    "\n",
    "    queue = deque([node for node in range(n) if in_degree[node] == 0])\n",
    "    ordering = []\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        ordering.append(node)\n",
    "        for neighbor, _ in graph[node]:\n",
    "            in_degree[neighbor] -= 1\n",
    "            if in_degree[neighbor] == 0:\n",
    "                queue.append(neighbor)\n",
    "\n",
    "    # Ensure all nodes are in the ordering\n",
    "    if len(ordering) < n:\n",
    "        print(\"Warning: Graph contains cycles or disconnected components.\")\n",
    "        for i in range(n):\n",
    "            if i not in ordering:\n",
    "                ordering.append(i)\n",
    "\n",
    "    # Phase 2: Compute removed edges (those not respecting the ordering)\n",
    "    # Phase 2: Compute removed edges (those not respecting the ordering)\n",
    "    removed_edges = []\n",
    "    removed_weights = {}\n",
    "    \n",
    "    for u in graph:\n",
    "        for v, edge_id in graph[u]:\n",
    "            edge_key = (u, v, edge_id)\n",
    "            if ordering.index(u) > ordering.index(v):\n",
    "                removed_edges.append(edge_key)\n",
    "                if edge_key in weights:\n",
    "                    removed_weights[edge_key] = weights[edge_key]\n",
    "                else:\n",
    "                    print(f\"Warning: Weight missing for edge {edge_key}. Defaulting to 0.\")\n",
    "                    removed_weights[edge_key] = 0\n",
    "    \n",
    "    # Remove invalid edges from graph\n",
    "    for u, v, edge_id in removed_edges:\n",
    "        graph[u] = [(neighbor, eid) for neighbor, eid in graph[u] if not (neighbor == v and eid == edge_id)]\n",
    "    \n",
    "    # Synchronize weights with the modified graph\n",
    "    updated_weights = {\n",
    "        edge_key: weights[edge_key]\n",
    "        for u in graph\n",
    "        for v, edge_id in graph[u]\n",
    "        if (edge_key := (u, v, edge_id)) in weights\n",
    "    }\n",
    "    \n",
    "    weights = updated_weights\n",
    "    num_removed_edges = len(removed_edges)  # Number of edges removed\n",
    "    total_removed_weight = sum(removed_weights.values())  # Total weight of removed edges\n",
    "\n",
    "# Return results\n",
    "    return {\n",
    "    \"total_edges\": total_edges,\n",
    "    \"total_weight\": total_weight,\n",
    "    \"num_removed_edges\": num_removed_edges,\n",
    "    \"removed_weight\": total_removed_weight,\n",
    "    \"final_graph\": graph,\n",
    "    \"removed_edges\": removed_edges,\n",
    "    \"removed_weights\": removed_weights,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb61d0-d814-4891-9faa-c11a20470c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae999ad2-836d-4c7c-a465-48eec352dbba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5efbb96d-66cb-4ff9-a7d9-6e46488f403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vertex_rankings(graph, weights, n):\n",
    "    \"\"\"\n",
    "    Compute rankings for the vertices in a DAG with parallel edges.\n",
    "    :param graph: Adjacency list of the DAG with (neighbor, edge_id) pairs.\n",
    "    :param weights: Dictionary of edge weights with keys (u, v, edge_id).\n",
    "    :param n: Total number of vertices in the graph.\n",
    "    :return: A list of rankings for the vertices.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate in-degrees\n",
    "    in_degree = [0] * n\n",
    "    for u in graph:\n",
    "        for v, _ in graph[u]:  # Ignore edge_id for in-degree calculation\n",
    "            in_degree[v] += 1\n",
    "\n",
    "    # Step 2: Perform topological sort using a min-heap\n",
    "    from heapq import heappop, heappush\n",
    "    min_heap = []\n",
    "    for i in range(n):\n",
    "        if in_degree[i] == 0:\n",
    "            heappush(min_heap, i)\n",
    "\n",
    "    topological_order = []\n",
    "    while min_heap:\n",
    "        current = heappop(min_heap)\n",
    "        topological_order.append(current)\n",
    "        for neighbor, _ in graph[current]:  # Ignore edge_id for topological sort\n",
    "            in_degree[neighbor] -= 1\n",
    "            if in_degree[neighbor] == 0:\n",
    "                heappush(min_heap, neighbor)\n",
    "\n",
    "    # Step 3: Calculate outgoing and incoming edge weight sums for all vertices\n",
    "    outgoing_weights = {v: 0 for v in range(n)}\n",
    "    incoming_weights = {v: 0 for v in range(n)}\n",
    "\n",
    "    for u in graph:\n",
    "        for v, edge_id in graph[u]:\n",
    "            edge_key = (u, v, edge_id)\n",
    "            outgoing_weights[u] += weights.get(edge_key, 0)\n",
    "            incoming_weights[v] += weights.get(edge_key, 0)\n",
    "\n",
    "    # Step 4: Assign rankings\n",
    "    rankings = [-1] * n\n",
    "    current_rank = 0\n",
    "    for vertex in topological_order:\n",
    "        rankings[vertex] = current_rank\n",
    "        current_rank += 1\n",
    "\n",
    "    # Break ties for vertices with the same ranking based on outgoing and incoming edge weights\n",
    "    tied_vertices = sorted(\n",
    "        [(rankings[v], -(outgoing_weights[v] - incoming_weights[v]) / \n",
    "          (outgoing_weights[v] + incoming_weights[v] if outgoing_weights[v] + incoming_weights[v] > 0 else 1), v)\n",
    "         for v in range(n)],\n",
    "        key=lambda x: (x[0], x[1])  # Sort by rank first, then by normalized weight difference\n",
    "    )\n",
    "\n",
    "    scores = [0] * n\n",
    "    for final_rank, (_, _, vertex) in enumerate(tied_vertices):\n",
    "        scores[vertex] = n - final_rank - 1\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0465ba07-e038-43fc-8b77-f7d5f98b2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimize_updated_scores(adjacency_matrix, scores, epsilon=1e-8):\n",
    "#     import cvxpy as cp\n",
    "#     import numpy as np\n",
    "\n",
    "#     n = len(scores)\n",
    "\n",
    "#     # Convert inputs to numpy\n",
    "#     adjacency_matrix_np = adjacency_matrix.numpy()\n",
    "#     scores_np = scores.numpy().flatten()\n",
    "\n",
    "#     # Define optimization variables\n",
    "#     updated_scores = cp.Variable(n)\n",
    "#     R = cp.Variable((n, n))  # Auxiliary variable for ratios\n",
    "\n",
    "#     # Compute M (skew-symmetric pairwise matrix)\n",
    "#     M = adjacency_matrix_np - adjacency_matrix_np.T\n",
    "\n",
    "#     # Edge mask\n",
    "#     edge_mask = (adjacency_matrix_np + adjacency_matrix_np.T > 0).astype(float)\n",
    "\n",
    "#     # Objective: Minimize squared difference between M and R\n",
    "#     objective = cp.sum_squares(cp.multiply(edge_mask, M - R))\n",
    "\n",
    "#     # Constraints for R_ij\n",
    "#     constraints = []\n",
    "#     for i in range(n):\n",
    "#         for j in range(n):\n",
    "#             if edge_mask[i, j] > 0:\n",
    "#                 constraints.append(R[i, j] * (updated_scores[i] + updated_scores[j] + epsilon) == updated_scores[i] - updated_scores[j])\n",
    "\n",
    "#     # Order-preserving constraints\n",
    "#     constraints += [\n",
    "#         updated_scores[i] <= updated_scores[j]\n",
    "#         for i in range(n)\n",
    "#         for j in range(n)\n",
    "#         if scores_np[i] <= scores_np[j]\n",
    "#     ]\n",
    "\n",
    "#     # Solve the problem\n",
    "#     problem = cp.Problem(cp.Minimize(objective), constraints)\n",
    "#     problem.solve()\n",
    "\n",
    "#     return updated_scores.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970aa5dc-cb8b-4d04-a90a-973d74396090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def graph_to_adjacency_matrix(graph, weights, n):\n",
    "    \"\"\"\n",
    "    Convert the graph to an adjacency matrix with weights.\n",
    "    :param graph: Adjacency list of the DAG with (neighbor, edge_id) pairs.\n",
    "    :param weights: Dictionary of edge weights with keys (u, v, edge_id).\n",
    "    :param n: Total number of vertices in the graph.\n",
    "    :return: An adjacency matrix (n x n) with weights as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    adjacency_matrix = torch.zeros((n, n), dtype=torch.float32)\n",
    "\n",
    "    for u in graph:\n",
    "        for v, edge_id in graph[u]:\n",
    "            edge_key = (u, v, edge_id)\n",
    "            adjacency_matrix[u, v] = weights.get(edge_key, 0)\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e70cf-ef80-4e9a-adb4-39e31e3e1b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09a3b680-b16e-4ab5-a9b7-7e2faa7766ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def reorder_floats(x):\n",
    "    n = len(x)\n",
    "    random_floats = np.random.uniform(0,  2*n/3, n)\n",
    "    y = np.zeros(n)\n",
    "    for idx, val in enumerate(np.argsort(x)):\n",
    "        y[val] = sorted(random_floats)[idx]\n",
    "    return y\n",
    "\n",
    "def calculate_upset_loss(adjacency_matrix, scores, style='ratio', margin=0.01):\n",
    "    \"\"\"\n",
    "    Calculate the upset loss for the graph rankings using adjacency matrix and scores.\n",
    "\n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
    "    :param style: Type of upset loss ('naive', 'simple', 'ratio', or 'margin').\n",
    "    :param margin: Margin for margin loss (default: 0.01).\n",
    "    :return: Torch FloatTensor upset loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8  # For numerical stability\n",
    "\n",
    "    # Ensure scores are 2D\n",
    "    if scores.ndim == 1:\n",
    "        scores = scores.view(-1, 1)\n",
    "\n",
    "    # Skew-symmetric pairwise comparison matrix (M)\n",
    "    M1 = adjacency_matrix - adjacency_matrix.T\n",
    "\n",
    "    # Normalize scores to [0, 1] range\n",
    "    normalized_scores = scores\n",
    "\n",
    "    # Pairwise score differences (T)\n",
    "    T1 = normalized_scores - normalized_scores.T\n",
    "\n",
    "    # Edge mask: Only consider meaningful edges (where M != 0)\n",
    "    edge_mask = M1 != 0\n",
    "\n",
    "    if style == 'ratio':\n",
    "        min_upset = float('inf')  # Initialize with a large value\n",
    "        \n",
    "        for _ in range(40):\n",
    "            # Generate reordered scores using reorder_floats\n",
    "            if _==0:\n",
    "                reordered_scores=scores\n",
    "            else:\n",
    "                reordered_scores = torch.FloatTensor(reorder_floats(scores.flatten().tolist()))\n",
    "            reordered_scores = reordered_scores.view(-1, 1)\n",
    "\n",
    "            # Compute T2 for normalized scores\n",
    "            T2 = reordered_scores + reordered_scores.T + epsilon\n",
    "            T = torch.div(T1, T2)\n",
    "            M2 = adjacency_matrix + adjacency_matrix.T + epsilon\n",
    "            M3 = torch.div(M1, M2)  # Normalize the adjacency matrix\n",
    "            \n",
    "            # Compute ratio-based upset loss for this iteration\n",
    "            powers = torch.pow((M3 - T)[edge_mask], 2)\n",
    "            upset_loss = torch.sum(powers) / torch.sum(edge_mask)\n",
    "\n",
    "            # Track the minimum upset loss\n",
    "            min_upset = min(min_upset, upset_loss.item())\n",
    "        \n",
    "        return torch.tensor(min_upset)\n",
    "\n",
    "    elif style == 'naive':\n",
    "        upset = torch.sum(torch.sign(T1[edge_mask]) != torch.sign(M1[edge_mask])) / torch.sum(edge_mask)\n",
    "\n",
    "    elif style == 'simple':\n",
    "        upset = torch.mean((torch.sign(T1[edge_mask]) - torch.sign(M1[edge_mask]))**2)\n",
    "\n",
    "    elif style == 'margin':\n",
    "        upset = torch.mean(torch.nn.functional.relu(-M1[edge_mask] * (T1[edge_mask] - margin)))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported style: {style}\")\n",
    "\n",
    "    return upset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb48e27-720b-4562-9a4b-0c7b314f97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_ratio_upset_loss(adjacency_matrix, scores, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Compute the ratio upset loss for the graph rankings using adjacency matrix and scores.\n",
    "\n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
    "    :param epsilon: Small value for numerical stability (default: 1e-8).\n",
    "    :return: Torch FloatTensor ratio upset loss value.\n",
    "    \"\"\"\n",
    "    # Ensure scores are 2D\n",
    "    if scores.ndim == 1:\n",
    "        scores = scores.view(-1, 1)\n",
    "\n",
    "    # Skew-symmetric pairwise comparison matrix (M)\n",
    "    M1 = adjacency_matrix - adjacency_matrix.T\n",
    "\n",
    "    # Pairwise score differences (T1)\n",
    "    T1 = scores - scores.T\n",
    "\n",
    "    # Edge mask: Only consider meaningful edges (where M1 != 0)\n",
    "    edge_mask = M1 != 0\n",
    "\n",
    "    # Compute T2 for normalized scores\n",
    "    T2 = scores + scores.T + epsilon\n",
    "    T = torch.div(T1, T2)\n",
    "\n",
    "    # Normalize M1 using adjacency matrix\n",
    "    M2 = adjacency_matrix + adjacency_matrix.T + epsilon\n",
    "    M3 = torch.div(M1, M2)  # Normalize the adjacency matrix\n",
    "\n",
    "    # Compute ratio upset loss\n",
    "    powers = torch.pow((M3 - T)[edge_mask], 2)\n",
    "    upset_loss = torch.sum(powers) / torch.sum(edge_mask)\n",
    "\n",
    "    return upset_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b336e9-7412-4d1a-8a2f-8fa7f37d5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_ratio_upset_loss(adjacency_matrix, scores, epsilon=1e-2, max_time=120):\n",
    "    \"\"\"\n",
    "    Perform optimization to minimize the ratio upset loss, ensuring naive and simple losses do not worsen.\n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param scores: Initial scores for optimization.\n",
    "    :param epsilon: Small value for numerical stability (default: 1e-2).\n",
    "    :param max_time: Maximum time for optimization in seconds.\n",
    "    :return: Tuple (optimal_scores, minimized_loss)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from scipy.optimize import minimize\n",
    "    import time\n",
    "\n",
    "    n = adjacency_matrix.shape[0]\n",
    "\n",
    "    # Compute initial losses\n",
    "    initial_scores = scores.clone().detach().view(-1).numpy()\n",
    "    initial_losses = {\n",
    "        \"naive\": calculate_upset_loss(adjacency_matrix, scores.view(-1, 1), style=\"naive\"),\n",
    "        \"simple\": calculate_upset_loss(adjacency_matrix, scores.view(-1, 1), style=\"simple\"),\n",
    "    }\n",
    "\n",
    "    # Objective function\n",
    "    def objective_function(updated_scores, adjacency_matrix, initial_losses):\n",
    "        updated_scores = torch.tensor(updated_scores, dtype=torch.float32).view(-1, 1)\n",
    "        ratio_loss = calculate_upset_loss(adjacency_matrix, updated_scores, style=\"ratio\")\n",
    "        naive_loss = calculate_upset_loss(adjacency_matrix, updated_scores, style=\"naive\")\n",
    "        simple_loss = calculate_upset_loss(adjacency_matrix, updated_scores, style=\"simple\")\n",
    "\n",
    "        penalty = 0\n",
    "        if naive_loss > initial_losses[\"naive\"]:\n",
    "            penalty += naive_loss - initial_losses[\"naive\"]\n",
    "        if simple_loss > initial_losses[\"simple\"]:\n",
    "            penalty += simple_loss - initial_losses[\"simple\"]\n",
    "\n",
    "        return ratio_loss + 100 * penalty\n",
    "\n",
    "    # Timer callback\n",
    "    class TimerCallback:\n",
    "        def __init__(self, max_time, objective_function, adjacency_matrix, initial_losses):\n",
    "            self.start_time = time.time()\n",
    "            self.max_time = max_time\n",
    "            self.iterations = 0  # Track iterations\n",
    "            self.objective_function = objective_function\n",
    "            self.adjacency_matrix = adjacency_matrix\n",
    "            self.initial_losses = initial_losses\n",
    "            self.min_loss = float(\"inf\")  # Track minimum loss\n",
    "\n",
    "        def __call__(self, xk, *args, **kwargs):\n",
    "            self.iterations += 1\n",
    "\n",
    "            # Compute the objective function value\n",
    "            current_loss = self.objective_function(\n",
    "                xk, self.adjacency_matrix, self.initial_losses\n",
    "            )\n",
    "\n",
    "            # Update and print the minimum loss found so far\n",
    "            self.min_loss = min(self.min_loss, current_loss)\n",
    "            print(f\"Iteration {self.iterations}: Minimum loss so far: {self.min_loss:.6f}\")\n",
    "\n",
    "            # Stop optimization if the time limit is exceeded\n",
    "            if time.time() - self.start_time > self.max_time:\n",
    "                print(\"Time limit exceeded, stopping optimization.\")\n",
    "                raise StopIteration  # Signal COBYLA to stop\n",
    "\n",
    "    # Create the callback instance\n",
    "    callback = TimerCallback(\n",
    "        max_time=max_time,\n",
    "        objective_function=objective_function,\n",
    "        adjacency_matrix=adjacency_matrix,\n",
    "        initial_losses=initial_losses,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Run optimization with COBYLA\n",
    "        result = minimize(\n",
    "            fun=objective_function,\n",
    "            x0=initial_scores,\n",
    "            method=\"COBYLA\",\n",
    "            args=(adjacency_matrix, initial_losses),  # Pass required arguments\n",
    "            options={\"maxiter\": 500, \"disp\": False},\n",
    "            callback=callback,  # Logs the minimum loss after each iteration\n",
    "        )\n",
    "    except StopIteration:\n",
    "        print(\"Optimization stopped early due to time limit.\")\n",
    "\n",
    "    # Extract results\n",
    "    optimal_scores = result.x\n",
    "    minimized_loss = result.fun\n",
    "\n",
    "    print(f\"Optimization completed after {callback.iterations} iterations.\")\n",
    "    print(f\"Final minimum loss: {callback.min_loss:.6f}\")\n",
    "    return optimal_scores, minimized_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2567d-9114-4688-98f5-85c6b93a1527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68495d94-ad4d-43b2-9333-c84dc62fd752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c6b21d9-6337-4113-8cdf-bdc54cd095c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_upset_losses(file_path, rankings):\n",
    "    \"\"\"\n",
    "    Evaluate upset losses (naive, simple, ratio, margin) for a graph and given rankings.\n",
    "\n",
    "    :param file_path: Path to the graph file.\n",
    "    :param rankings: List of rankings for the vertices.\n",
    "    \"\"\"\n",
    "    # Step 1: Prepare Graph and Adjacency Matrix\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _ in edges) + 1\n",
    "    graph, weights = initialize_graph(edges)\n",
    "    adjacency_matrix = graph_to_adjacency_matrix(graph, weights, n)\n",
    "\n",
    "    # Step 2: Convert Rankings to Scores Tensor\n",
    "    scores = torch.FloatTensor(rankings).view(-1, 1)\n",
    "\n",
    "    # Step 3: Calculate Upset Losses\n",
    "    naive_loss = calculate_upset_loss(adjacency_matrix, scores, style='naive').item()\n",
    "    simple_loss = calculate_upset_loss(adjacency_matrix, scores, style='simple').item()\n",
    "    ratio_loss = calculate_upset_loss(adjacency_matrix, scores, style='ratio').item()\n",
    "    margin_loss = calculate_upset_loss(adjacency_matrix, scores, style='margin').item()\n",
    "\n",
    "    # Step 4: Print Results\n",
    "    print(\"Upset Losses for the Graph Rankings:\")\n",
    "    print(f\"Naive Upset Loss: {naive_loss:.4f}\")\n",
    "    print(f\"Simple Upset Loss: {simple_loss:.4f}\")\n",
    "    print(f\"Differentiable Upset Loss (Ratio): {ratio_loss:.4f}\")\n",
    "    print(f\"Upset Margin Loss: {margin_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e96b02a-635c-40c9-9e43-26cb60a76680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 1987adj.txt - Number of nodes: 290, Number of edges: 3045\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Naive Loss: 0.13031385838985443\n",
      "Simple Loss: 0.5212554335594177\n",
      "Ratio Loss: 0.5652036070823669\n",
      "Elapsed Time: 2.0653 seconds\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# List of input files\n",
    "file_paths = [\"1987adj.txt\"]\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Iterate through each input file\n",
    "for file_path in file_paths:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Read the graph and initialize structures\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    print(f\"Processing file: {file_path} - Number of nodes: {n}, Number of edges: {len(edges)}\")\n",
    "\n",
    "    init_graph, init_weights = initialize_graph(edges)\n",
    "\n",
    "    # Step 2: Ensure the graph is a DAG by removing cycles\n",
    "    result = mwfas(file_path)\n",
    "    new_graph = result['final_graph']\n",
    "    new_weights = {key: value for key, value in init_weights.items() if key not in result['removed_weights']}\n",
    "\n",
    "    # Step 3: Compute rankings for the vertices using the modified graph\n",
    "    final_rankings = compute_vertex_rankings(new_graph, new_weights, n)\n",
    "\n",
    "    # Step 4: Evaluate upset losses before optimization\n",
    "    scores = torch.FloatTensor(final_rankings).view(-1, 1)\n",
    "    adjacency_matrix = graph_to_adjacency_matrix(init_graph, init_weights, n)\n",
    "\n",
    "    print(\"Evaluating losses before optimization...\")\n",
    "    naive_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='naive')\n",
    "    simple_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='simple')\n",
    "    ratio_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='ratio')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Before Optimization:\")\n",
    "    print(f\"Naive Loss: {naive_loss_before}\")\n",
    "    print(f\"Simple Loss: {simple_loss_before}\")\n",
    "    print(f\"Ratio Loss: {ratio_loss_before}\")\n",
    "    print(f\"Elapsed Time: {elapsed_time:.4f} seconds\")\n",
    "    # Step 5: Perform optimization\n",
    "  #  optimized_scores = minimize_ratio_upset_loss(adjacency_matrix, scores)[0]\n",
    "  #  optimized_scores = torch.FloatTensor(optimized_scores).view(-1, 1)  # Ensure Torch Tensor format\n",
    "\n",
    "    # Step 6: Evaluate upset losses after optimization\n",
    "   # print(\"Evaluating losses after optimization...\")\n",
    "   # naive_loss_after = calculate_upset_loss(adjacency_matrix, optimized_scores, style='naive')\n",
    "   # simple_loss_after = calculate_upset_loss(adjacency_matrix, optimized_scores, style='simple')\n",
    "   # ratio_loss_after = calculate_upset_loss(adjacency_matrix, optimized_scores, style='ratio')\n",
    "\n",
    "   # end_time = time.time()\n",
    "   # elapsed_time = end_time - start_time\n",
    "\n",
    "    # Step 7: Print final results\n",
    "   # print(\"After Optimization:\")\n",
    "   # print(f\"Naive Loss: {naive_loss_after}\")\n",
    "   # print(f\"Simple Loss: {simple_loss_after}\")\n",
    "   # print(f\"Ratio Loss: {ratio_loss_after}\")\n",
    "   # print(f\"Elapsed Time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "   \n",
    "    \n",
    "    # Store results in a list\n",
    "    # results.append({\n",
    "    #     \"File\": \"Basketball_\"+str(file_path[:9]),\n",
    "    #     \"Nodes\": n,\n",
    "    #     \"Edges\": len(edges),\n",
    "    #     \"Naive Loss\": round(naive_loss.item(),2),\n",
    "    #     \"Simple Loss\": round(simple_loss.item(),2),\n",
    "    #     \"Ratio Loss\": round(ratio_loss.item(),2),\n",
    "    #     \"Elapsed Time (s)\": round(elapsed_time,2)\n",
    "    # })\n",
    "    # print\n",
    "    # print(\"simple loss=\",simple_loss.item())\n",
    "\n",
    "# Write results to an Excel file\n",
    "#output_file = \"graph_analysis_results.xlsx\"\n",
    "#df = pd.DataFrame(results)\n",
    "#df.to_excel(output_file, index=False)\n",
    "\n",
    "#print(f\"Results written to {output_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d9e60-8023-40bb-9071-14ca23f5ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd692d53-f974-4f68-b104-ea89317cfa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: England 2009 2010.txt - Number of nodes: 20, Number of edges: 164\n",
      "Graph loaded successfully.\n",
      "Weights dictionary: {(10, 14, 0): 1.0, (12, 13, 1): 5.0, (17, 9, 2): 5.0, (2, 8, 3): 1.0, (11, 4, 4): 5.0, (15, 3, 5): 2.0, (0, 7, 6): 2.0, (18, 6, 7): 1.0, (1, 16, 8): 1.0, (2, 17, 9): 7.0, (10, 19, 10): 1.0, (6, 3, 11): 2.0, (18, 7, 12): 2.0, (16, 8, 13): 4.0, (1, 0, 14): 4.0, (11, 3, 15): 6.0, (6, 0, 16): 1.0, (8, 9, 17): 1.0, (12, 19, 18): 4.0, (17, 13, 19): 1.0, (18, 10, 20): 10.0, (4, 7, 21): 1.0, (2, 15, 22): 3.0, (16, 5, 23): 3.0, (14, 1, 24): 1.0, (1, 9, 25): 3.0, (2, 7, 26): 4.0, (18, 11, 27): 3.0, (0, 17, 28): 1.0, (16, 6, 29): 1.0, (14, 15, 30): 4.0, (4, 10, 31): 2.0, (12, 3, 32): 3.0, (13, 19, 33): 2.0, (1, 7, 34): 8.0, (12, 11, 35): 2.0, (9, 3, 36): 1.0, (2, 0, 37): 8.0, (17, 8, 38): 4.0, (18, 16, 39): 4.0, (14, 6, 40): 2.0, (11, 10, 41): 3.0, (14, 3, 42): 3.0, (9, 0, 43): 1.0, (7, 17, 44): 1.0, (6, 8, 45): 1.0, (1, 5, 46): 4.0, (2, 16, 47): 2.0, (4, 13, 48): 4.0, (18, 12, 49): 2.0, (19, 15, 50): 1.0, (9, 6, 51): 2.0, (13, 14, 52): 2.0, (11, 15, 53): 5.0, (1, 8, 54): 5.0, (4, 3, 55): 2.0, (18, 0, 56): 6.0, (16, 7, 57): 3.0, (2, 10, 58): 6.0, (17, 19, 59): 2.0, (12, 5, 60): 2.0, (16, 9, 61): 1.0, (8, 10, 62): 1.0, (18, 17, 63): 1.0, (3, 19, 64): 3.0, (11, 13, 65): 3.0, (2, 1, 66): 4.0, (15, 5, 67): 1.0, (12, 14, 68): 2.0, (11, 6, 69): 2.0, (2, 14, 70): 5.0, (18, 9, 71): 5.0, (16, 3, 72): 3.0, (0, 5, 73): 2.0, (1, 17, 74): 2.0, (13, 7, 75): 2.0, (12, 10, 76): 3.0, (17, 6, 77): 1.0, (10, 7, 78): 3.0, (2, 13, 79): 5.0, (3, 8, 80): 1.0, (4, 9, 81): 1.0, (1, 18, 82): 1.0, (12, 15, 83): 1.0, (11, 5, 84): 2.0, (11, 16, 85): 2.0, (2, 9, 86): 5.0, (7, 8, 87): 5.0, (15, 1, 88): 2.0, (18, 13, 89): 2.0, (3, 10, 90): 4.0, (5, 17, 91): 1.0, (12, 6, 92): 4.0, (5, 14, 93): 1.0, (14, 9, 94): 5.0, (13, 3, 95): 2.0, (12, 7, 96): 5.0, (11, 19, 97): 4.0, (2, 18, 98): 2.0, (0, 8, 99): 1.0, (4, 5, 100): 1.0, (15, 10, 101): 1.0, (14, 7, 102): 3.0, (2, 19, 103): 6.0, (5, 8, 104): 3.0, (18, 4, 105): 1.0, (11, 17, 106): 1.0, (13, 9, 107): 5.0, (0, 3, 108): 2.0, (16, 10, 109): 11.0, (15, 13, 110): 1.0, (4, 8, 111): 3.0, (0, 13, 112): 3.0, (8, 12, 113): 1.0, (18, 3, 114): 8.0, (5, 7, 115): 1.0, (10, 17, 116): 1.0, (2, 11, 117): 5.0, (1, 4, 118): 3.0, (6, 19, 119): 2.0, (11, 0, 120): 4.0, (14, 8, 121): 5.0, (1, 13, 122): 1.0, (12, 2, 123): 3.0, (3, 7, 124): 3.0, (18, 5, 125): 7.0, (6, 10, 126): 2.0, (16, 4, 127): 1.0, (15, 17, 128): 1.0, (5, 6, 129): 1.0, (12, 9, 130): 2.0, (15, 7, 131): 3.0, (4, 2, 132): 1.0, (13, 8, 133): 1.0, (14, 18, 134): 1.0, (19, 16, 135): 2.0, (11, 1, 136): 2.0, (9, 5, 137): 3.0, (18, 19, 138): 4.0, (14, 17, 139): 2.0, (11, 7, 140): 2.0, (2, 3, 141): 6.0, (16, 12, 142): 4.0, (11, 8, 143): 4.0, (14, 0, 144): 1.0, (16, 13, 145): 4.0, (12, 17, 146): 1.0, (1, 3, 147): 1.0, (2, 5, 148): 3.0, (19, 7, 149): 3.0, (2, 6, 150): 3.0, (9, 7, 151): 1.0, (16, 15, 152): 2.0, (1, 19, 153): 2.0, (12, 0, 154): 2.0, (4, 17, 155): 2.0, (5, 3, 156): 2.0, (13, 10, 157): 1.0, (11, 14, 158): 3.0, (18, 8, 159): 6.0, (0, 15, 160): 2.0, (4, 12, 161): 4.0, (11, 9, 162): 4.0, (9, 10, 163): 4.0}\n",
      "Warning: Graph contains cycles or disconnected components.\n",
      "Warning: Missing weight for edge (10, 14, 0). Defaulting to 0.\n",
      "Warning: Missing weight for edge (10, 19, 10). Defaulting to 0.\n",
      "Warning: Missing weight for edge (10, 17, 116). Defaulting to 0.\n",
      "Warning: Missing weight for edge (12, 13, 1). Defaulting to 0.\n",
      "Warning: Missing weight for edge (12, 19, 18). Defaulting to 0.\n",
      "Warning: Missing weight for edge (12, 14, 68). Defaulting to 0.\n",
      "Warning: Missing weight for edge (12, 15, 83). Defaulting to 0.\n",
      "Warning: Missing weight for edge (12, 17, 146). Defaulting to 0.\n",
      "Warning: Missing weight for edge (17, 19, 59). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 8, 3). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 17, 9). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 15, 22). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 7, 26). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 16, 47). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 10, 58). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 14, 70). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 13, 79). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 9, 86). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 18, 98). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 19, 103). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 11, 117). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 3, 141). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 5, 148). Defaulting to 0.\n",
      "Warning: Missing weight for edge (2, 6, 150). Defaulting to 0.\n",
      "Warning: Missing weight for edge (11, 15, 53). Defaulting to 0.\n",
      "Warning: Missing weight for edge (11, 13, 65). Defaulting to 0.\n",
      "Warning: Missing weight for edge (11, 16, 85). Defaulting to 0.\n",
      "Warning: Missing weight for edge (11, 19, 97). Defaulting to 0.\n",
      "Warning: Missing weight for edge (11, 17, 106). Defaulting to 0.\n",
      "Warning: Missing weight for edge (11, 14, 158). Defaulting to 0.\n",
      "Warning: Missing weight for edge (15, 17, 128). Defaulting to 0.\n",
      "Warning: Missing weight for edge (0, 7, 6). Defaulting to 0.\n",
      "Warning: Missing weight for edge (0, 17, 28). Defaulting to 0.\n",
      "Warning: Missing weight for edge (0, 5, 73). Defaulting to 0.\n",
      "Warning: Missing weight for edge (0, 8, 99). Defaulting to 0.\n",
      "Warning: Missing weight for edge (0, 3, 108). Defaulting to 0.\n",
      "Warning: Missing weight for edge (0, 13, 112). Defaulting to 0.\n",
      "Warning: Missing weight for edge (0, 15, 160). Defaulting to 0.\n",
      "Warning: Missing weight for edge (18, 19, 138). Defaulting to 0.\n",
      "Warning: Missing weight for edge (1, 16, 8). Defaulting to 0.\n",
      "Warning: Missing weight for edge (1, 9, 25). Defaulting to 0.\n",
      "Warning: Missing weight for edge (1, 7, 34). Defaulting to 0.\n",
      "Warning: Missing weight for edge (1, 5, 46). Defaulting to 0.\n",
      "Warning: Missing weight for edge (1, 8, 54). Defaulting to 0.\n",
      "Warning: Missing weight for edge (1, 17, 74). Defaulting to 0.\n",
      "Warning: Missing weight for edge (1, 18, 82). Defaulting to 0.\n",
      "Warning: Missing weight for edge (1, 4, 118). Defaulting to 0.\n",
      "Warning: Missing weight for edge (1, 13, 122). Defaulting to 0.\n",
      "Warning: Missing weight for edge (1, 3, 147). Defaulting to 0.\n",
      "Warning: Missing weight for edge (1, 19, 153). Defaulting to 0.\n",
      "Warning: Missing weight for edge (6, 8, 45). Defaulting to 0.\n",
      "Warning: Missing weight for edge (6, 19, 119). Defaulting to 0.\n",
      "Warning: Missing weight for edge (6, 10, 126). Defaulting to 0.\n",
      "Warning: Missing weight for edge (8, 9, 17). Defaulting to 0.\n",
      "Warning: Missing weight for edge (8, 10, 62). Defaulting to 0.\n",
      "Warning: Missing weight for edge (8, 12, 113). Defaulting to 0.\n",
      "Warning: Missing weight for edge (4, 7, 21). Defaulting to 0.\n",
      "Warning: Missing weight for edge (4, 10, 31). Defaulting to 0.\n",
      "Warning: Missing weight for edge (4, 13, 48). Defaulting to 0.\n",
      "Warning: Missing weight for edge (4, 9, 81). Defaulting to 0.\n",
      "Warning: Missing weight for edge (4, 5, 100). Defaulting to 0.\n",
      "Warning: Missing weight for edge (4, 8, 111). Defaulting to 0.\n",
      "Warning: Missing weight for edge (4, 17, 155). Defaulting to 0.\n",
      "Warning: Missing weight for edge (4, 12, 161). Defaulting to 0.\n",
      "Warning: Missing weight for edge (14, 15, 30). Defaulting to 0.\n",
      "Warning: Missing weight for edge (14, 18, 134). Defaulting to 0.\n",
      "Warning: Missing weight for edge (14, 17, 139). Defaulting to 0.\n",
      "Warning: Missing weight for edge (13, 19, 33). Defaulting to 0.\n",
      "Warning: Missing weight for edge (13, 14, 52). Defaulting to 0.\n",
      "Warning: Missing weight for edge (9, 10, 163). Defaulting to 0.\n",
      "Warning: Missing weight for edge (7, 17, 44). Defaulting to 0.\n",
      "Warning: Missing weight for edge (7, 8, 87). Defaulting to 0.\n",
      "Warning: Missing weight for edge (3, 19, 64). Defaulting to 0.\n",
      "Warning: Missing weight for edge (3, 8, 80). Defaulting to 0.\n",
      "Warning: Missing weight for edge (3, 10, 90). Defaulting to 0.\n",
      "Warning: Missing weight for edge (3, 7, 124). Defaulting to 0.\n",
      "Warning: Missing weight for edge (5, 17, 91). Defaulting to 0.\n",
      "Warning: Missing weight for edge (5, 14, 93). Defaulting to 0.\n",
      "Warning: Missing weight for edge (5, 8, 104). Defaulting to 0.\n",
      "Warning: Missing weight for edge (5, 7, 115). Defaulting to 0.\n",
      "Warning: Missing weight for edge (5, 6, 129). Defaulting to 0.\n",
      "Graph processed: Total edges: 164, Removed weight: 248.0\n",
      "Final graph: defaultdict(<class 'list'>, {10: [(14, 0), (19, 10), (17, 116)], 12: [(13, 1), (19, 18), (14, 68), (15, 83), (17, 146)], 17: [(19, 59)], 2: [(8, 3), (17, 9), (15, 22), (7, 26), (16, 47), (10, 58), (14, 70), (13, 79), (9, 86), (18, 98), (19, 103), (11, 117), (3, 141), (5, 148), (6, 150)], 11: [(15, 53), (13, 65), (16, 85), (19, 97), (17, 106), (14, 158)], 15: [(17, 128)], 0: [(7, 6), (17, 28), (5, 73), (8, 99), (3, 108), (13, 112), (15, 160)], 18: [(19, 138)], 1: [(16, 8), (9, 25), (7, 34), (5, 46), (8, 54), (17, 74), (18, 82), (4, 118), (13, 122), (3, 147), (19, 153)], 6: [(8, 45), (19, 119), (10, 126)], 16: [], 8: [(9, 17), (10, 62), (12, 113)], 4: [(7, 21), (10, 31), (13, 48), (9, 81), (5, 100), (8, 111), (17, 155), (12, 161)], 14: [(15, 30), (18, 134), (17, 139)], 13: [(19, 33), (14, 52)], 9: [(10, 163)], 7: [(17, 44), (8, 87)], 19: [], 3: [(19, 64), (8, 80), (10, 90), (7, 124)], 5: [(17, 91), (14, 93), (8, 104), (7, 115), (6, 129)]})\n",
      "Removed weights: {(10, 7, 78): 3.0, (12, 3, 32): 3.0, (12, 11, 35): 2.0, (12, 5, 60): 2.0, (12, 10, 76): 3.0, (12, 6, 92): 4.0, (12, 7, 96): 5.0, (12, 2, 123): 3.0, (12, 9, 130): 2.0, (12, 0, 154): 2.0, (17, 9, 2): 5.0, (17, 13, 19): 1.0, (17, 8, 38): 4.0, (17, 6, 77): 1.0, (2, 0, 37): 8.0, (2, 1, 66): 4.0, (11, 4, 4): 5.0, (11, 3, 15): 6.0, (11, 10, 41): 3.0, (11, 6, 69): 2.0, (11, 5, 84): 2.0, (11, 0, 120): 4.0, (11, 1, 136): 2.0, (11, 7, 140): 2.0, (11, 8, 143): 4.0, (11, 9, 162): 4.0, (15, 3, 5): 2.0, (15, 5, 67): 1.0, (15, 1, 88): 2.0, (15, 10, 101): 1.0, (15, 13, 110): 1.0, (15, 7, 131): 3.0, (18, 6, 7): 1.0, (18, 7, 12): 2.0, (18, 10, 20): 10.0, (18, 11, 27): 3.0, (18, 16, 39): 4.0, (18, 12, 49): 2.0, (18, 0, 56): 6.0, (18, 17, 63): 1.0, (18, 9, 71): 5.0, (18, 13, 89): 2.0, (18, 4, 105): 1.0, (18, 3, 114): 8.0, (18, 5, 125): 7.0, (18, 8, 159): 6.0, (1, 0, 14): 4.0, (6, 3, 11): 2.0, (6, 0, 16): 1.0, (16, 8, 13): 4.0, (16, 5, 23): 3.0, (16, 6, 29): 1.0, (16, 7, 57): 3.0, (16, 9, 61): 1.0, (16, 3, 72): 3.0, (16, 10, 109): 11.0, (16, 4, 127): 1.0, (16, 12, 142): 4.0, (16, 13, 145): 4.0, (16, 15, 152): 2.0, (4, 3, 55): 2.0, (4, 2, 132): 1.0, (14, 1, 24): 1.0, (14, 6, 40): 2.0, (14, 3, 42): 3.0, (14, 9, 94): 5.0, (14, 7, 102): 3.0, (14, 8, 121): 5.0, (14, 0, 144): 1.0, (13, 7, 75): 2.0, (13, 3, 95): 2.0, (13, 9, 107): 5.0, (13, 8, 133): 1.0, (13, 10, 157): 1.0, (9, 3, 36): 1.0, (9, 0, 43): 1.0, (9, 6, 51): 2.0, (9, 5, 137): 3.0, (9, 7, 151): 1.0, (19, 15, 50): 1.0, (19, 16, 135): 2.0, (19, 7, 149): 3.0, (5, 3, 156): 2.0, (10, 14, 0): 0, (10, 19, 10): 0, (10, 17, 116): 0, (12, 13, 1): 0, (12, 19, 18): 0, (12, 14, 68): 0, (12, 15, 83): 0, (12, 17, 146): 0, (17, 19, 59): 0, (2, 8, 3): 0, (2, 17, 9): 0, (2, 15, 22): 0, (2, 7, 26): 0, (2, 16, 47): 0, (2, 10, 58): 0, (2, 14, 70): 0, (2, 13, 79): 0, (2, 9, 86): 0, (2, 18, 98): 0, (2, 19, 103): 0, (2, 11, 117): 0, (2, 3, 141): 0, (2, 5, 148): 0, (2, 6, 150): 0, (11, 15, 53): 0, (11, 13, 65): 0, (11, 16, 85): 0, (11, 19, 97): 0, (11, 17, 106): 0, (11, 14, 158): 0, (15, 17, 128): 0, (0, 7, 6): 0, (0, 17, 28): 0, (0, 5, 73): 0, (0, 8, 99): 0, (0, 3, 108): 0, (0, 13, 112): 0, (0, 15, 160): 0, (18, 19, 138): 0, (1, 16, 8): 0, (1, 9, 25): 0, (1, 7, 34): 0, (1, 5, 46): 0, (1, 8, 54): 0, (1, 17, 74): 0, (1, 18, 82): 0, (1, 4, 118): 0, (1, 13, 122): 0, (1, 3, 147): 0, (1, 19, 153): 0, (6, 8, 45): 0, (6, 19, 119): 0, (6, 10, 126): 0, (8, 9, 17): 0, (8, 10, 62): 0, (8, 12, 113): 0, (4, 7, 21): 0, (4, 10, 31): 0, (4, 13, 48): 0, (4, 9, 81): 0, (4, 5, 100): 0, (4, 8, 111): 0, (4, 17, 155): 0, (4, 12, 161): 0, (14, 15, 30): 0, (14, 18, 134): 0, (14, 17, 139): 0, (13, 19, 33): 0, (13, 14, 52): 0, (9, 10, 163): 0, (7, 17, 44): 0, (7, 8, 87): 0, (3, 19, 64): 0, (3, 8, 80): 0, (3, 10, 90): 0, (3, 7, 124): 0, (5, 17, 91): 0, (5, 14, 93): 0, (5, 8, 104): 0, (5, 7, 115): 0, (5, 6, 129): 0}\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Naive Loss: nan\n",
      "Simple Loss: nan\n",
      "Ratio Loss: inf\n",
      "Elapsed Time: 0.0140 seconds\n"
     ]
    }
   ],
   "source": [
    "file_paths = [\"England 2009 2010.txt\"]\n",
    "results = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Read and process the graph\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1  # Get number of nodes\n",
    "    print(f\"Processing file: {file_path} - Number of nodes: {n}, Number of edges: {len(edges)}\")\n",
    "\n",
    "    # Step 2: Ensure the graph is a DAG by removing cycles\n",
    "    result = mwfas2(file_path)\n",
    "    new_graph = result['final_graph']\n",
    "    removed_weights = result['removed_weights']\n",
    "    total_edges = result['total_edges']\n",
    "    total_removed_weight = result['removed_weight']\n",
    "\n",
    "    for u in new_graph:\n",
    "        for v, edge_id in new_graph[u]:\n",
    "            edge_key = (u, v, edge_id)\n",
    "            if edge_key not in removed_weights:\n",
    "                print(f\"Warning: Missing weight for edge {edge_key}. Defaulting to 0.\")\n",
    "                removed_weights[edge_key] = 0\n",
    "\n",
    "    print(f\"Graph processed: Total edges: {total_edges}, Removed weight: {total_removed_weight}\")\n",
    "\n",
    "    # Step 3: Compute rankings for the vertices using the modified graph\n",
    "    final_rankings = compute_vertex_rankings(new_graph, removed_weights, n)\n",
    "    print(\"Final graph:\", new_graph)\n",
    "    print(\"Removed weights:\", removed_weights)\n",
    "\n",
    "    # Step 4: Evaluate upset losses before optimization\n",
    "    scores = torch.FloatTensor(final_rankings).view(-1, 1)\n",
    "    adjacency_matrix = graph_to_adjacency_matrix(new_graph, removed_weights, n)\n",
    "\n",
    "    print(\"Evaluating losses before optimization...\")\n",
    "    naive_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='naive')\n",
    "    simple_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='simple')\n",
    "    ratio_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='ratio')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Before Optimization:\")\n",
    "    print(f\"Naive Loss: {naive_loss_before}\")\n",
    "    print(f\"Simple Loss: {simple_loss_before}\")\n",
    "    print(f\"Ratio Loss: {ratio_loss_before}\")\n",
    "    print(f\"Elapsed Time: {elapsed_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21703be-eeec-47c4-84fe-538f65ab3dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d6378-70ce-4715-b76d-743a87c1b41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881443a-c02b-434f-800f-9e5d5d429355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f7154-8bb8-47f3-8326-ff52dadef477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d7d3d-dad1-4ae7-9f5d-ea138b6be78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28f460-56cc-401b-93a0-b8b795cc619a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44889cc-3b19-49b3-9ce3-08b7bd930827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782e6b5-522b-4aec-ac12-96aa6947e83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c737627-0f69-4afe-b779-6a187921b2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45c268-57f6-4a92-b3a4-b9bb20d30b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37111c44-1af3-4f85-a47a-91fb20d3c6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
