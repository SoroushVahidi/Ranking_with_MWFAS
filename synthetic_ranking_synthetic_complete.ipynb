{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo4E7udN4P9T"
      },
      "outputs": [],
      "source": [
        "def initialize_graph(edges):\n",
        "    \"\"\"\n",
        "    Converts edge list to an adjacency list and weights dictionary.\n",
        "    Handles parallel edges by including edge_id as part of the structure.\n",
        "\n",
        "    Args:\n",
        "        edges (list): List of edges as (start, end, weight, edge_id).\n",
        "\n",
        "    Returns:\n",
        "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
        "        weights (dict): Dictionary mapping (start, end, edge_id) to weights.\n",
        "    \"\"\"\n",
        "    graph = defaultdict(list)\n",
        "    weights = {}\n",
        "    for u, v, w, edge_id in edges:\n",
        "        graph[u].append((v, edge_id))\n",
        "        weights[(u, v, edge_id)] = w\n",
        "    return graph, weights\n",
        "\n",
        "\n",
        "def find_cycles_and_reduce(graph, weights, n):\n",
        "    \"\"\"\n",
        "    Phase 1: Find cycles and reduce weights using a copy.\n",
        "    Handles graphs with parallel edges by considering edge identifiers.\n",
        "\n",
        "    Args:\n",
        "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
        "        weights (dict): Dictionary mapping (u, v, edge_id) to weights.\n",
        "        n (int): Number of vertices in the graph.\n",
        "\n",
        "    Returns:\n",
        "        removed_edges (set): Set of removed edges as (u, v, edge_id).\n",
        "        removed_weights (dict): Dictionary of removed edges with their original weights.\n",
        "    \"\"\"\n",
        "    weights_copy = weights.copy()  # Work with a copy of weights\n",
        "    removed_edges = set()\n",
        "    removed_weights = {}\n",
        "\n",
        "    while True:\n",
        "        cycle = find_cycle(graph, n)  # Modified `find_cycle` returns edges with edge_id\n",
        "        if not cycle:  # No cycle found\n",
        "            break\n",
        "\n",
        "        # Ensure all edges in the cycle exist in the weights dictionary\n",
        "        cycle = [(u, v, edge_id) for u, v, edge_id in cycle if (u, v, edge_id) in weights_copy]\n",
        "\n",
        "        if not cycle:  # If no valid cycle exists, continue\n",
        "            continue\n",
        "\n",
        "        # Find the minimum weight in the cycle\n",
        "        min_weight = min(weights_copy[(u, v, edge_id)] for u, v, edge_id in cycle)\n",
        "\n",
        "        for u, v, edge_id in cycle:\n",
        "            weights_copy[(u, v, edge_id)] -= min_weight\n",
        "            if weights_copy[(u, v, edge_id)] <= 0:\n",
        "                # Ensure the edge is in the graph before removing\n",
        "                if (v, edge_id) in graph[u]:\n",
        "                    graph[u].remove((v, edge_id))\n",
        "                    removed_edges.add((u, v, edge_id))\n",
        "                    removed_weights[(u, v, edge_id)] = weights[(u, v, edge_id)]\n",
        "\n",
        "    return removed_edges, removed_weights\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "def find_cycle(graph, n):\n",
        "    \"\"\"\n",
        "    Detect a cycle in the graph using DFS and return the cycle as a list of edges.\n",
        "    Handles parallel edges and cycles of length 2 caused by reverse edges.\n",
        "\n",
        "    Args:\n",
        "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
        "        n (int): Number of vertices in the graph.\n",
        "\n",
        "    Returns:\n",
        "        cycle (list): List of edges forming the cycle, or None if no cycle is found.\n",
        "    \"\"\"\n",
        "    visited = [False] * n\n",
        "    stack = [False] * n\n",
        "    parent = [-1] * n\n",
        "    edge_to_parent = {}  # Map to track edge_id for cycle reconstruction\n",
        "\n",
        "    def dfs(v):\n",
        "        visited[v] = True\n",
        "        stack[v] = True\n",
        "        for neighbor, edge_id in graph[v]:\n",
        "            if not visited[neighbor]:\n",
        "                parent[neighbor] = v\n",
        "                edge_to_parent[neighbor] = edge_id\n",
        "                cycle = dfs(neighbor)\n",
        "                if cycle:\n",
        "                    return cycle\n",
        "            elif stack[neighbor]:\n",
        "                # Found a cycle, reconstruct it\n",
        "                cycle = []\n",
        "                current = v\n",
        "                while current != neighbor:\n",
        "                    if current not in edge_to_parent:\n",
        "                        break  # Avoid KeyError if edge metadata is missing\n",
        "                    cycle.append((parent[current], current, edge_to_parent[current]))\n",
        "                    current = parent[current]\n",
        "\n",
        "                # Handle the root of the cycle\n",
        "                if neighbor in edge_to_parent and parent[neighbor] != -1:\n",
        "                    cycle.append((parent[neighbor], neighbor, edge_to_parent[neighbor]))\n",
        "                return cycle\n",
        "\n",
        "        stack[v] = False\n",
        "        return None\n",
        "\n",
        "    # Detect length-2 cycles caused by reverse edges\n",
        "    for u in list(graph):  # Use list(graph) to iterate over a static copy of keys\n",
        "        for neighbor, edge_id1 in graph[u]:\n",
        "            for neighbor_of_neighbor, edge_id2 in graph[neighbor]:\n",
        "                if neighbor_of_neighbor == u and edge_id1 != edge_id2:\n",
        "                    # Found a length-2 cycle\n",
        "                    return [(u, neighbor, edge_id1), (neighbor, u, edge_id2)]\n",
        "\n",
        "    # Run DFS for longer cycles\n",
        "    for i in range(n):\n",
        "        if not visited[i]:\n",
        "            cycle = dfs(i)\n",
        "            if cycle:\n",
        "                return cycle\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def update_edge_weights(graph, weights):\n",
        "    \"\"\"\n",
        "    Updates the weights of the edges in the graph based on the specified rule.\n",
        "    If both (i, j) and (j, i) edges exist, the smaller weight is removed, and the larger weight is updated to:\n",
        "    w_{i,j} / (w_{i,j} + w_{j,i}).\n",
        "    If only one edge exists, its weight is updated to 1.\n",
        "\n",
        "    Args:\n",
        "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
        "        weights (dict): Dictionary mapping (start, end, edge_id) to weights.\n",
        "\n",
        "    Returns:\n",
        "        updated_graph (dict): Updated adjacency list.\n",
        "        updated_weights (dict): Updated weights dictionary.\n",
        "    \"\"\"\n",
        "    processed_edges = set()  # Keep track of processed edges\n",
        "    updated_graph = defaultdict(list)\n",
        "    updated_weights = {}\n",
        "\n",
        "    # Iterate over a copy of the graph's keys to avoid modification during iteration\n",
        "    for u in list(graph.keys()):\n",
        "        for v, edge_id in graph[u]:\n",
        "            if (u, v, edge_id) not in processed_edges:\n",
        "                reverse_edge = next(\n",
        "                    ((w, rev_edge_id) for w, rev_edge_id in graph[v] if w == u),\n",
        "                    None\n",
        "                )\n",
        "                if reverse_edge:\n",
        "                    # Get reverse edge weight\n",
        "                    rev_edge_id = reverse_edge[1]\n",
        "                    w_uv = weights[(u, v, edge_id)]\n",
        "                    w_vu = weights[(v, u, rev_edge_id)]\n",
        "\n",
        "                    if w_uv >= w_vu:\n",
        "                        # Update weight of (u, v)\n",
        "                        updated_weight = w_uv / (w_uv + w_vu)\n",
        "                        updated_graph[u].append((v, edge_id))\n",
        "                        updated_weights[(u, v, edge_id)] = updated_weight\n",
        "                        # Mark (v, u) as processed\n",
        "                        processed_edges.add((v, u, rev_edge_id))\n",
        "                    else:\n",
        "                        # Update weight of (v, u)\n",
        "                        updated_weight = w_vu / (w_uv + w_vu)\n",
        "                        updated_graph[v].append((u, rev_edge_id))\n",
        "                        updated_weights[(v, u, rev_edge_id)] = updated_weight\n",
        "                        # Mark (u, v) as processed\n",
        "                        processed_edges.add((u, v, edge_id))\n",
        "                else:\n",
        "                    # No reverse edge, update the weight to 1\n",
        "                    updated_graph[u].append((v, edge_id))\n",
        "                    updated_weights[(u, v, edge_id)] = 1.0\n",
        "\n",
        "                # Mark this edge as processed\n",
        "                processed_edges.add((u, v, edge_id))\n",
        "\n",
        "    return updated_graph, updated_weights\n",
        "\n",
        "def check_and_readd_edges(graph, removed_edges, n):\n",
        "    \"\"\"\n",
        "    Phase 2: Check and re-add edges if they do not create a cycle.\n",
        "    Handles graphs with parallel edges using edge identifiers.\n",
        "\n",
        "    Args:\n",
        "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
        "        removed_edges (set): Set of removed edges as (u, v, edge_id).\n",
        "        n (int): Number of vertices in the graph.\n",
        "\n",
        "    Returns:\n",
        "        readded_edges (set): Set of edges that were successfully re-added.\n",
        "        remaining_removed_edges (set): Set of edges that could not be re-added.\n",
        "    \"\"\"\n",
        "\n",
        "    def has_path(start, end, graph):\n",
        "        \"\"\"\n",
        "        Helper function to check if there is a path from start to end using DFS.\n",
        "        Avoids cycles when re-adding edges.\n",
        "        \"\"\"\n",
        "        visited = [False] * n\n",
        "        stack = [start]\n",
        "        while stack:\n",
        "            node = stack.pop()\n",
        "            if node == end:\n",
        "                return True\n",
        "            if not visited[node]:\n",
        "                visited[node] = True\n",
        "                stack.extend(neighbor for neighbor, _ in graph[node])  # Add only neighbors\n",
        "        return False\n",
        "\n",
        "    readded_edges = set()\n",
        "    removed_edges_list = sorted(list(removed_edges), reverse=True)\n",
        "\n",
        "    for u, v, edge_id in removed_edges_list:\n",
        "        if not has_path(v, u, graph):  # Only re-add if it doesn't create a cycle\n",
        "            graph[u].append((v, edge_id))\n",
        "            readded_edges.add((u, v, edge_id))\n",
        "\n",
        "    remaining_removed_edges = removed_edges - readded_edges\n",
        "    return readded_edges, remaining_removed_edges\n",
        "def mwfas_synthetic(edges):\n",
        "    \"\"\"\n",
        "    Main function to find Minimum Weighted Feedback Arc Set (MWFAS) in a graph with parallel edges.\n",
        "\n",
        "    :param file_path: Path to the file containing the graph.\n",
        "    :return: A dictionary with metrics, updated graph, removed edges, and their weights.\n",
        "    \"\"\"\n",
        "    # Read the graph and initialize its structure\n",
        "\n",
        "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
        "    graph, weights = initialize_graph(edges)\n",
        "\n",
        "    # Original graph statistics\n",
        "    total_edges = len(edges)\n",
        "    total_weight = sum(w for _, _, w, _ in edges)\n",
        "\n",
        "    # Phase 1: Reduce cycles\n",
        "    removed_edges, removed_weights = find_cycles_and_reduce(graph, weights, n)\n",
        "\n",
        "    # Phase 2: Re-add edges (if applicable)\n",
        "    readded_edges, remaining_removed_edges = check_and_readd_edges(graph, removed_edges, n)\n",
        "\n",
        "    # Compute final metrics\n",
        "    num_removed_edges = len(remaining_removed_edges)\n",
        "    total_removed_weight = sum(removed_weights.get(edge, 0) for edge in remaining_removed_edges)\n",
        "\n",
        "    # Return results\n",
        "    return {\n",
        "        \"total_edges\": total_edges,\n",
        "        \"total_weight\": total_weight,\n",
        "        \"num_removed_edges\": num_removed_edges,\n",
        "        \"removed_weight\": total_removed_weight,\n",
        "        \"final_graph\": graph,\n",
        "        \"removed_edges\": remaining_removed_edges,\n",
        "        \"removed_weights\": {edge: removed_weights.get(edge, 0) for edge in remaining_removed_edges},\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "\n",
        "class DirectedGraph:\n",
        "    def __init__(self, vertices):\n",
        "        self.graph = defaultdict(list)\n",
        "        self.vertices = vertices\n",
        "        self.weights = {}  # Maintain weights dictionary with unique edge keys\n",
        "\n",
        "    def add_edge(self, src, dest, weight=1, edge_id=None):\n",
        "        \"\"\"\n",
        "        Add a directed edge with weight and an optional unique edge_id.\n",
        "        \"\"\"\n",
        "        if edge_id is None:\n",
        "            edge_id = (src, dest, len(self.graph[src]))  # Generate unique edge_id\n",
        "        self.graph[src].append((dest, edge_id))\n",
        "        self.weights[(src, dest, edge_id)] = weight\n",
        "\n",
        "    def get_edges(self):\n",
        "        edges = []\n",
        "        for src in self.graph:\n",
        "            for dest, edge_id in self.graph[src]:\n",
        "                edges.append((src, dest, self.weights[(src, dest, edge_id)]))\n",
        "        return edges\n",
        "\n",
        "    def remove_edge(self, src, dest):\n",
        "        self.graph[src] = [(d, eid) for d, eid in self.graph[src] if d != dest]\n",
        "        self.weights = {key: weight for key, weight in self.weights.items() if key[0] != src or key[1] != dest}\n",
        "\n",
        "    def get_indegree(self):\n",
        "        indegree = {v: 0 for v in range(self.vertices)}\n",
        "        for src in self.graph:\n",
        "            for dest, _ in self.graph[src]:\n",
        "                indegree[dest] += 1\n",
        "        return indegree\n",
        "\n",
        "    def get_outdegree(self):\n",
        "        outdegree = {v: 0 for v in range(self.vertices)}\n",
        "        for src in self.graph:\n",
        "            for dest, _ in self.graph[src]:\n",
        "                outdegree[src] += 1\n",
        "        return outdegree\n",
        "\n",
        "    def eliminate_parallel_arcs(self):\n",
        "        for src in self.graph:\n",
        "            seen = {}\n",
        "            for dest, weight in self.graph[src]:\n",
        "                if dest in seen:\n",
        "                    seen[dest] += self.weights[(src, dest, weight)]\n",
        "                else:\n",
        "                    seen[dest] = self.weights[(src, dest, weight)]\n",
        "            self.graph[src] = [(dest, weight) for dest, weight in seen.items()]\n",
        "\n",
        "    def eliminate_two_cycles(self):\n",
        "        for src in list(self.graph.keys()):\n",
        "            for dest, weight in self.graph[src]:\n",
        "                for back_dest, back_weight in self.graph[dest]:\n",
        "                    if back_dest == src:\n",
        "                        if weight > back_weight:\n",
        "                            self.remove_edge(src, dest)\n",
        "                            self.remove_edge(dest, src)\n",
        "                        elif weight == back_weight:\n",
        "                            self.remove_edge(src, dest)\n",
        "                            self.remove_edge(dest, src)\n",
        "\n",
        "def compute_vertex_rankings(graph, weights, n):\n",
        "    \"\"\"\n",
        "    Compute rankings for the vertices in a DAG with parallel edges.\n",
        "    :param graph: Adjacency list of the DAG with (neighbor, edge_id) pairs.\n",
        "    :param weights: Dictionary of edge weights with keys (u, v, edge_id).\n",
        "    :param n: Total number of vertices in the graph.\n",
        "    :return: A list of rankings for the vertices.\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate in-degrees\n",
        "    in_degree = [0] * n\n",
        "    for u in graph:\n",
        "        for v, _ in graph[u]:  # Ignore edge_id for in-degree calculation\n",
        "            in_degree[v] += 1\n",
        "\n",
        "    # Step 2: Perform topological sort using a min-heap\n",
        "    from heapq import heappop, heappush\n",
        "    min_heap = []\n",
        "    for i in range(n):\n",
        "        if in_degree[i] == 0:\n",
        "            heappush(min_heap, i)\n",
        "\n",
        "    topological_order = []\n",
        "    while min_heap:\n",
        "        current = heappop(min_heap)\n",
        "        topological_order.append(current)\n",
        "        for neighbor, _ in graph[current]:  # Ignore edge_id for topological sort\n",
        "            in_degree[neighbor] -= 1\n",
        "            if in_degree[neighbor] == 0:\n",
        "                heappush(min_heap, neighbor)\n",
        "\n",
        "    # Step 3: Calculate outgoing and incoming edge weight sums for all vertices\n",
        "    outgoing_weights = {v: 0 for v in range(n)}\n",
        "    incoming_weights = {v: 0 for v in range(n)}\n",
        "\n",
        "    for u in graph:\n",
        "        for v, edge_id in graph[u]:\n",
        "            edge_key = (u, v, edge_id)\n",
        "            outgoing_weights[u] += weights.get(edge_key, 0)\n",
        "            incoming_weights[v] += weights.get(edge_key, 0)\n",
        "\n",
        "    # Step 4: Assign rankings\n",
        "    rankings = [-1] * n\n",
        "    current_rank = 0\n",
        "    for vertex in topological_order:\n",
        "        rankings[vertex] = current_rank\n",
        "        current_rank += 1\n",
        "\n",
        "    # Break ties for vertices with the same ranking based on outgoing and incoming edge weights\n",
        "    tied_vertices = sorted(\n",
        "        [(rankings[v], -(outgoing_weights[v] - incoming_weights[v]) /\n",
        "          (outgoing_weights[v] + incoming_weights[v] if outgoing_weights[v] + incoming_weights[v] > 0 else 1), v)\n",
        "         for v in range(n)],\n",
        "        key=lambda x: (x[0], x[1])  # Sort by rank first, then by normalized weight difference\n",
        "    )\n",
        "\n",
        "    scores = [0] * n\n",
        "    for final_rank, (_, _, vertex) in enumerate(tied_vertices):\n",
        "        scores[vertex] = n - final_rank - 1\n",
        "\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIFPsejs457D",
        "outputId": "bd158c80-1127-47b4-9441-f6230a44c79d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing synthetic graph - Nodes: 350, p: 1.0, eta: 0.6, style: uniform, Edges: 61075\n",
            "Evaluating losses before optimization...\n",
            "Before Optimization:\n",
            "Graph Parameters - Nodes: 350, p: 1.0, eta: 0.6, style: uniform\n",
            "Naive Loss: 0.3825787901878357\n",
            "Simple Loss: 1.5303151607513428\n",
            "Ratio Loss: 1.0406372547149658\n",
            "Kendall Tau Loss: 0.5263200982398689\n",
            "Elapsed Time: 6668.6010 seconds\n",
            "Processing synthetic graph - Nodes: 350, p: 1.0, eta: 0.6, style: gamma, Edges: 61075\n",
            "Evaluating losses before optimization...\n",
            "Before Optimization:\n",
            "Graph Parameters - Nodes: 350, p: 1.0, eta: 0.6, style: gamma\n",
            "Naive Loss: 0.40365123748779297\n",
            "Simple Loss: 1.6146049499511719\n",
            "Ratio Loss: 1.0796955823898315\n",
            "Kendall Tau Loss: 0.4325665165779779\n",
            "Elapsed Time: 7192.8082 seconds\n",
            "Processing synthetic graph - Nodes: 350, p: 1.0, eta: 0.7, style: uniform, Edges: 61075\n",
            "Evaluating losses before optimization...\n",
            "Before Optimization:\n",
            "Graph Parameters - Nodes: 350, p: 1.0, eta: 0.7, style: uniform\n",
            "Naive Loss: 0.4122144877910614\n",
            "Simple Loss: 1.6488579511642456\n",
            "Ratio Loss: 1.0943456888198853\n",
            "Kendall Tau Loss: 0.5044781006958656\n",
            "Elapsed Time: 9366.1090 seconds\n",
            "Processing synthetic graph - Nodes: 350, p: 1.0, eta: 0.7, style: gamma, Edges: 61075\n",
            "Evaluating losses before optimization...\n",
            "Before Optimization:\n",
            "Graph Parameters - Nodes: 350, p: 1.0, eta: 0.7, style: gamma\n",
            "Naive Loss: 0.42711421847343445\n",
            "Simple Loss: 1.7084568738937378\n",
            "Ratio Loss: 1.1280064582824707\n",
            "Kendall Tau Loss: 0.41242734343020876\n",
            "Elapsed Time: 8833.6727 seconds\n",
            "Processing synthetic graph - Nodes: 350, p: 1.0, eta: 0.8, style: uniform, Edges: 61075\n",
            "Evaluating losses before optimization...\n",
            "Before Optimization:\n",
            "Graph Parameters - Nodes: 350, p: 1.0, eta: 0.8, style: uniform\n",
            "Naive Loss: 0.4343675673007965\n",
            "Simple Loss: 1.737470269203186\n",
            "Ratio Loss: 1.128852367401123\n",
            "Kendall Tau Loss: 0.46989766680311085\n",
            "Elapsed Time: 10040.4421 seconds\n",
            "Processing synthetic graph - Nodes: 350, p: 1.0, eta: 0.8, style: gamma, Edges: 61075\n",
            "Evaluating losses before optimization...\n",
            "Before Optimization:\n",
            "Graph Parameters - Nodes: 350, p: 1.0, eta: 0.8, style: gamma\n",
            "Naive Loss: 0.44733524322509766\n",
            "Simple Loss: 1.7893409729003906\n",
            "Ratio Loss: 1.1583330631256104\n",
            "Kendall Tau Loss: 0.379746213671715\n",
            "Elapsed Time: 18963.6921 seconds\n",
            "Experiment completed. Results saved to synthetic_experiment_results.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def graph_to_adjacency_matrix(graph, weights, n):\n",
        "    \"\"\"\n",
        "    Convert the graph to an adjacency matrix with weights.\n",
        "    :param graph: Adjacency list of the DAG with (neighbor, edge_id) pairs.\n",
        "    :param weights: Dictionary of edge weights with keys (u, v, edge_id).\n",
        "    :param n: Total number of vertices in the graph.\n",
        "    :return: An adjacency matrix (n x n) with weights as a PyTorch tensor.\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    adjacency_matrix = torch.zeros((n, n), dtype=torch.float32)\n",
        "\n",
        "    for u in graph:\n",
        "        for v, edge_id in graph[u]:\n",
        "            edge_key = (u, v, edge_id)\n",
        "            adjacency_matrix[u, v] = weights.get(edge_key, 0)\n",
        "\n",
        "    return adjacency_matrix\n",
        "def reorder_floats(x):\n",
        "    n = len(x)\n",
        "    random_floats = np.random.uniform(0,  2*n/3, n)\n",
        "    y = np.zeros(n)\n",
        "    for idx, val in enumerate(np.argsort(x)):\n",
        "        y[val] = sorted(random_floats)[idx]\n",
        "    return y\n",
        "\n",
        "def calculate_upset_loss(adjacency_matrix, scores, style='ratio', margin=0.01):\n",
        "    \"\"\"\n",
        "    Calculate the upset loss for the graph rankings using adjacency matrix and scores.\n",
        "\n",
        "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
        "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
        "    :param style: Type of upset loss ('naive', 'simple', 'ratio', or 'margin').\n",
        "    :param margin: Margin for margin loss (default: 0.01).\n",
        "    :return: Torch FloatTensor upset loss value.\n",
        "    \"\"\"\n",
        "    epsilon = 1e-8  # For numerical stability\n",
        "\n",
        "    # Ensure scores are 2D\n",
        "    if scores.ndim == 1:\n",
        "        scores = scores.view(-1, 1)\n",
        "\n",
        "    # Skew-symmetric pairwise comparison matrix (M)\n",
        "    M1 = adjacency_matrix - adjacency_matrix.T\n",
        "\n",
        "    # Normalize scores to [0, 1] range\n",
        "    normalized_scores = scores\n",
        "\n",
        "    # Pairwise score differences (T)\n",
        "    T1 = normalized_scores - normalized_scores.T\n",
        "\n",
        "    # Edge mask: Only consider meaningful edges (where M != 0)\n",
        "    edge_mask = M1 != 0\n",
        "\n",
        "    if style == 'ratio':\n",
        "        min_upset = float('inf')  # Initialize with a large value\n",
        "\n",
        "        for _ in range(40):\n",
        "            # Generate reordered scores using reorder_floats\n",
        "            if _==0:\n",
        "                reordered_scores=scores\n",
        "            else:\n",
        "                reordered_scores = torch.FloatTensor(reorder_floats(scores.flatten().tolist()))\n",
        "            reordered_scores = reordered_scores.view(-1, 1)\n",
        "\n",
        "            # Compute T2 for normalized scores\n",
        "            T2 = reordered_scores + reordered_scores.T + epsilon\n",
        "            T = torch.div(T1, T2)\n",
        "            M2 = adjacency_matrix + adjacency_matrix.T + epsilon\n",
        "            M3 = torch.div(M1, M2)  # Normalize the adjacency matrix\n",
        "\n",
        "            # Compute ratio-based upset loss for this iteration\n",
        "            powers = torch.pow((M3 - T)[edge_mask], 2)\n",
        "            upset_loss = torch.sum(powers) / torch.sum(edge_mask)\n",
        "\n",
        "            # Track the minimum upset loss\n",
        "            min_upset = min(min_upset, upset_loss.item())\n",
        "\n",
        "        return torch.tensor(min_upset)\n",
        "\n",
        "    elif style == 'naive':\n",
        "        upset = torch.sum(torch.sign(T1[edge_mask]) != torch.sign(M1[edge_mask])) / torch.sum(edge_mask)\n",
        "\n",
        "    elif style == 'simple':\n",
        "        upset = torch.mean((torch.sign(T1[edge_mask]) - torch.sign(M1[edge_mask]))**2)\n",
        "\n",
        "    elif style == 'margin':\n",
        "        upset = torch.mean(torch.nn.functional.relu(-M1[edge_mask] * (T1[edge_mask] - margin)))\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported style: {style}\")\n",
        "\n",
        "    return upset\n",
        "def compute_ratio_upset_loss(adjacency_matrix, scores, epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    Compute the ratio upset loss for the graph rankings using adjacency matrix and scores.\n",
        "\n",
        "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
        "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
        "    :param epsilon: Small value for numerical stability (default: 1e-8).\n",
        "    :return: Torch FloatTensor ratio upset loss value.\n",
        "    \"\"\"\n",
        "    # Ensure scores are 2D\n",
        "    if scores.ndim == 1:\n",
        "        scores = scores.view(-1, 1)\n",
        "\n",
        "    # Skew-symmetric pairwise comparison matrix (M)\n",
        "    M1 = adjacency_matrix - adjacency_matrix.T\n",
        "\n",
        "    # Pairwise score differences (T1)\n",
        "    T1 = scores - scores.T\n",
        "\n",
        "    # Edge mask: Only consider meaningful edges (where M1 != 0)\n",
        "    edge_mask = M1 != 0\n",
        "\n",
        "    # Compute T2 for normalized scores\n",
        "    T2 = scores + scores.T + epsilon\n",
        "    T = torch.div(T1, T2)\n",
        "\n",
        "    # Normalize M1 using adjacency matrix\n",
        "    M2 = adjacency_matrix + adjacency_matrix.T + epsilon\n",
        "    M3 = torch.div(M1, M2)  # Normalize the adjacency matrix\n",
        "\n",
        "    # Compute ratio upset loss\n",
        "    powers = torch.pow((M3 - T)[edge_mask], 2)\n",
        "    upset_loss = torch.sum(powers) / torch.sum(edge_mask)\n",
        "\n",
        "    return upset_loss\n",
        "def minimize_ratio_upset_loss(adjacency_matrix, scores, epsilon=1e-2, max_time=120):\n",
        "    \"\"\"\n",
        "    Perform optimization to minimize the ratio upset loss, ensuring naive and simple losses do not worsen.\n",
        "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
        "    :param scores: Initial scores for optimization.\n",
        "    :param epsilon: Small value for numerical stability (default: 1e-2).\n",
        "    :param max_time: Maximum time for optimization in seconds.\n",
        "    :return: Tuple (optimal_scores, minimized_loss)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    from scipy.optimize import minimize\n",
        "    import time\n",
        "\n",
        "    n = adjacency_matrix.shape[0]\n",
        "\n",
        "    # Compute initial losses\n",
        "    initial_scores = scores.clone().detach().view(-1).numpy()\n",
        "    initial_losses = {\n",
        "        \"naive\": calculate_upset_loss(adjacency_matrix, scores.view(-1, 1), style=\"naive\"),\n",
        "        \"simple\": calculate_upset_loss(adjacency_matrix, scores.view(-1, 1), style=\"simple\"),\n",
        "    }\n",
        "\n",
        "    # Objective function\n",
        "    def objective_function(updated_scores, adjacency_matrix, initial_losses):\n",
        "        updated_scores = torch.tensor(updated_scores, dtype=torch.float32).view(-1, 1)\n",
        "        ratio_loss = calculate_upset_loss(adjacency_matrix, updated_scores, style=\"ratio\")\n",
        "        naive_loss = calculate_upset_loss(adjacency_matrix, updated_scores, style=\"naive\")\n",
        "        simple_loss = calculate_upset_loss(adjacency_matrix, updated_scores, style=\"simple\")\n",
        "\n",
        "        penalty = 0\n",
        "        if naive_loss > initial_losses[\"naive\"]:\n",
        "            penalty += naive_loss - initial_losses[\"naive\"]\n",
        "        if simple_loss > initial_losses[\"simple\"]:\n",
        "            penalty += simple_loss - initial_losses[\"simple\"]\n",
        "\n",
        "        return ratio_loss + 100 * penalty\n",
        "\n",
        "    # Timer callback\n",
        "    class TimerCallback:\n",
        "        def __init__(self, max_time, objective_function, adjacency_matrix, initial_losses):\n",
        "            self.start_time = time.time()\n",
        "            self.max_time = max_time\n",
        "            self.iterations = 0  # Track iterations\n",
        "            self.objective_function = objective_function\n",
        "            self.adjacency_matrix = adjacency_matrix\n",
        "            self.initial_losses = initial_losses\n",
        "            self.min_loss = float(\"inf\")  # Track minimum loss\n",
        "\n",
        "        def __call__(self, xk, *args, **kwargs):\n",
        "            self.iterations += 1\n",
        "\n",
        "            # Compute the objective function value\n",
        "            current_loss = self.objective_function(\n",
        "                xk, self.adjacency_matrix, self.initial_losses\n",
        "            )\n",
        "\n",
        "            # Update and print the minimum loss found so far\n",
        "            self.min_loss = min(self.min_loss, current_loss)\n",
        "            print(f\"Iteration {self.iterations}: Minimum loss so far: {self.min_loss:.6f}\")\n",
        "\n",
        "            # Stop optimization if the time limit is exceeded\n",
        "            if time.time() - self.start_time > self.max_time:\n",
        "                print(\"Time limit exceeded, stopping optimization.\")\n",
        "                raise StopIteration  # Signal COBYLA to stop\n",
        "\n",
        "    # Create the callback instance\n",
        "    callback = TimerCallback(\n",
        "        max_time=max_time,\n",
        "        objective_function=objective_function,\n",
        "        adjacency_matrix=adjacency_matrix,\n",
        "        initial_losses=initial_losses,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Run optimization with COBYLA\n",
        "        result = minimize(\n",
        "            fun=objective_function,\n",
        "            x0=initial_scores,\n",
        "            method=\"COBYLA\",\n",
        "            args=(adjacency_matrix, initial_losses),  # Pass required arguments\n",
        "            options={\"maxiter\": 500, \"disp\": False},\n",
        "            callback=callback,  # Logs the minimum loss after each iteration\n",
        "        )\n",
        "    except StopIteration:\n",
        "        print(\"Optimization stopped early due to time limit.\")\n",
        "\n",
        "    # Extract results\n",
        "    optimal_scores = result.x\n",
        "    minimized_loss = result.fun\n",
        "\n",
        "    print(f\"Optimization completed after {callback.iterations} iterations.\")\n",
        "    print(f\"Final minimum loss: {callback.min_loss:.6f}\")\n",
        "    return optimal_scores, minimized_loss\n",
        "\n",
        "\n",
        "\n",
        "def compute_ratio_upset_loss(adjacency_matrix, scores, epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    Compute the ratio upset loss for the graph rankings using adjacency matrix and scores.\n",
        "\n",
        "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
        "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
        "    :param epsilon: Small value for numerical stability (default: 1e-8).\n",
        "    :return: Torch FloatTensor ratio upset loss value.\n",
        "    \"\"\"\n",
        "    if scores.ndim == 1:\n",
        "        scores = scores.view(-1, 1)\n",
        "\n",
        "    M1 = adjacency_matrix - adjacency_matrix.T\n",
        "    T1 = scores - scores.T\n",
        "    edge_mask = M1 != 0\n",
        "\n",
        "    T2 = scores + scores.T + epsilon\n",
        "    T = torch.div(T1, T2)\n",
        "\n",
        "    M2 = adjacency_matrix + adjacency_matrix.T + epsilon\n",
        "    M3 = torch.div(M1, M2)\n",
        "\n",
        "    powers = torch.pow((M3 - T)[edge_mask], 2)\n",
        "    upset_loss = torch.sum(powers) / torch.sum(edge_mask)\n",
        "\n",
        "    return upset_loss\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "def plot_ratio_loss(adjacency_matrix, scores, index, lower_bound, upper_bound, steps=100, output_folder=\"loss_plots\"):\n",
        "    \"\"\"\n",
        "    Plot and save the compute_ratio_upset_loss function for scores[index] between lower_bound and upper_bound.\n",
        "\n",
        "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
        "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
        "    :param index: Index of the score to vary.\n",
        "    :param lower_bound: Lower bound for the score value.\n",
        "    :param upper_bound: Upper bound for the score value.\n",
        "    :param steps: Number of steps for sampling the range.\n",
        "    :param output_folder: Directory to save the plots.\n",
        "    \"\"\"\n",
        "    # Ensure the output folder exists\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    x_values = torch.linspace(lower_bound, upper_bound, steps)\n",
        "    y_values = []\n",
        "\n",
        "    original_score = scores[index].item()\n",
        "\n",
        "    for x in x_values:\n",
        "        scores[index] = x\n",
        "        loss = compute_ratio_upset_loss(adjacency_matrix, scores)\n",
        "        y_values.append(loss.item())\n",
        "\n",
        "    # Restore the original score\n",
        "    scores[index] = original_score\n",
        "\n",
        "    # Plot the graph\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x_values.numpy(), y_values, label=f\"Loss vs. scores[{index}]\")\n",
        "    plt.xlabel(f\"scores[{index}] value\")\n",
        "    plt.ylabel(\"Ratio Upset Loss\")\n",
        "    plt.title(f\"Compute Ratio Upset Loss for Varying scores[{index}]\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Save the plot with a unique name\n",
        "    plot_path = os.path.join(output_folder, f\"loss_plot_index_{index}.png\")\n",
        "    counter = 1\n",
        "    while os.path.exists(plot_path):\n",
        "        plot_path = os.path.join(output_folder, f\"loss_plot_index_{index}_{counter}.png\")\n",
        "        counter += 1\n",
        "\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Plot saved: {plot_path}\")\n",
        "\n",
        "\n",
        "def trinary_search_optimize(adjacency_matrix, scores, index, lower_bound, upper_bound, epsilon=1e-8, steps=100):\n",
        "    \"\"\"\n",
        "    Perform ternary search to find the optimal score value for a given index that minimizes ratio upset loss.\n",
        "\n",
        "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
        "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
        "    :param index: Index of the score to optimize.\n",
        "    :param lower_bound: Lower bound for the score value.\n",
        "    :param upper_bound: Upper bound for the score value.\n",
        "    :param epsilon: Small value for numerical stability.\n",
        "    :param steps: Number of steps for ternary search.\n",
        "    :return: Optimal score value for the given index.\n",
        "    \"\"\"\n",
        "    for _ in range(steps):\n",
        "        mid1 = lower_bound + (upper_bound - lower_bound) / 3.0\n",
        "        mid2 = upper_bound - (upper_bound - lower_bound) / 3.0\n",
        "\n",
        "        scores[index] = mid1\n",
        "        loss1 = compute_ratio_upset_loss(adjacency_matrix, scores)\n",
        "\n",
        "        scores[index] = mid2\n",
        "        loss2 = compute_ratio_upset_loss(adjacency_matrix, scores)\n",
        "\n",
        "        if loss1 < loss2:\n",
        "            upper_bound = mid2\n",
        "        elif loss1 > loss2:\n",
        "            lower_bound = mid1\n",
        "        else:\n",
        "            lower_bound = mid1\n",
        "            upper_bound = mid2\n",
        "\n",
        "        # Break if the range is small enough\n",
        "        if upper_bound - lower_bound < epsilon:\n",
        "            break\n",
        "\n",
        "    # After the loop, check the losses at lower_bound, upper_bound, and midpoint\n",
        "    mid_point = (lower_bound + upper_bound) / 2.0\n",
        "    scores[index] = mid_point\n",
        "    mid_point_loss = compute_ratio_upset_loss(adjacency_matrix, scores)\n",
        "\n",
        "    scores[index] = lower_bound\n",
        "    lower_bound_loss = compute_ratio_upset_loss(adjacency_matrix, scores)\n",
        "\n",
        "    scores[index] = upper_bound\n",
        "    upper_bound_loss = compute_ratio_upset_loss(adjacency_matrix, scores)\n",
        "\n",
        "    # Find the minimum loss and corresponding score\n",
        "    min_loss = min(mid_point_loss, lower_bound_loss, upper_bound_loss)\n",
        "    if min_loss == lower_bound_loss:\n",
        "        optimal_score = lower_bound\n",
        "    elif min_loss == upper_bound_loss:\n",
        "        optimal_score = upper_bound\n",
        "    else:\n",
        "        optimal_score = mid_point\n",
        "\n",
        "    scores[index] = optimal_score\n",
        "    return scores[index]\n",
        "\n",
        "\n",
        "from scipy.optimize import minimize_scalar\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def minimize_ratio_loss(adjacency_matrix, scores):\n",
        "    \"\"\"\n",
        "    Minimize the ratio upset loss by iteratively optimizing each score using binary search.\n",
        "\n",
        "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
        "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
        "    :return: Optimized scores.\n",
        "    \"\"\"\n",
        "    scores = scores.clone()  # Create a copy to avoid modifying the original\n",
        "\n",
        "    for _ in range(40):\n",
        "        sorted_indices = torch.argsort(scores.squeeze())\n",
        "        print(_)\n",
        "        for i in range(len(sorted_indices)):\n",
        "\n",
        "\n",
        "\n",
        "            index = sorted_indices[i]\n",
        "\n",
        "            if i == 0:\n",
        "                lower_bound = 0\n",
        "                upper_bound = scores[sorted_indices[i + 1]].item()\n",
        "            elif i == len(sorted_indices) - 1:\n",
        "                lower_bound = scores[sorted_indices[i - 1]].item()\n",
        "                upper_bound = scores.max().item() + 10  # Extend beyond max for the last element\n",
        "            else:\n",
        "                lower_bound = scores[sorted_indices[i - 1]].item()\n",
        "                upper_bound = scores[sorted_indices[i + 1]].item()\n",
        "\n",
        "            # Perform binary search optimization for the current score\n",
        "            scores[index] = trinary_search_optimize(adjacency_matrix, scores, index, lower_bound, upper_bound)\n",
        "          #  print(\"i am here\")\n",
        "    return scores\n",
        "\n",
        "# Example usage\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def generate_ero_graph(n: int, p: float, eta: float, style: str = 'uniform'):\n",
        "    \"\"\"\n",
        "    Generates an Erdős-Rényi Outliers (ERO) model graph in a format compatible with initialize_graph() and mwfas().\n",
        "\n",
        "    Args:\n",
        "        n (int): Number of nodes.\n",
        "        p (float): Edge probability (sparsity).\n",
        "        eta (float): Noise level (between 0 and 1).\n",
        "        style (str): How to generate ground-truth scores ('uniform' or 'gamma').\n",
        "\n",
        "    Returns:\n",
        "        list: List of edges (start, end, weight, edge_id) for MWFAS.\n",
        "        np.array: Ground-truth ranking of nodes.\n",
        "    \"\"\"\n",
        "    # Generate node scores\n",
        "    if style == 'uniform':\n",
        "        scores = np.random.rand(n, 1)\n",
        "        R_noise = np.random.rand(n, n) * 2 - 1\n",
        "    elif style == 'gamma':\n",
        "        scores = np.random.gamma(shape=0.5, scale=1, size=(n, 1))\n",
        "        R_noise = np.random.rand(n, n) * 4 - 2  # Gamma noise\n",
        "\n",
        "    # Compute ground-truth ranking\n",
        "    labels = rankdata(-scores.flatten(), 'min')\n",
        "\n",
        "    # Generate pairwise comparisons matrix\n",
        "    R_GT = scores - scores.T  # True pairwise differences\n",
        "    R_choice = np.random.rand(n, n)\n",
        "    R = np.zeros((n, n))\n",
        "    R[R_choice <= p] = R_noise[R_choice <= p]  # Assign noisy comparisons\n",
        "    R[R_choice <= p * (1 - eta)] = R_GT[R_choice <= p * (1 - eta)]  # Assign correct comparisons\n",
        "\n",
        "    # Ensure antisymmetry\n",
        "    lower_ind = np.tril_indices(n)\n",
        "    diag_ind = np.diag_indices(n)\n",
        "    R[lower_ind] = -R.T[lower_ind]\n",
        "    R[diag_ind] = 0\n",
        "    R[R < 0] = 0  # Ensure positive weights\n",
        "\n",
        "    # Convert matrix to edge list format for mwfas (edge_id is the last element)\n",
        "    edges = []\n",
        "    edge_id = 0\n",
        "    R_coo = sp.csr_matrix(R).tocoo()\n",
        "    for u, v, w in zip(R_coo.row, R_coo.col, R_coo.data):\n",
        "        edges.append((int(u), int(v), float(w), edge_id))\n",
        "        edge_id += 1\n",
        "\n",
        "    return edges, labels\n",
        "\n",
        "\n",
        "\n",
        "def kendall_tau_loss(true_ranking, predicted_ranking):\n",
        "    \"\"\"\n",
        "    Computes Kendall tau loss, which measures ranking disagreement.\n",
        "\n",
        "    Args:\n",
        "        true_ranking (np.array): Ground-truth ranking.\n",
        "        predicted_ranking (np.array): Computed ranking.\n",
        "\n",
        "    Returns:\n",
        "        float: Kendall tau loss (fraction of discordant pairs).\n",
        "    \"\"\"\n",
        "    n = len(true_ranking)\n",
        "    true_order = np.argsort(true_ranking)\n",
        "    predicted_order = np.argsort(predicted_ranking)\n",
        "    discordant_pairs = sum(\n",
        "        (true_order[i] > true_order[j]) != (predicted_order[i] > predicted_order[j])\n",
        "        for i in range(n)\n",
        "        for j in range(i + 1, n)\n",
        "    )\n",
        "    total_pairs = n * (n - 1) / 2\n",
        "    return discordant_pairs / total_pairs\n",
        "import time\n",
        "import torch\n",
        "import pandas as pd\n",
        "from scipy.stats import kendalltau\n",
        "\n",
        "# Experiment parameters\n",
        "num_nodes = 350  # Fixed number of nodes for all graphs\n",
        "probabilities = [ 1.0]  # Sparsity values\n",
        "noise_levels = [ 0.6, 0.7, 0.8]  # Noise levels\n",
        "styles = ['uniform', 'gamma']  # Score distributions\n",
        "\n",
        "# Initialize results storage\n",
        "results = []\n",
        "\n",
        "# Iterate over synthetic dataset configurations\n",
        "for p in probabilities:\n",
        "    for eta in noise_levels:\n",
        "        for style in styles:\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Generate ERO graph\n",
        "            edges, true_ranking = generate_ero_graph(n=num_nodes, p=p, eta=eta, style=style)\n",
        "\n",
        "            if not edges:\n",
        "                print(f\"Warning: No edges generated for Nodes: {num_nodes}, p: {p}, eta: {eta}, style: {style}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing synthetic graph - Nodes: {num_nodes}, p: {p}, eta: {eta}, style: {style}, Edges: {len(edges)}\")\n",
        "\n",
        "            # Step 1: Initialize the graph\n",
        "            init_graph, init_weights = initialize_graph(edges)\n",
        "\n",
        "            # Step 2: Ensure the graph is a DAG by removing cycles\n",
        "            result = mwfas_synthetic(edges)\n",
        "            new_graph = result['final_graph']\n",
        "            new_weights = {key: value for key, value in init_weights.items() if key not in result['removed_weights']}\n",
        "\n",
        "            if not new_graph:\n",
        "                print(f\"Warning: Graph is empty after cycle removal for Nodes: {num_nodes}, p: {p}, eta: {eta}, style: {style}\")\n",
        "                continue\n",
        "\n",
        "            # Step 3: Compute rankings for the vertices using the modified graph\n",
        "            final_rankings = compute_vertex_rankings(new_graph, new_weights, num_nodes)\n",
        "\n",
        "            if len(final_rankings) != num_nodes:\n",
        "                print(f\"Warning: Mismatch in ranking size for Nodes: {num_nodes}, p: {p}, eta: {eta}, style: {style}\")\n",
        "                continue\n",
        "\n",
        "            # Step 4: Evaluate upset losses before optimization\n",
        "            scores = torch.FloatTensor(final_rankings).view(-1, 1)\n",
        "            adjacency_matrix = graph_to_adjacency_matrix(init_graph, init_weights, num_nodes)\n",
        "\n",
        "            if adjacency_matrix.numel() == 0:\n",
        "                print(f\"Warning: Empty adjacency matrix for Nodes: {num_nodes}, p: {p}, eta: {eta}, style: {style}\")\n",
        "                continue\n",
        "\n",
        "            print(\"Evaluating losses before optimization...\")\n",
        "            naive_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='naive')\n",
        "            simple_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='simple')\n",
        "            ratio_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='ratio')\n",
        "\n",
        "            # Compute Kendall Tau Loss using scipy\n",
        "            kendall_tau_before, _ = kendalltau(true_ranking, [-rank for rank in final_rankings])\n",
        "\n",
        "\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "\n",
        "            print(\"Before Optimization:\")\n",
        "            print(f\"Graph Parameters - Nodes: {num_nodes}, p: {p}, eta: {eta}, style: {style}\")\n",
        "            print(f\"Naive Loss: {naive_loss_before}\")\n",
        "            print(f\"Simple Loss: {simple_loss_before}\")\n",
        "            print(f\"Ratio Loss: {ratio_loss_before}\")\n",
        "            print(f\"Kendall Tau Loss: {kendall_tau_before}\")\n",
        "            print(f\"Elapsed Time: {elapsed_time:.4f} seconds\")\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                \"Nodes\": num_nodes,\n",
        "                \"p\": p,\n",
        "                \"eta\": eta,\n",
        "                \"Style\": style,\n",
        "                \"Naive Loss\": naive_loss_before.item(),\n",
        "                \"Simple Loss\": simple_loss_before.item(),\n",
        "                \"Ratio Loss\": ratio_loss_before.item(),\n",
        "                \"Kendall Tau Loss\": kendall_tau_before,\n",
        "                \"Elapsed Time\": elapsed_time\n",
        "            })\n",
        "\n",
        "# Convert results to DataFrame and save\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"synthetic_experiment_results.csv\", index=False)\n",
        "print(\"Experiment completed. Results saved to synthetic_experiment_results.csv\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}