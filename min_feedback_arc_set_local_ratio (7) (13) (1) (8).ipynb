{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60769118-7444-47ac-8af5-4dd96e7fbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_graph(file_path):\n",
    "    \"\"\"\n",
    "    Reads a weighted directed graph from a file. Each line contains three values:\n",
    "    start vertex, end vertex, and edge weight.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing the graph.\n",
    "\n",
    "    Returns:\n",
    "        edges (list): List of tuples representing directed edges (start, end, weight, edge_id).\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    edge_id = 0  # Unique identifier for each edge\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line:  # Skip empty lines\n",
    "                    start, end, weight = map(float, line.split())\n",
    "                    edges.append((int(start), int(end), weight, edge_id))\n",
    "                    edge_id += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading graph: {e}\")\n",
    "    return edges\n",
    "\n",
    "\n",
    "def initialize_graph(edges):\n",
    "    \"\"\"\n",
    "    Converts edge list to an adjacency list and weights dictionary.\n",
    "    Handles parallel edges by including edge_id as part of the structure.\n",
    "\n",
    "    Args:\n",
    "        edges (list): List of edges as (start, end, weight, edge_id).\n",
    "\n",
    "    Returns:\n",
    "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
    "        weights (dict): Dictionary mapping (start, end, edge_id) to weights.\n",
    "    \"\"\"\n",
    "    graph = defaultdict(list)\n",
    "    weights = {}\n",
    "    for u, v, w, edge_id in edges:\n",
    "        graph[u].append((v, edge_id))\n",
    "        weights[(u, v, edge_id)] = w\n",
    "    return graph, weights\n",
    "\n",
    "\n",
    "def find_cycles_and_reduce(graph, weights, n):\n",
    "    \"\"\"\n",
    "    Phase 1: Find cycles and reduce weights using a copy.\n",
    "    Handles graphs with parallel edges by considering edge identifiers.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
    "        weights (dict): Dictionary mapping (u, v, edge_id) to weights.\n",
    "        n (int): Number of vertices in the graph.\n",
    "\n",
    "    Returns:\n",
    "        removed_edges (set): Set of removed edges as (u, v, edge_id).\n",
    "        removed_weights (dict): Dictionary of removed edges with their original weights.\n",
    "    \"\"\"\n",
    "    weights_copy = weights.copy()  # Work with a copy of weights\n",
    "    removed_edges = set()\n",
    "    removed_weights = {}\n",
    "\n",
    "    while True:\n",
    "        cycle = find_cycle(graph, n)  # Modified `find_cycle` returns edges with edge_id\n",
    "        if not cycle:  # No cycle found\n",
    "            break\n",
    "\n",
    "        # Ensure all edges in the cycle exist in the weights dictionary\n",
    "        cycle = [(u, v, edge_id) for u, v, edge_id in cycle if (u, v, edge_id) in weights_copy]\n",
    "\n",
    "        if not cycle:  # If no valid cycle exists, continue\n",
    "            continue\n",
    "\n",
    "        # Find the minimum weight in the cycle\n",
    "        min_weight = min(weights_copy[(u, v, edge_id)] for u, v, edge_id in cycle)\n",
    "\n",
    "        for u, v, edge_id in cycle:\n",
    "            weights_copy[(u, v, edge_id)] -= min_weight\n",
    "            if weights_copy[(u, v, edge_id)] <= 0:\n",
    "                # Ensure the edge is in the graph before removing\n",
    "                if (v, edge_id) in graph[u]:\n",
    "                    graph[u].remove((v, edge_id))\n",
    "                    removed_edges.add((u, v, edge_id))\n",
    "                    removed_weights[(u, v, edge_id)] = weights[(u, v, edge_id)]\n",
    "\n",
    "    return removed_edges, removed_weights\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def find_cycle(graph, n):\n",
    "    \"\"\"\n",
    "    Detect a cycle in the graph using DFS and return the cycle as a list of edges.\n",
    "    Handles parallel edges and cycles of length 2 caused by reverse edges.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
    "        n (int): Number of vertices in the graph.\n",
    "\n",
    "    Returns:\n",
    "        cycle (list): List of edges forming the cycle, or None if no cycle is found.\n",
    "    \"\"\"\n",
    "    visited = [False] * n\n",
    "    stack = [False] * n\n",
    "    parent = [-1] * n\n",
    "    edge_to_parent = {}  # Map to track edge_id for cycle reconstruction\n",
    "\n",
    "    def dfs(v):\n",
    "        visited[v] = True\n",
    "        stack[v] = True\n",
    "        for neighbor, edge_id in graph[v]:\n",
    "            if not visited[neighbor]:\n",
    "                parent[neighbor] = v\n",
    "                edge_to_parent[neighbor] = edge_id\n",
    "                cycle = dfs(neighbor)\n",
    "                if cycle:\n",
    "                    return cycle\n",
    "            elif stack[neighbor]:\n",
    "                # Found a cycle, reconstruct it\n",
    "                cycle = []\n",
    "                current = v\n",
    "                while current != neighbor:\n",
    "                    if current not in edge_to_parent:\n",
    "                        break  # Avoid KeyError if edge metadata is missing\n",
    "                    cycle.append((parent[current], current, edge_to_parent[current]))\n",
    "                    current = parent[current]\n",
    "\n",
    "                # Handle the root of the cycle\n",
    "                if neighbor in edge_to_parent and parent[neighbor] != -1:\n",
    "                    cycle.append((parent[neighbor], neighbor, edge_to_parent[neighbor]))\n",
    "                return cycle\n",
    "\n",
    "        stack[v] = False\n",
    "        return None\n",
    "\n",
    "    # Detect length-2 cycles caused by reverse edges\n",
    "    for u in list(graph):  # Use list(graph) to iterate over a static copy of keys\n",
    "        for neighbor, edge_id1 in graph[u]:\n",
    "            for neighbor_of_neighbor, edge_id2 in graph[neighbor]:\n",
    "                if neighbor_of_neighbor == u and edge_id1 != edge_id2:\n",
    "                    # Found a length-2 cycle\n",
    "                    return [(u, neighbor, edge_id1), (neighbor, u, edge_id2)]\n",
    "\n",
    "    # Run DFS for longer cycles\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            cycle = dfs(i)\n",
    "            if cycle:\n",
    "                return cycle\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def update_edge_weights(graph, weights):\n",
    "    \"\"\"\n",
    "    Updates the weights of the edges in the graph based on the specified rule.\n",
    "    If both (i, j) and (j, i) edges exist, the smaller weight is removed, and the larger weight is updated to:\n",
    "    w_{i,j} / (w_{i,j} + w_{j,i}).\n",
    "    If only one edge exists, its weight is updated to 1.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
    "        weights (dict): Dictionary mapping (start, end, edge_id) to weights.\n",
    "\n",
    "    Returns:\n",
    "        updated_graph (dict): Updated adjacency list.\n",
    "        updated_weights (dict): Updated weights dictionary.\n",
    "    \"\"\"\n",
    "    processed_edges = set()  # Keep track of processed edges\n",
    "    updated_graph = defaultdict(list)\n",
    "    updated_weights = {}\n",
    "\n",
    "    # Iterate over a copy of the graph's keys to avoid modification during iteration\n",
    "    for u in list(graph.keys()):\n",
    "        for v, edge_id in graph[u]:\n",
    "            if (u, v, edge_id) not in processed_edges:\n",
    "                reverse_edge = next(\n",
    "                    ((w, rev_edge_id) for w, rev_edge_id in graph[v] if w == u),\n",
    "                    None\n",
    "                )\n",
    "                if reverse_edge:\n",
    "                    # Get reverse edge weight\n",
    "                    rev_edge_id = reverse_edge[1]\n",
    "                    w_uv = weights[(u, v, edge_id)]\n",
    "                    w_vu = weights[(v, u, rev_edge_id)]\n",
    "\n",
    "                    if w_uv >= w_vu:\n",
    "                        # Update weight of (u, v)\n",
    "                        updated_weight = w_uv / (w_uv + w_vu)\n",
    "                        updated_graph[u].append((v, edge_id))\n",
    "                        updated_weights[(u, v, edge_id)] = updated_weight\n",
    "                        # Mark (v, u) as processed\n",
    "                        processed_edges.add((v, u, rev_edge_id))\n",
    "                    else:\n",
    "                        # Update weight of (v, u)\n",
    "                        updated_weight = w_vu / (w_uv + w_vu)\n",
    "                        updated_graph[v].append((u, rev_edge_id))\n",
    "                        updated_weights[(v, u, rev_edge_id)] = updated_weight\n",
    "                        # Mark (u, v) as processed\n",
    "                        processed_edges.add((u, v, edge_id))\n",
    "                else:\n",
    "                    # No reverse edge, update the weight to 1\n",
    "                    updated_graph[u].append((v, edge_id))\n",
    "                    updated_weights[(u, v, edge_id)] = 1.0\n",
    "\n",
    "                # Mark this edge as processed\n",
    "                processed_edges.add((u, v, edge_id))\n",
    "\n",
    "    return updated_graph, updated_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def find_minimum_weight_cycle(graph, weights, n):\n",
    "    \"\"\"\n",
    "    Find the minimum weight cycle in the graph using Floyd-Warshall.\n",
    "\n",
    "    :param graph: Adjacency list representation of the graph.\n",
    "    :param weights: Dictionary of edge weights.\n",
    "    :param n: Total number of vertices in the graph.\n",
    "    :return: List of edges representing the minimum weight cycle.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize distance and predecessor matrices\n",
    "    dist = np.full((n, n), float('inf'))\n",
    "    pred = [[-1 for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "    # Fill in the distances based on edge weights\n",
    "    for u in range(n):\n",
    "        dist[u][u] = 0\n",
    "        if u in graph:\n",
    "            for v in graph[u]:\n",
    "                dist[u][v] = weights.get((u, v), float('inf'))\n",
    "                pred[u][v] = u\n",
    "\n",
    "    # Step 2: Run Floyd-Warshall algorithm\n",
    "    for k in range(n):\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if dist[i][j] > dist[i][k] + dist[k][j]:\n",
    "                    dist[i][j] = dist[i][k] + dist[k][j]\n",
    "                    pred[i][j] = pred[k][j]\n",
    "\n",
    "    # Step 3: Find the minimum weight cycle\n",
    "    min_cycle_weight = float('inf')\n",
    "    cycle = []\n",
    "\n",
    "    for u in range(n):\n",
    "        for v in range(n):\n",
    "            if u != v and dist[u][v] < float('inf') and dist[v][u] < float('inf'):\n",
    "                cycle_weight = dist[u][v] + dist[v][u]\n",
    "                if cycle_weight < min_cycle_weight:\n",
    "                    min_cycle_weight = cycle_weight\n",
    "                    # Reconstruct the cycle\n",
    "                    cycle = []\n",
    "                    # Trace path from u to v\n",
    "                    current = v\n",
    "                    while current != u:\n",
    "                        cycle.append((pred[u][current], current))\n",
    "                        current = pred[u][current]\n",
    "                    # Trace path from v back to u\n",
    "                    current = u\n",
    "                    while current != v:\n",
    "                        cycle.append((pred[v][current], current))\n",
    "                        current = pred[v][current]\n",
    "\n",
    "    # Return the minimum weight cycle if found\n",
    "    if min_cycle_weight == float('inf'):\n",
    "        return None\n",
    "    else:\n",
    "        return cycle\n",
    "\n",
    "\n",
    "def check_and_readd_edges(graph, removed_edges, n):\n",
    "    \"\"\"\n",
    "    Phase 2: Check and re-add edges if they do not create a cycle.\n",
    "    Handles graphs with parallel edges using edge identifiers.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
    "        removed_edges (set): Set of removed edges as (u, v, edge_id).\n",
    "        n (int): Number of vertices in the graph.\n",
    "\n",
    "    Returns:\n",
    "        readded_edges (set): Set of edges that were successfully re-added.\n",
    "        remaining_removed_edges (set): Set of edges that could not be re-added.\n",
    "    \"\"\"\n",
    "    \n",
    "    def has_path(start, end, graph):\n",
    "        \"\"\"\n",
    "        Helper function to check if there is a path from start to end using DFS.\n",
    "        Avoids cycles when re-adding edges.\n",
    "        \"\"\"\n",
    "        visited = [False] * n\n",
    "        stack = [start]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            if node == end:\n",
    "                return True\n",
    "            if not visited[node]:\n",
    "                visited[node] = True\n",
    "                stack.extend(neighbor for neighbor, _ in graph[node])  # Add only neighbors\n",
    "        return False\n",
    "\n",
    "    readded_edges = set()\n",
    "    removed_edges_list = sorted(list(removed_edges), reverse=True)\n",
    "\n",
    "    for u, v, edge_id in removed_edges_list:\n",
    "        if not has_path(v, u, graph):  # Only re-add if it doesn't create a cycle\n",
    "            graph[u].append((v, edge_id))\n",
    "            readded_edges.add((u, v, edge_id))\n",
    "\n",
    "    remaining_removed_edges = removed_edges - readded_edges\n",
    "    return readded_edges, remaining_removed_edges\n",
    "\n",
    "def mwfas(file_path):\n",
    "    \"\"\"\n",
    "    Main function to find Minimum Weighted Feedback Arc Set (MWFAS) in a graph with parallel edges.\n",
    "    \n",
    "    :param file_path: Path to the file containing the graph.\n",
    "    :return: A dictionary with metrics, updated graph, removed edges, and their weights.\n",
    "    \"\"\"\n",
    "    # Read the graph and initialize its structure\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    graph, weights = initialize_graph(edges)\n",
    "\n",
    "    # Original graph statistics\n",
    "    total_edges = len(edges)\n",
    "    total_weight = sum(w for _, _, w, _ in edges)\n",
    "\n",
    "    # Phase 1: Reduce cycles\n",
    "    removed_edges, removed_weights = find_cycles_and_reduce(graph, weights, n)\n",
    "\n",
    "    # Phase 2: Re-add edges (if applicable)\n",
    "    readded_edges, remaining_removed_edges = check_and_readd_edges(graph, removed_edges, n)\n",
    "\n",
    "    # Compute final metrics\n",
    "    num_removed_edges = len(remaining_removed_edges)\n",
    "    total_removed_weight = sum(removed_weights.get(edge, 0) for edge in remaining_removed_edges)\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        \"total_edges\": total_edges,\n",
    "        \"total_weight\": total_weight,\n",
    "        \"num_removed_edges\": num_removed_edges,\n",
    "        \"removed_weight\": total_removed_weight,\n",
    "        \"final_graph\": graph,\n",
    "        \"removed_edges\": remaining_removed_edges,\n",
    "        \"removed_weights\": {edge: removed_weights.get(edge, 0) for edge in remaining_removed_edges},\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "\n",
    "def mwfas_synthetic(edges):\n",
    "    \"\"\"\n",
    "    Main function to find Minimum Weighted Feedback Arc Set (MWFAS) in a graph with parallel edges.\n",
    "    \n",
    "    :param file_path: Path to the file containing the graph.\n",
    "    :return: A dictionary with metrics, updated graph, removed edges, and their weights.\n",
    "    \"\"\"\n",
    "    # Read the graph and initialize its structure\n",
    " \n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    graph, weights = initialize_graph(edges)\n",
    "\n",
    "    # Original graph statistics\n",
    "    total_edges = len(edges)\n",
    "    total_weight = sum(w for _, _, w, _ in edges)\n",
    "\n",
    "    # Phase 1: Reduce cycles\n",
    "    removed_edges, removed_weights = find_cycles_and_reduce(graph, weights, n)\n",
    "\n",
    "    # Phase 2: Re-add edges (if applicable)\n",
    "    readded_edges, remaining_removed_edges = check_and_readd_edges(graph, removed_edges, n)\n",
    "\n",
    "    # Compute final metrics\n",
    "    num_removed_edges = len(remaining_removed_edges)\n",
    "    total_removed_weight = sum(removed_weights.get(edge, 0) for edge in remaining_removed_edges)\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        \"total_edges\": total_edges,\n",
    "        \"total_weight\": total_weight,\n",
    "        \"num_removed_edges\": num_removed_edges,\n",
    "        \"removed_weight\": total_removed_weight,\n",
    "        \"final_graph\": graph,\n",
    "        \"removed_edges\": remaining_removed_edges,\n",
    "        \"removed_weights\": {edge: removed_weights.get(edge, 0) for edge in remaining_removed_edges},\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff99c0-7497-4c1b-b8a0-261ae33cbcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830eebee-e905-4ffa-958d-bc4f22a7a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "\n",
    "class DirectedGraph:\n",
    "    def __init__(self, vertices):\n",
    "        self.graph = defaultdict(list)\n",
    "        self.vertices = vertices\n",
    "        self.weights = {}  # Maintain weights dictionary with unique edge keys\n",
    "\n",
    "    def add_edge(self, src, dest, weight=1, edge_id=None):\n",
    "        \"\"\"\n",
    "        Add a directed edge with weight and an optional unique edge_id.\n",
    "        \"\"\"\n",
    "        if edge_id is None:\n",
    "            edge_id = (src, dest, len(self.graph[src]))  # Generate unique edge_id\n",
    "        self.graph[src].append((dest, edge_id))\n",
    "        self.weights[(src, dest, edge_id)] = weight\n",
    "\n",
    "    def get_edges(self):\n",
    "        edges = []\n",
    "        for src in self.graph:\n",
    "            for dest, edge_id in self.graph[src]:\n",
    "                edges.append((src, dest, self.weights[(src, dest, edge_id)]))\n",
    "        return edges\n",
    "\n",
    "    def remove_edge(self, src, dest):\n",
    "        self.graph[src] = [(d, eid) for d, eid in self.graph[src] if d != dest]\n",
    "        self.weights = {key: weight for key, weight in self.weights.items() if key[0] != src or key[1] != dest}\n",
    "\n",
    "    def get_indegree(self):\n",
    "        indegree = {v: 0 for v in range(self.vertices)}\n",
    "        for src in self.graph:\n",
    "            for dest, _ in self.graph[src]:\n",
    "                indegree[dest] += 1\n",
    "        return indegree\n",
    "\n",
    "    def get_outdegree(self):\n",
    "        outdegree = {v: 0 for v in range(self.vertices)}\n",
    "        for src in self.graph:\n",
    "            for dest, _ in self.graph[src]:\n",
    "                outdegree[src] += 1\n",
    "        return outdegree\n",
    "\n",
    "    def eliminate_parallel_arcs(self):\n",
    "        for src in self.graph:\n",
    "            seen = {}\n",
    "            for dest, weight in self.graph[src]:\n",
    "                if dest in seen:\n",
    "                    seen[dest] += self.weights[(src, dest, weight)]\n",
    "                else:\n",
    "                    seen[dest] = self.weights[(src, dest, weight)]\n",
    "            self.graph[src] = [(dest, weight) for dest, weight in seen.items()]\n",
    "\n",
    "    def eliminate_two_cycles(self):\n",
    "        for src in list(self.graph.keys()):\n",
    "            for dest, weight in self.graph[src]:\n",
    "                for back_dest, back_weight in self.graph[dest]:\n",
    "                    if back_dest == src:\n",
    "                        if weight > back_weight:\n",
    "                            self.remove_edge(src, dest)\n",
    "                            self.remove_edge(dest, src)\n",
    "                        elif weight == back_weight:\n",
    "                            self.remove_edge(src, dest)\n",
    "                            self.remove_edge(dest, src)\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcb61d0-d814-4891-9faa-c11a20470c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mwfas2(file_path):\n",
    "    \"\"\"\n",
    "    Computes a feedback arc set for a graph, removes the edges in the FAS, and returns the resulting DAG.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing the graph.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains the final DAG, removed edges, and related metrics.\n",
    "    \"\"\"\n",
    "    # Step 1: Read the graph\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    graph, weights = initialize_graph(edges)\n",
    "\n",
    "    # Step 2: Graph reduction (similar to fas_red.cpp)\n",
    "    reduced_graph = reduce_graph(graph, weights)\n",
    "\n",
    "    # Step 3: Compute FAS using a heuristic (similar to fas_alg.cpp)\n",
    "    feedback_arc_set, removed_weights = compute_feedback_arc_set(reduced_graph, weights)\n",
    "\n",
    "    # Step 4: Remove FAS edges from the graph\n",
    "    for u, v, edge_id in feedback_arc_set:\n",
    "        reduced_graph[u] = [\n",
    "            (neighbor, eid) for neighbor, eid in reduced_graph[u]\n",
    "            if not (neighbor == v and eid == edge_id)\n",
    "        ]\n",
    "\n",
    "    # Step 5: Return results\n",
    "    return {\n",
    "        \"final_graph\": reduced_graph,\n",
    "        \"removed_edges\": feedback_arc_set,\n",
    "        \"removed_weights\": removed_weights,\n",
    "        \"total_removed_weight\": sum(removed_weights.values()),\n",
    "    }\n",
    "\n",
    "\n",
    "def reduce_graph(graph, weights):\n",
    "    \"\"\"\n",
    "    Simplifies the graph by removing trivial components like sinks and sources.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Adjacency list.\n",
    "        weights (dict): Edge weights.\n",
    "\n",
    "    Returns:\n",
    "        dict: Reduced graph.\n",
    "    \"\"\"\n",
    "    # Remove sinks and sources\n",
    "    while True:\n",
    "        sinks = [u for u in graph if len(graph[u]) == 0]\n",
    "        sources = [u for u in graph if all(u not in [v for v, _ in graph[u2]] for u2 in graph)]\n",
    "\n",
    "        if not sinks and not sources:\n",
    "            break\n",
    "\n",
    "        for sink in sinks:\n",
    "            graph.pop(sink, None)\n",
    "\n",
    "        for source in sources:\n",
    "            graph.pop(source, None)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def compute_feedback_arc_set(graph, weights):\n",
    "    \"\"\"\n",
    "    Computes a feedback arc set using a heuristic.\n",
    "\n",
    "    Args:\n",
    "        graph (dict): Adjacency list.\n",
    "        weights (dict): Edge weights.\n",
    "\n",
    "    Returns:\n",
    "        list: Feedback arc set (list of edges).\n",
    "        dict: Weights of removed edges.\n",
    "    \"\"\"\n",
    "    feedback_arc_set = []\n",
    "    removed_weights = {}\n",
    "\n",
    "    # Ensure in_degree includes all nodes\n",
    "    in_degree = {u: 0 for u in range(max(graph.keys(), default=-1) + 1)}\n",
    "    for u in graph:\n",
    "        for v, _ in graph[u]:\n",
    "            if v not in in_degree:\n",
    "                in_degree[v] = 0\n",
    "            in_degree[v] += 1\n",
    "\n",
    "    # Topological sort with cycle detection\n",
    "    queue = deque([u for u in in_degree if in_degree[u] == 0])\n",
    "    order = []\n",
    "\n",
    "    while queue:\n",
    "        u = queue.popleft()\n",
    "        order.append(u)\n",
    "        for v, edge_id in graph.get(u, []):  # Handle cases where u might not have outgoing edges\n",
    "            in_degree[v] -= 1\n",
    "            if in_degree[v] == 0:\n",
    "                queue.append(v)\n",
    "\n",
    "    # Identify cycles and select edges to remove\n",
    "    for u in graph:\n",
    "        for v, edge_id in graph[u]:\n",
    "            if u in order and v in order and order.index(u) > order.index(v):\n",
    "                edge_key = (u, v, edge_id)\n",
    "                feedback_arc_set.append(edge_key)\n",
    "                removed_weights[edge_key] = weights[edge_key]\n",
    "\n",
    "    return feedback_arc_set, removed_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae999ad2-836d-4c7c-a465-48eec352dbba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5efbb96d-66cb-4ff9-a7d9-6e46488f403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vertex_rankings(graph, weights, n):\n",
    "    \"\"\"\n",
    "    Compute rankings for the vertices in a DAG with parallel edges.\n",
    "    :param graph: Adjacency list of the DAG with (neighbor, edge_id) pairs.\n",
    "    :param weights: Dictionary of edge weights with keys (u, v, edge_id).\n",
    "    :param n: Total number of vertices in the graph.\n",
    "    :return: A list of rankings for the vertices.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate in-degrees\n",
    "    in_degree = [0] * n\n",
    "    for u in graph:\n",
    "        for v, _ in graph[u]:  # Ignore edge_id for in-degree calculation\n",
    "            in_degree[v] += 1\n",
    "\n",
    "    # Step 2: Perform topological sort using a min-heap\n",
    "    from heapq import heappop, heappush\n",
    "    min_heap = []\n",
    "    for i in range(n):\n",
    "        if in_degree[i] == 0:\n",
    "            heappush(min_heap, i)\n",
    "\n",
    "    topological_order = []\n",
    "    while min_heap:\n",
    "        current = heappop(min_heap)\n",
    "        topological_order.append(current)\n",
    "        for neighbor, _ in graph[current]:  # Ignore edge_id for topological sort\n",
    "            in_degree[neighbor] -= 1\n",
    "            if in_degree[neighbor] == 0:\n",
    "                heappush(min_heap, neighbor)\n",
    "\n",
    "    # Step 3: Calculate outgoing and incoming edge weight sums for all vertices\n",
    "    outgoing_weights = {v: 0 for v in range(n)}\n",
    "    incoming_weights = {v: 0 for v in range(n)}\n",
    "\n",
    "    for u in graph:\n",
    "        for v, edge_id in graph[u]:\n",
    "            edge_key = (u, v, edge_id)\n",
    "            outgoing_weights[u] += weights.get(edge_key, 0)\n",
    "            incoming_weights[v] += weights.get(edge_key, 0)\n",
    "\n",
    "    # Step 4: Assign rankings\n",
    "    rankings = [-1] * n\n",
    "    current_rank = 0\n",
    "    for vertex in topological_order:\n",
    "        rankings[vertex] = current_rank\n",
    "        current_rank += 1\n",
    "\n",
    "    # Break ties for vertices with the same ranking based on outgoing and incoming edge weights\n",
    "    tied_vertices = sorted(\n",
    "        [(rankings[v], -(outgoing_weights[v] - incoming_weights[v]) / \n",
    "          (outgoing_weights[v] + incoming_weights[v] if outgoing_weights[v] + incoming_weights[v] > 0 else 1), v)\n",
    "         for v in range(n)],\n",
    "        key=lambda x: (x[0], x[1])  # Sort by rank first, then by normalized weight difference\n",
    "    )\n",
    "\n",
    "    scores = [0] * n\n",
    "    for final_rank, (_, _, vertex) in enumerate(tied_vertices):\n",
    "        scores[vertex] = n - final_rank - 1\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0465ba07-e038-43fc-8b77-f7d5f98b2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimize_updated_scores(adjacency_matrix, scores, epsilon=1e-8):\n",
    "#     import cvxpy as cp\n",
    "#     import numpy as np\n",
    "\n",
    "#     n = len(scores)\n",
    "\n",
    "#     # Convert inputs to numpy\n",
    "#     adjacency_matrix_np = adjacency_matrix.numpy()\n",
    "#     scores_np = scores.numpy().flatten()\n",
    "\n",
    "#     # Define optimization variables\n",
    "#     updated_scores = cp.Variable(n)\n",
    "#     R = cp.Variable((n, n))  # Auxiliary variable for ratios\n",
    "\n",
    "#     # Compute M (skew-symmetric pairwise matrix)\n",
    "#     M = adjacency_matrix_np - adjacency_matrix_np.T\n",
    "\n",
    "#     # Edge mask\n",
    "#     edge_mask = (adjacency_matrix_np + adjacency_matrix_np.T > 0).astype(float)\n",
    "\n",
    "#     # Objective: Minimize squared difference between M and R\n",
    "#     objective = cp.sum_squares(cp.multiply(edge_mask, M - R))\n",
    "\n",
    "#     # Constraints for R_ij\n",
    "#     constraints = []\n",
    "#     for i in range(n):\n",
    "#         for j in range(n):\n",
    "#             if edge_mask[i, j] > 0:\n",
    "#                 constraints.append(R[i, j] * (updated_scores[i] + updated_scores[j] + epsilon) == updated_scores[i] - updated_scores[j])\n",
    "\n",
    "#     # Order-preserving constraints\n",
    "#     constraints += [\n",
    "#         updated_scores[i] <= updated_scores[j]\n",
    "#         for i in range(n)\n",
    "#         for j in range(n)\n",
    "#         if scores_np[i] <= scores_np[j]\n",
    "#     ]\n",
    "\n",
    "#     # Solve the problem\n",
    "#     problem = cp.Problem(cp.Minimize(objective), constraints)\n",
    "#     problem.solve()\n",
    "\n",
    "#     return updated_scores.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "970aa5dc-cb8b-4d04-a90a-973d74396090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def graph_to_adjacency_matrix(graph, weights, n):\n",
    "    \"\"\"\n",
    "    Convert the graph to an adjacency matrix with weights.\n",
    "    :param graph: Adjacency list of the DAG with (neighbor, edge_id) pairs.\n",
    "    :param weights: Dictionary of edge weights with keys (u, v, edge_id).\n",
    "    :param n: Total number of vertices in the graph.\n",
    "    :return: An adjacency matrix (n x n) with weights as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    adjacency_matrix = torch.zeros((n, n), dtype=torch.float32)\n",
    "\n",
    "    for u in graph:\n",
    "        for v, edge_id in graph[u]:\n",
    "            edge_key = (u, v, edge_id)\n",
    "            adjacency_matrix[u, v] = weights.get(edge_key, 0)\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e70cf-ef80-4e9a-adb4-39e31e3e1b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a3b680-b16e-4ab5-a9b7-7e2faa7766ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def reorder_floats(x):\n",
    "    n = len(x)\n",
    "    random_floats = np.random.uniform(0,  2*n/3, n)\n",
    "    y = np.zeros(n)\n",
    "    for idx, val in enumerate(np.argsort(x)):\n",
    "        y[val] = sorted(random_floats)[idx]\n",
    "    return y\n",
    "\n",
    "def calculate_upset_loss(adjacency_matrix, scores, style='ratio', margin=0.01):\n",
    "    \"\"\"\n",
    "    Calculate the upset loss for the graph rankings using adjacency matrix and scores.\n",
    "\n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
    "    :param style: Type of upset loss ('naive', 'simple', 'ratio', or 'margin').\n",
    "    :param margin: Margin for margin loss (default: 0.01).\n",
    "    :return: Torch FloatTensor upset loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8  # For numerical stability\n",
    "\n",
    "    # Ensure scores are 2D\n",
    "    if scores.ndim == 1:\n",
    "        scores = scores.view(-1, 1)\n",
    "\n",
    "    # Skew-symmetric pairwise comparison matrix (M)\n",
    "    M1 = adjacency_matrix - adjacency_matrix.T\n",
    "\n",
    "    # Normalize scores to [0, 1] range\n",
    "    normalized_scores = scores\n",
    "\n",
    "    # Pairwise score differences (T)\n",
    "    T1 = normalized_scores - normalized_scores.T\n",
    "\n",
    "    # Edge mask: Only consider meaningful edges (where M != 0)\n",
    "    edge_mask = M1 != 0\n",
    "\n",
    "    if style == 'ratio':\n",
    "        min_upset = float('inf')  # Initialize with a large value\n",
    "        \n",
    "        for _ in range(40):\n",
    "            # Generate reordered scores using reorder_floats\n",
    "            if _==0:\n",
    "                reordered_scores=scores\n",
    "            else:\n",
    "                reordered_scores = torch.FloatTensor(reorder_floats(scores.flatten().tolist()))\n",
    "            reordered_scores = reordered_scores.view(-1, 1)\n",
    "\n",
    "            # Compute T2 for normalized scores\n",
    "            T2 = reordered_scores + reordered_scores.T + epsilon\n",
    "            T = torch.div(T1, T2)\n",
    "            M2 = adjacency_matrix + adjacency_matrix.T + epsilon\n",
    "            M3 = torch.div(M1, M2)  # Normalize the adjacency matrix\n",
    "            \n",
    "            # Compute ratio-based upset loss for this iteration\n",
    "            powers = torch.pow((M3 - T)[edge_mask], 2)\n",
    "            upset_loss = torch.sum(powers) / torch.sum(edge_mask)\n",
    "\n",
    "            # Track the minimum upset loss\n",
    "            min_upset = min(min_upset, upset_loss.item())\n",
    "        \n",
    "        return torch.tensor(min_upset)\n",
    "\n",
    "    elif style == 'naive':\n",
    "        upset = torch.sum(torch.sign(T1[edge_mask]) != torch.sign(M1[edge_mask])) / torch.sum(edge_mask)\n",
    "\n",
    "    elif style == 'simple':\n",
    "        upset = torch.mean((torch.sign(T1[edge_mask]) - torch.sign(M1[edge_mask]))**2)\n",
    "\n",
    "    elif style == 'margin':\n",
    "        upset = torch.mean(torch.nn.functional.relu(-M1[edge_mask] * (T1[edge_mask] - margin)))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported style: {style}\")\n",
    "\n",
    "    return upset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb48e27-720b-4562-9a4b-0c7b314f97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_ratio_upset_loss(adjacency_matrix, scores, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Compute the ratio upset loss for the graph rankings using adjacency matrix and scores.\n",
    "\n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
    "    :param epsilon: Small value for numerical stability (default: 1e-8).\n",
    "    :return: Torch FloatTensor ratio upset loss value.\n",
    "    \"\"\"\n",
    "    # Ensure scores are 2D\n",
    "    if scores.ndim == 1:\n",
    "        scores = scores.view(-1, 1)\n",
    "\n",
    "    # Skew-symmetric pairwise comparison matrix (M)\n",
    "    M1 = adjacency_matrix - adjacency_matrix.T\n",
    "\n",
    "    # Pairwise score differences (T1)\n",
    "    T1 = scores - scores.T\n",
    "\n",
    "    # Edge mask: Only consider meaningful edges (where M1 != 0)\n",
    "    edge_mask = M1 != 0\n",
    "\n",
    "    # Compute T2 for normalized scores\n",
    "    T2 = scores + scores.T + epsilon\n",
    "    T = torch.div(T1, T2)\n",
    "\n",
    "    # Normalize M1 using adjacency matrix\n",
    "    M2 = adjacency_matrix + adjacency_matrix.T + epsilon\n",
    "    M3 = torch.div(M1, M2)  # Normalize the adjacency matrix\n",
    "\n",
    "    # Compute ratio upset loss\n",
    "    powers = torch.pow((M3 - T)[edge_mask], 2)\n",
    "    upset_loss = torch.sum(powers) / torch.sum(edge_mask)\n",
    "\n",
    "    return upset_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27b336e9-7412-4d1a-8a2f-8fa7f37d5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_ratio_upset_loss(adjacency_matrix, scores, epsilon=1e-2, max_time=120):\n",
    "    \"\"\"\n",
    "    Perform optimization to minimize the ratio upset loss, ensuring naive and simple losses do not worsen.\n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param scores: Initial scores for optimization.\n",
    "    :param epsilon: Small value for numerical stability (default: 1e-2).\n",
    "    :param max_time: Maximum time for optimization in seconds.\n",
    "    :return: Tuple (optimal_scores, minimized_loss)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from scipy.optimize import minimize\n",
    "    import time\n",
    "\n",
    "    n = adjacency_matrix.shape[0]\n",
    "\n",
    "    # Compute initial losses\n",
    "    initial_scores = scores.clone().detach().view(-1).numpy()\n",
    "    initial_losses = {\n",
    "        \"naive\": calculate_upset_loss(adjacency_matrix, scores.view(-1, 1), style=\"naive\"),\n",
    "        \"simple\": calculate_upset_loss(adjacency_matrix, scores.view(-1, 1), style=\"simple\"),\n",
    "    }\n",
    "\n",
    "    # Objective function\n",
    "    def objective_function(updated_scores, adjacency_matrix, initial_losses):\n",
    "        updated_scores = torch.tensor(updated_scores, dtype=torch.float32).view(-1, 1)\n",
    "        ratio_loss = calculate_upset_loss(adjacency_matrix, updated_scores, style=\"ratio\")\n",
    "        naive_loss = calculate_upset_loss(adjacency_matrix, updated_scores, style=\"naive\")\n",
    "        simple_loss = calculate_upset_loss(adjacency_matrix, updated_scores, style=\"simple\")\n",
    "\n",
    "        penalty = 0\n",
    "        if naive_loss > initial_losses[\"naive\"]:\n",
    "            penalty += naive_loss - initial_losses[\"naive\"]\n",
    "        if simple_loss > initial_losses[\"simple\"]:\n",
    "            penalty += simple_loss - initial_losses[\"simple\"]\n",
    "\n",
    "        return ratio_loss + 100 * penalty\n",
    "\n",
    "    # Timer callback\n",
    "    class TimerCallback:\n",
    "        def __init__(self, max_time, objective_function, adjacency_matrix, initial_losses):\n",
    "            self.start_time = time.time()\n",
    "            self.max_time = max_time\n",
    "            self.iterations = 0  # Track iterations\n",
    "            self.objective_function = objective_function\n",
    "            self.adjacency_matrix = adjacency_matrix\n",
    "            self.initial_losses = initial_losses\n",
    "            self.min_loss = float(\"inf\")  # Track minimum loss\n",
    "\n",
    "        def __call__(self, xk, *args, **kwargs):\n",
    "            self.iterations += 1\n",
    "\n",
    "            # Compute the objective function value\n",
    "            current_loss = self.objective_function(\n",
    "                xk, self.adjacency_matrix, self.initial_losses\n",
    "            )\n",
    "\n",
    "            # Update and print the minimum loss found so far\n",
    "            self.min_loss = min(self.min_loss, current_loss)\n",
    "            print(f\"Iteration {self.iterations}: Minimum loss so far: {self.min_loss:.6f}\")\n",
    "\n",
    "            # Stop optimization if the time limit is exceeded\n",
    "            if time.time() - self.start_time > self.max_time:\n",
    "                print(\"Time limit exceeded, stopping optimization.\")\n",
    "                raise StopIteration  # Signal COBYLA to stop\n",
    "\n",
    "    # Create the callback instance\n",
    "    callback = TimerCallback(\n",
    "        max_time=max_time,\n",
    "        objective_function=objective_function,\n",
    "        adjacency_matrix=adjacency_matrix,\n",
    "        initial_losses=initial_losses,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Run optimization with COBYLA\n",
    "        result = minimize(\n",
    "            fun=objective_function,\n",
    "            x0=initial_scores,\n",
    "            method=\"COBYLA\",\n",
    "            args=(adjacency_matrix, initial_losses),  # Pass required arguments\n",
    "            options={\"maxiter\": 500, \"disp\": False},\n",
    "            callback=callback,  # Logs the minimum loss after each iteration\n",
    "        )\n",
    "    except StopIteration:\n",
    "        print(\"Optimization stopped early due to time limit.\")\n",
    "\n",
    "    # Extract results\n",
    "    optimal_scores = result.x\n",
    "    minimized_loss = result.fun\n",
    "\n",
    "    print(f\"Optimization completed after {callback.iterations} iterations.\")\n",
    "    print(f\"Final minimum loss: {callback.min_loss:.6f}\")\n",
    "    return optimal_scores, minimized_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2567d-9114-4688-98f5-85c6b93a1527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68495d94-ad4d-43b2-9333-c84dc62fd752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def compute_ratio_upset_loss(adjacency_matrix, scores, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Compute the ratio upset loss for the graph rankings using adjacency matrix and scores.\n",
    "    \n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
    "    :param epsilon: Small value for numerical stability (default: 1e-8).\n",
    "    :return: Torch FloatTensor ratio upset loss value.\n",
    "    \"\"\"\n",
    "    if scores.ndim == 1:\n",
    "        scores = scores.view(-1, 1)\n",
    "\n",
    "    M1 = adjacency_matrix - adjacency_matrix.T\n",
    "    T1 = scores - scores.T\n",
    "    edge_mask = M1 != 0\n",
    "\n",
    "    T2 = scores + scores.T + epsilon\n",
    "    T = torch.div(T1, T2)\n",
    "\n",
    "    M2 = adjacency_matrix + adjacency_matrix.T + epsilon\n",
    "    M3 = torch.div(M1, M2)\n",
    "\n",
    "    powers = torch.pow((M3 - T)[edge_mask], 2)\n",
    "    upset_loss = torch.sum(powers) / torch.sum(edge_mask)\n",
    "\n",
    "    return upset_loss\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_ratio_loss(adjacency_matrix, scores, index, lower_bound, upper_bound, steps=100, output_folder=\"loss_plots\"):\n",
    "    \"\"\"\n",
    "    Plot and save the compute_ratio_upset_loss function for scores[index] between lower_bound and upper_bound.\n",
    "\n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
    "    :param index: Index of the score to vary.\n",
    "    :param lower_bound: Lower bound for the score value.\n",
    "    :param upper_bound: Upper bound for the score value.\n",
    "    :param steps: Number of steps for sampling the range.\n",
    "    :param output_folder: Directory to save the plots.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    x_values = torch.linspace(lower_bound, upper_bound, steps)\n",
    "    y_values = []\n",
    "\n",
    "    original_score = scores[index].item()\n",
    "\n",
    "    for x in x_values:\n",
    "        scores[index] = x\n",
    "        loss = compute_ratio_upset_loss(adjacency_matrix, scores)\n",
    "        y_values.append(loss.item())\n",
    "\n",
    "    # Restore the original score\n",
    "    scores[index] = original_score\n",
    "\n",
    "    # Plot the graph\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x_values.numpy(), y_values, label=f\"Loss vs. scores[{index}]\")\n",
    "    plt.xlabel(f\"scores[{index}] value\")\n",
    "    plt.ylabel(\"Ratio Upset Loss\")\n",
    "    plt.title(f\"Compute Ratio Upset Loss for Varying scores[{index}]\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot with a unique name\n",
    "    plot_path = os.path.join(output_folder, f\"loss_plot_index_{index}.png\")\n",
    "    counter = 1\n",
    "    while os.path.exists(plot_path):\n",
    "        plot_path = os.path.join(output_folder, f\"loss_plot_index_{index}_{counter}.png\")\n",
    "        counter += 1\n",
    "\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Plot saved: {plot_path}\")\n",
    "\n",
    "\n",
    "def trinary_search_optimize(adjacency_matrix, scores, index, lower_bound, upper_bound, epsilon=1e-8, steps=100):\n",
    "    \"\"\"\n",
    "    Perform ternary search to find the optimal score value for a given index that minimizes ratio upset loss.\n",
    "\n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
    "    :param index: Index of the score to optimize.\n",
    "    :param lower_bound: Lower bound for the score value.\n",
    "    :param upper_bound: Upper bound for the score value.\n",
    "    :param epsilon: Small value for numerical stability.\n",
    "    :param steps: Number of steps for ternary search.\n",
    "    :return: Optimal score value for the given index.\n",
    "    \"\"\"\n",
    "    for _ in range(steps):\n",
    "        mid1 = lower_bound + (upper_bound - lower_bound) / 3.0\n",
    "        mid2 = upper_bound - (upper_bound - lower_bound) / 3.0\n",
    "\n",
    "        scores[index] = mid1\n",
    "        loss1 = compute_ratio_upset_loss(adjacency_matrix, scores)\n",
    "\n",
    "        scores[index] = mid2\n",
    "        loss2 = compute_ratio_upset_loss(adjacency_matrix, scores)\n",
    "\n",
    "        if loss1 < loss2:\n",
    "            upper_bound = mid2\n",
    "        elif loss1 > loss2:\n",
    "            lower_bound = mid1\n",
    "        else:\n",
    "            lower_bound = mid1\n",
    "            upper_bound = mid2\n",
    "\n",
    "        # Break if the range is small enough\n",
    "        if upper_bound - lower_bound < epsilon:\n",
    "            break\n",
    "\n",
    "    # After the loop, check the losses at lower_bound, upper_bound, and midpoint\n",
    "    mid_point = (lower_bound + upper_bound) / 2.0\n",
    "    scores[index] = mid_point\n",
    "    mid_point_loss = compute_ratio_upset_loss(adjacency_matrix, scores)\n",
    "\n",
    "    scores[index] = lower_bound\n",
    "    lower_bound_loss = compute_ratio_upset_loss(adjacency_matrix, scores)\n",
    "\n",
    "    scores[index] = upper_bound\n",
    "    upper_bound_loss = compute_ratio_upset_loss(adjacency_matrix, scores)\n",
    "\n",
    "    # Find the minimum loss and corresponding score\n",
    "    min_loss = min(mid_point_loss, lower_bound_loss, upper_bound_loss)\n",
    "    if min_loss == lower_bound_loss:\n",
    "        optimal_score = lower_bound\n",
    "    elif min_loss == upper_bound_loss:\n",
    "        optimal_score = upper_bound\n",
    "    else:\n",
    "        optimal_score = mid_point\n",
    "\n",
    "    scores[index] = optimal_score\n",
    "    return scores[index]\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize_scalar\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def minimize_ratio_loss(adjacency_matrix, scores):\n",
    "    \"\"\"\n",
    "    Minimize the ratio upset loss by iteratively optimizing each score using binary search.\n",
    "\n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
    "    :return: Optimized scores.\n",
    "    \"\"\"\n",
    "    scores = scores.clone()  # Create a copy to avoid modifying the original\n",
    "\n",
    "    for _ in range(40):\n",
    "        sorted_indices = torch.argsort(scores.squeeze())\n",
    "        print(_)\n",
    "        for i in range(len(sorted_indices)):\n",
    "            \n",
    "            \n",
    "            \n",
    "            index = sorted_indices[i]\n",
    "\n",
    "            if i == 0:\n",
    "                lower_bound = 0\n",
    "                upper_bound = scores[sorted_indices[i + 1]].item()\n",
    "            elif i == len(sorted_indices) - 1:\n",
    "                lower_bound = scores[sorted_indices[i - 1]].item()\n",
    "                upper_bound = scores.max().item() + 10  # Extend beyond max for the last element\n",
    "            else:\n",
    "                lower_bound = scores[sorted_indices[i - 1]].item()\n",
    "                upper_bound = scores[sorted_indices[i + 1]].item()\n",
    "\n",
    "            # Perform binary search optimization for the current score\n",
    "            scores[index] = trinary_search_optimize(adjacency_matrix, scores, index, lower_bound, upper_bound)\n",
    "          #  print(\"i am here\")\n",
    "    return scores\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c6b21d9-6337-4113-8cdf-bdc54cd095c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_upset_losses(file_path, rankings):\n",
    "    \"\"\"\n",
    "    Evaluate upset losses (naive, simple, ratio, margin) for a graph and given rankings.\n",
    "\n",
    "    :param file_path: Path to the graph file.\n",
    "    :param rankings: List of rankings for the vertices.\n",
    "    \"\"\"\n",
    "    # Step 1: Prepare Graph and Adjacency Matrix\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _ in edges) + 1\n",
    "    graph, weights = initialize_graph(edges)\n",
    "    adjacency_matrix = graph_to_adjacency_matrix(graph, weights, n)\n",
    "\n",
    "    # Step 2: Convert Rankings to Scores Tensor\n",
    "    scores = torch.FloatTensor(rankings).view(-1, 1)\n",
    "\n",
    "    # Step 3: Calculate Upset Losses\n",
    "    naive_loss = calculate_upset_loss(adjacency_matrix, scores, style='naive').item()\n",
    "    simple_loss = calculate_upset_loss(adjacency_matrix, scores, style='simple').item()\n",
    "    ratio_loss = calculate_upset_loss(adjacency_matrix, scores, style='ratio').item()\n",
    "    margin_loss = calculate_upset_loss(adjacency_matrix, scores, style='margin').item()\n",
    "\n",
    "    # Step 4: Print Results\n",
    "    print(\"Upset Losses for the Graph Rankings:\")\n",
    "    print(f\"Naive Upset Loss: {naive_loss:.4f}\")\n",
    "    print(f\"Simple Upset Loss: {simple_loss:.4f}\")\n",
    "    print(f\"Differentiable Upset Loss (Ratio): {ratio_loss:.4f}\")\n",
    "    print(f\"Upset Margin Loss: {margin_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e96b02a-635c-40c9-9e43-26cb60a76680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0, 1.0, 0), (3, 2, 3.0, 1), (4, 5, 8.0, 2), (7, 6, 2.0, 3), (9, 8, 1.0, 4), (11, 10, 2.0, 5), (13, 12, 2.0, 6), (16, 17, 3.0, 7), (15, 11, 8.0, 8), (7, 0, 1.0, 9), (1, 12, 3.0, 10), (8, 13, 2.0, 11), (5, 6, 2.0, 12), (2, 18, 5.0, 13), (4, 10, 7.0, 14), (16, 3, 2.0, 15), (17, 19, 5.0, 16), (15, 0, 1.0, 17), (3, 11, 3.0, 18), (4, 13, 2.0, 19), (16, 18, 5.0, 20), (10, 8, 1.0, 21), (17, 12, 3.0, 22), (19, 1, 1.0, 23), (7, 2, 1.0, 24), (9, 6, 4.0, 25), (15, 2, 2.0, 26), (16, 1, 1.0, 27), (3, 12, 1.0, 28), (9, 0, 1.0, 29), (11, 17, 2.0, 30), (4, 18, 5.0, 31), (6, 10, 2.0, 32), (14, 7, 5.0, 33), (13, 19, 1.0, 34), (2, 19, 1.0, 35), (3, 0, 1.0, 36), (18, 13, 3.0, 37), (8, 12, 2.0, 38), (5, 7, 4.0, 39), (4, 11, 6.0, 40), (14, 16, 1.0, 41), (9, 10, 3.0, 42), (5, 15, 1.0, 43), (10, 7, 1.0, 44), (0, 11, 1.0, 45), (1, 3, 1.0, 46), (14, 6, 2.0, 47), (4, 9, 1.0, 48), (18, 8, 1.0, 49), (16, 2, 1.0, 50), (13, 17, 5.0, 51), (1, 7, 2.0, 52), (13, 0, 3.0, 53), (16, 6, 2.0, 54), (8, 19, 2.0, 55), (2, 5, 2.0, 56), (18, 3, 2.0, 57), (10, 12, 3.0, 58), (11, 14, 2.0, 59), (9, 17, 3.0, 60), (15, 7, 4.0, 61), (13, 2, 1.0, 62), (8, 3, 2.0, 63), (16, 5, 1.0, 64), (17, 10, 1.0, 65), (18, 12, 2.0, 66), (9, 11, 2.0, 67), (1, 14, 2.0, 68), (6, 0, 3.0, 69), (7, 11, 3.0, 70), (4, 12, 1.0, 71), (6, 19, 2.0, 72), (1, 8, 1.0, 73), (3, 5, 2.0, 74), (17, 18, 6.0, 75), (0, 14, 1.0, 76), (15, 9, 3.0, 77), (16, 13, 2.0, 78), (15, 18, 4.0, 79), (4, 0, 3.0, 80), (13, 1, 1.0, 81), (3, 10, 2.0, 82), (16, 8, 2.0, 83), (14, 2, 2.0, 84), (17, 6, 4.0, 85), (7, 18, 1.0, 86), (1, 11, 2.0, 87), (2, 8, 1.0, 88), (6, 13, 1.0, 89), (17, 15, 1.0, 90), (14, 4, 3.0, 91), (9, 5, 5.0, 92), (8, 6, 1.0, 93), (19, 11, 1.0, 94), (4, 3, 1.0, 95), (2, 1, 2.0, 96), (16, 9, 1.0, 97), (0, 17, 1.0, 98), (15, 12, 4.0, 99), (16, 19, 2.0, 100), (3, 17, 1.0, 101), (8, 0, 3.0, 102), (18, 11, 2.0, 103), (10, 5, 1.0, 104), (2, 12, 2.0, 105), (6, 4, 1.0, 106), (15, 1, 2.0, 107), (8, 15, 1.0, 108), (4, 7, 1.0, 109), (12, 11, 3.0, 110), (2, 17, 4.0, 111), (14, 18, 1.0, 112), (16, 10, 6.0, 113), (13, 5, 3.0, 114), (19, 0, 1.0, 115), (9, 3, 3.0, 116), (1, 6, 2.0, 117), (15, 19, 1.0, 118), (11, 2, 1.0, 119), (5, 1, 4.0, 120), (3, 7, 2.0, 121), (16, 0, 6.0, 122), (9, 13, 3.0, 123), (18, 10, 1.0, 124), (12, 6, 3.0, 125), (8, 14, 3.0, 126), (15, 3, 1.0, 127), (8, 7, 1.0, 128), (0, 12, 4.0, 129), (1, 4, 1.0, 130), (9, 2, 3.0, 131), (10, 13, 1.0, 132), (6, 18, 4.0, 133), (5, 17, 2.0, 134), (14, 19, 2.0, 135), (3, 6, 3.0, 136), (14, 17, 1.0, 137), (11, 13, 1.0, 138), (9, 18, 3.0, 139), (4, 8, 1.0, 140), (12, 7, 1.0, 141), (6, 2, 2.0, 142), (1, 9, 2.0, 143), (16, 7, 5.0, 144), (9, 19, 3.0, 145), (3, 13, 4.0, 146), (11, 6, 1.0, 147), (8, 17, 2.0, 148), (0, 5, 4.0, 149), (4, 2, 5.0, 150), (14, 12, 2.0, 151), (15, 10, 3.0, 152), (16, 11, 3.0, 153), (19, 10, 1.0, 154), (14, 3, 4.0, 155), (17, 7, 3.0, 156), (12, 5, 2.0, 157), (11, 8, 2.0, 158), (13, 15, 1.0, 159), (19, 18, 1.0, 160)]\n",
      "Processing file: England 2010 2011.txt - Number of nodes: 20, Number of edges: 161\n",
      "Evaluating losses before optimization...\n",
      "[2, 10, 12, 14, 17, 11, 8, 3, 19, 15, 5, 1, 4, 13, 18, 16, 20, 9, 6, 7]\n",
      "Before Optimization:\n",
      "Naive Loss: 0.23602484166622162\n",
      "Simple Loss: 0.9440993666648865\n",
      "Ratio Loss: 0.8602190613746643\n",
      "Elapsed Time: 0.0321 seconds\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "Evaluating losses after optimization...\n",
      "After Optimization:\n",
      "Naive Loss: 0.23602484166622162\n",
      "Simple Loss: 0.9440993666648865\n",
      "Ratio Loss: 0.7821655869483948\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "# List of input files\n",
    "file_paths = [\"England 2010 2011.txt\"]\n",
    "\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Iterate through each input file\n",
    "for file_path in file_paths:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Read the graph and initialize structures\n",
    "    edges = read_graph(file_path)\n",
    "    print(edges)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    print(f\"Processing file: {file_path} - Number of nodes: {n}, Number of edges: {len(edges)}\")\n",
    "\n",
    "    init_graph, init_weights = initialize_graph(edges)\n",
    "#    init_graph, init_weights =update_edge_weights(init_graph,init_weights)\n",
    "#    print(init_weights)\n",
    "    # Step 2: Ensure the graph is a DAG by removing cycles\n",
    "    result = mwfas(file_path)\n",
    "    new_graph = result['final_graph']\n",
    "    new_weights = {key: value for key, value in init_weights.items() if key not in result['removed_weights']}\n",
    "\n",
    "    # Step 3: Compute rankings for the vertices using the modified graph\n",
    "    final_rankings = compute_vertex_rankings(new_graph, new_weights, n)\n",
    "\n",
    "    # Step 4: Evaluate upset losses before optimization\n",
    "    scores = torch.FloatTensor(final_rankings).view(-1, 1)\n",
    "    adjacency_matrix = graph_to_adjacency_matrix(init_graph, init_weights, n)\n",
    "\n",
    "    print(\"Evaluating losses before optimization...\")\n",
    "    naive_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='naive')\n",
    "    simple_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='simple')\n",
    "    ratio_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='ratio')\n",
    "    final_rankings=[x+1 for x in final_rankings]\n",
    "    print(final_rankings)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Before Optimization:\")\n",
    "    print(f\"Naive Loss: {naive_loss_before}\")\n",
    "    print(f\"Simple Loss: {simple_loss_before}\")\n",
    "    print(f\"Ratio Loss: {ratio_loss_before}\")\n",
    "    print(f\"Elapsed Time: {elapsed_time:.4f} seconds\")\n",
    "#   Step 5: Perform optimization\n",
    "    optimized_scores = minimize_ratio_loss(adjacency_matrix, scores)\n",
    "#    optimized_scores=scores\n",
    "    optimized_scores = torch.FloatTensor(optimized_scores).view(-1, 1)  # Ensure Torch Tensor format\n",
    "\n",
    " #   Step 6: Evaluate upset losses after optimization\n",
    "    print(\"Evaluating losses after optimization...\")\n",
    "    naive_loss_after = calculate_upset_loss(adjacency_matrix, optimized_scores, style='naive')\n",
    "    simple_loss_after = calculate_upset_loss(adjacency_matrix, optimized_scores, style='simple')\n",
    "    ratio_loss_after = calculate_upset_loss(adjacency_matrix, optimized_scores, style='ratio')\n",
    "\n",
    "   # end_time = time.time()\n",
    "   # elapsed_time = end_time - start_time\n",
    "\n",
    "    # Step 7: Print final results\n",
    "    print(\"After Optimization:\")\n",
    "    print(f\"Naive Loss: {naive_loss_after}\")\n",
    "    print(f\"Simple Loss: {simple_loss_after}\")\n",
    "    print(f\"Ratio Loss: {ratio_loss_after}\")\n",
    "  # print(f\"Elapsed Time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "   \n",
    "    \n",
    "    #Store results in a list\n",
    "#     results.append({\n",
    "#         \"File\": \"Football_finer(\"+str(file_path[13:17])+')',\n",
    "#         \"Nodes\": n,\n",
    "#         \"Edges\": len(edges),\n",
    "#         \"Naive Loss\": round(naive_loss_before.item(),2),\n",
    "#         \"Simple Loss\": round(simple_loss_before.item(),2),\n",
    "#         \"Ratio Loss\": round(ratio_loss_before.item(),2),\n",
    "#         \"Elapsed Time (s)\": round(elapsed_time,2)\n",
    "#     })\n",
    "\n",
    "\n",
    "# #Write results to an Excel file\n",
    "# output_file = \"finer_Football_results.xlsx\"\n",
    "# df = pd.DataFrame(results)\n",
    "# df.to_excel(output_file, index=False)\n",
    "\n",
    "# print(f\"Results written to {output_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "432d42ab-f34f-4f4b-9f38-5115d71d4169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def generate_ero_graph(n: int, p: float, eta: float, style: str = 'uniform'):\n",
    "    \"\"\"\n",
    "    Generates an Erdős-Rényi Outliers (ERO) model graph in a format compatible with initialize_graph() and mwfas().\n",
    "\n",
    "    Args:\n",
    "        n (int): Number of nodes.\n",
    "        p (float): Edge probability (sparsity).\n",
    "        eta (float): Noise level (between 0 and 1).\n",
    "        style (str): How to generate ground-truth scores ('uniform' or 'gamma').\n",
    "\n",
    "    Returns:\n",
    "        list: List of edges (start, end, weight, edge_id) for MWFAS.\n",
    "        np.array: Ground-truth ranking of nodes.\n",
    "    \"\"\"\n",
    "    # Generate node scores\n",
    "    if style == 'uniform':\n",
    "        scores = np.random.rand(n, 1)\n",
    "        R_noise = np.random.rand(n, n) * 2 - 1\n",
    "    elif style == 'gamma':\n",
    "        scores = np.random.gamma(shape=0.5, scale=1, size=(n, 1))\n",
    "        R_noise = np.random.rand(n, n) * 4 - 2  # Gamma noise\n",
    "\n",
    "    # Compute ground-truth ranking\n",
    "    labels = rankdata(-scores.flatten(), 'min')\n",
    "\n",
    "    # Generate pairwise comparisons matrix\n",
    "    R_GT = scores - scores.T  # True pairwise differences\n",
    "    R_choice = np.random.rand(n, n)\n",
    "    R = np.zeros((n, n))\n",
    "    R[R_choice <= p] = R_noise[R_choice <= p]  # Assign noisy comparisons\n",
    "    R[R_choice <= p * (1 - eta)] = R_GT[R_choice <= p * (1 - eta)]  # Assign correct comparisons\n",
    "\n",
    "    # Ensure antisymmetry\n",
    "    lower_ind = np.tril_indices(n)\n",
    "    diag_ind = np.diag_indices(n)\n",
    "    R[lower_ind] = -R.T[lower_ind]\n",
    "    R[diag_ind] = 0\n",
    "    R[R < 0] = 0  # Ensure positive weights\n",
    "\n",
    "    # Convert matrix to edge list format for mwfas (edge_id is the last element)\n",
    "    edges = []\n",
    "    edge_id = 0\n",
    "    R_coo = sp.csr_matrix(R).tocoo()\n",
    "    for u, v, w in zip(R_coo.row, R_coo.col, R_coo.data):\n",
    "        edges.append((int(u), int(v), float(w), edge_id))\n",
    "        edge_id += 1\n",
    "\n",
    "    return edges, labels\n",
    "\n",
    "\n",
    "\n",
    "def kendall_tau_loss(true_ranking, predicted_ranking):\n",
    "    \"\"\"\n",
    "    Computes Kendall tau loss, which measures ranking disagreement.\n",
    "\n",
    "    Args:\n",
    "        true_ranking (np.array): Ground-truth ranking.\n",
    "        predicted_ranking (np.array): Computed ranking.\n",
    "\n",
    "    Returns:\n",
    "        float: Kendall tau loss (fraction of discordant pairs).\n",
    "    \"\"\"\n",
    "    n = len(true_ranking)\n",
    "    true_order = np.argsort(true_ranking)\n",
    "    predicted_order = np.argsort(predicted_ranking)\n",
    "    discordant_pairs = sum(\n",
    "        (true_order[i] > true_order[j]) != (predicted_order[i] > predicted_order[j])\n",
    "        for i in range(n)\n",
    "        for j in range(i + 1, n)\n",
    "    )\n",
    "    total_pairs = n * (n - 1) / 2\n",
    "    return discordant_pairs / total_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a7fbd3f-5843-45d5-9fab-aaa28d9c6b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0, style: uniform, Edges: 3132\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0, style: uniform\n",
      "Naive Loss: 0.0\n",
      "Simple Loss: 0.0\n",
      "Ratio Loss: 0.3589000403881073\n",
      "Kendall Tau Loss: 0.8269013507981988\n",
      "Elapsed Time: 1.6080 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0, style: gamma, Edges: 3073\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0, style: gamma\n",
      "Naive Loss: 0.0\n",
      "Simple Loss: 0.0\n",
      "Ratio Loss: 0.3586008548736572\n",
      "Kendall Tau Loss: 0.82189111747851\n",
      "Elapsed Time: 1.1475 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.1, style: uniform, Edges: 2993\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.1, style: uniform\n",
      "Naive Loss: 0.061476778239011765\n",
      "Simple Loss: 0.24590711295604706\n",
      "Ratio Loss: 0.5121503472328186\n",
      "Kendall Tau Loss: 0.6843553008595987\n",
      "Elapsed Time: 1.6282 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.1, style: gamma, Edges: 3146\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.1, style: gamma\n",
      "Naive Loss: 0.07787667214870453\n",
      "Simple Loss: 0.3115066885948181\n",
      "Ratio Loss: 0.5428107380867004\n",
      "Kendall Tau Loss: 0.6820630372492836\n",
      "Elapsed Time: 2.1074 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.2, style: uniform, Edges: 2992\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.2, style: uniform\n",
      "Naive Loss: 0.10193850100040436\n",
      "Simple Loss: 0.40775400400161743\n",
      "Ratio Loss: 0.6123512983322144\n",
      "Kendall Tau Loss: 0.637265656979124\n",
      "Elapsed Time: 1.8220 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.2, style: gamma, Edges: 3092\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.2, style: gamma\n",
      "Naive Loss: 0.12289780378341675\n",
      "Simple Loss: 0.491591215133667\n",
      "Ratio Loss: 0.6612828969955444\n",
      "Kendall Tau Loss: 0.5565452312730249\n",
      "Elapsed Time: 1.8823 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.3, style: uniform, Edges: 3102\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.3, style: uniform\n",
      "Naive Loss: 0.13475176692008972\n",
      "Simple Loss: 0.5390070676803589\n",
      "Ratio Loss: 0.6909708380699158\n",
      "Kendall Tau Loss: 0.6049119934506753\n",
      "Elapsed Time: 2.7519 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.3, style: gamma, Edges: 3001\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.3, style: gamma\n",
      "Naive Loss: 0.14428523182868958\n",
      "Simple Loss: 0.5771409273147583\n",
      "Ratio Loss: 0.7123753428459167\n",
      "Kendall Tau Loss: 0.5305116659844453\n",
      "Elapsed Time: 2.2321 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.4, style: uniform, Edges: 3063\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.4, style: uniform\n",
      "Naive Loss: 0.15899445116519928\n",
      "Simple Loss: 0.6359778046607971\n",
      "Ratio Loss: 0.7375521659851074\n",
      "Kendall Tau Loss: 0.5720343839541547\n",
      "Elapsed Time: 2.6635 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.4, style: gamma, Edges: 3083\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.4, style: gamma\n",
      "Naive Loss: 0.19721050560474396\n",
      "Simple Loss: 0.7888420224189758\n",
      "Ratio Loss: 0.8086540102958679\n",
      "Kendall Tau Loss: 0.45146131805157586\n",
      "Elapsed Time: 2.6424 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.5, style: uniform, Edges: 3005\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.5, style: uniform\n",
      "Naive Loss: 0.1856905221939087\n",
      "Simple Loss: 0.7427620887756348\n",
      "Ratio Loss: 0.7830882668495178\n",
      "Kendall Tau Loss: 0.4789357347523536\n",
      "Elapsed Time: 2.2899 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.5, style: gamma, Edges: 3038\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.5, style: gamma\n",
      "Naive Loss: 0.23535220324993134\n",
      "Simple Loss: 0.9414088129997253\n",
      "Ratio Loss: 0.8945040702819824\n",
      "Kendall Tau Loss: 0.3728039295947605\n",
      "Elapsed Time: 2.4192 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.6, style: uniform, Edges: 3041\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.6, style: uniform\n",
      "Naive Loss: 0.24005261063575745\n",
      "Simple Loss: 0.9602104425430298\n",
      "Ratio Loss: 0.90776526927948\n",
      "Kendall Tau Loss: 0.38059762586983215\n",
      "Elapsed Time: 2.4889 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.6, style: gamma, Edges: 3064\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.6, style: gamma\n",
      "Naive Loss: 0.24020887911319733\n",
      "Simple Loss: 0.9608355164527893\n",
      "Ratio Loss: 0.8989515900611877\n",
      "Kendall Tau Loss: 0.31949242734343014\n",
      "Elapsed Time: 2.7855 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.7, style: uniform, Edges: 3103\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.7, style: uniform\n",
      "Naive Loss: 0.2587818205356598\n",
      "Simple Loss: 1.0351272821426392\n",
      "Ratio Loss: 0.9540884494781494\n",
      "Kendall Tau Loss: 0.28618911174785094\n",
      "Elapsed Time: 4.6202 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.7, style: gamma, Edges: 3088\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.7, style: gamma\n",
      "Naive Loss: 0.25550517439842224\n",
      "Simple Loss: 1.022020697593689\n",
      "Ratio Loss: 0.9475570321083069\n",
      "Kendall Tau Loss: 0.30636103151862465\n",
      "Elapsed Time: 2.7767 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.8, style: uniform, Edges: 3171\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.8, style: uniform\n",
      "Naive Loss: 0.26426994800567627\n",
      "Simple Loss: 1.057079792022705\n",
      "Ratio Loss: 0.9643009305000305\n",
      "Kendall Tau Loss: 0.256225951698731\n",
      "Elapsed Time: 2.7776 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 0.05, eta: 0.8, style: gamma, Edges: 3207\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 0.05, eta: 0.8, style: gamma\n",
      "Naive Loss: 0.2706579267978668\n",
      "Simple Loss: 1.0826317071914673\n",
      "Ratio Loss: 0.9654358625411987\n",
      "Kendall Tau Loss: 0.16057306590257878\n",
      "Elapsed Time: 2.8194 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 1.0, eta: 0, style: uniform, Edges: 61075\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 1.0, eta: 0, style: uniform\n",
      "Naive Loss: 0.0\n",
      "Simple Loss: 0.0\n",
      "Ratio Loss: 0.3408551812171936\n",
      "Kendall Tau Loss: 0.9999999999999999\n",
      "Elapsed Time: 2.9727 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 1.0, eta: 0, style: gamma, Edges: 61075\n",
      "Evaluating losses before optimization...\n",
      "Before Optimization:\n",
      "Graph Parameters - Nodes: 350, p: 1.0, eta: 0, style: gamma\n",
      "Naive Loss: 0.0\n",
      "Simple Loss: 0.0\n",
      "Ratio Loss: 0.34090763330459595\n",
      "Kendall Tau Loss: 0.9999999999999999\n",
      "Elapsed Time: 1.9939 seconds\n",
      "Processing synthetic graph - Nodes: 350, p: 1.0, eta: 0.1, style: uniform, Edges: 61075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m init_graph, init_weights \u001b[38;5;241m=\u001b[39m initialize_graph(edges)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Step 2: Ensure the graph is a DAG by removing cycles\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m result \u001b[38;5;241m=\u001b[39m mwfas_synthetic(edges)\n\u001b[1;32m     35\u001b[0m new_graph \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_graph\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     36\u001b[0m new_weights \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m init_weights\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremoved_weights\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "Cell \u001b[0;32mIn[39], line 384\u001b[0m, in \u001b[0;36mmwfas_synthetic\u001b[0;34m(edges)\u001b[0m\n\u001b[1;32m    381\u001b[0m total_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(w \u001b[38;5;28;01mfor\u001b[39;00m _, _, w, _ \u001b[38;5;129;01min\u001b[39;00m edges)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Phase 1: Reduce cycles\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m removed_edges, removed_weights \u001b[38;5;241m=\u001b[39m find_cycles_and_reduce(graph, weights, n)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Phase 2: Re-add edges (if applicable)\u001b[39;00m\n\u001b[1;32m    387\u001b[0m readded_edges, remaining_removed_edges \u001b[38;5;241m=\u001b[39m check_and_readd_edges(graph, removed_edges, n)\n",
      "Cell \u001b[0;32mIn[39], line 72\u001b[0m, in \u001b[0;36mfind_cycles_and_reduce\u001b[0;34m(graph, weights, n)\u001b[0m\n\u001b[1;32m     69\u001b[0m removed_weights \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     cycle \u001b[38;5;241m=\u001b[39m find_cycle(graph, n)  \u001b[38;5;66;03m# Modified `find_cycle` returns edges with edge_id\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cycle:  \u001b[38;5;66;03m# No cycle found\u001b[39;00m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 147\u001b[0m, in \u001b[0;36mfind_cycle\u001b[0;34m(graph, n)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m neighbor, edge_id1 \u001b[38;5;129;01min\u001b[39;00m graph[u]:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m neighbor_of_neighbor, edge_id2 \u001b[38;5;129;01min\u001b[39;00m graph[neighbor]:\n\u001b[0;32m--> 147\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m neighbor_of_neighbor \u001b[38;5;241m==\u001b[39m u \u001b[38;5;129;01mand\u001b[39;00m edge_id1 \u001b[38;5;241m!=\u001b[39m edge_id2:\n\u001b[1;32m    148\u001b[0m                 \u001b[38;5;66;03m# Found a length-2 cycle\u001b[39;00m\n\u001b[1;32m    149\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m [(u, neighbor, edge_id1), (neighbor, u, edge_id2)]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Run DFS for longer cycles\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Experiment parameters\n",
    "num_nodes = 350  # Fixed number of nodes for all graphs\n",
    "probabilities = [0.05, 1.0]  # Sparsity values\n",
    "noise_levels = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]  # Noise levels\n",
    "styles = ['uniform', 'gamma']  # Score distributions\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Iterate over synthetic dataset configurations\n",
    "for p in probabilities:\n",
    "    for eta in noise_levels:\n",
    "        for style in styles:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Generate ERO graph\n",
    "            edges, true_ranking = generate_ero_graph(n=num_nodes, p=p, eta=eta, style=style)\n",
    "            \n",
    "            if not edges:\n",
    "                print(f\"Warning: No edges generated for Nodes: {num_nodes}, p: {p}, eta: {eta}, style: {style}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing synthetic graph - Nodes: {num_nodes}, p: {p}, eta: {eta}, style: {style}, Edges: {len(edges)}\")\n",
    "            \n",
    "            # Step 1: Initialize the graph\n",
    "            init_graph, init_weights = initialize_graph(edges)\n",
    "            \n",
    "            # Step 2: Ensure the graph is a DAG by removing cycles\n",
    "            result = mwfas_synthetic(edges)\n",
    "            new_graph = result['final_graph']\n",
    "            new_weights = {key: value for key, value in init_weights.items() if key not in result['removed_weights']}\n",
    "            \n",
    "            if not new_graph:\n",
    "                print(f\"Warning: Graph is empty after cycle removal for Nodes: {num_nodes}, p: {p}, eta: {eta}, style: {style}\")\n",
    "                continue\n",
    "            \n",
    "            # Step 3: Compute rankings for the vertices using the modified graph\n",
    "            final_rankings = compute_vertex_rankings(new_graph, new_weights, num_nodes)\n",
    "            \n",
    "            if len(final_rankings) != num_nodes:\n",
    "                print(f\"Warning: Mismatch in ranking size for Nodes: {num_nodes}, p: {p}, eta: {eta}, style: {style}\")\n",
    "                continue\n",
    "            \n",
    "            # Step 4: Evaluate upset losses before optimization\n",
    "            scores = torch.FloatTensor(final_rankings).view(-1, 1)\n",
    "            adjacency_matrix = graph_to_adjacency_matrix(init_graph, init_weights, num_nodes)\n",
    "            \n",
    "            if adjacency_matrix.numel() == 0:\n",
    "                print(f\"Warning: Empty adjacency matrix for Nodes: {num_nodes}, p: {p}, eta: {eta}, style: {style}\")\n",
    "                continue\n",
    "            \n",
    "            print(\"Evaluating losses before optimization...\")\n",
    "            naive_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='naive')\n",
    "            simple_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='simple')\n",
    "            ratio_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='ratio')\n",
    "            \n",
    "            # Compute Kendall Tau Loss using scipy\n",
    "            kendall_tau_before, _ = kendalltau(true_ranking, [-rank for rank in final_rankings])\n",
    "\n",
    "            \n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            \n",
    "            print(\"Before Optimization:\")\n",
    "            print(f\"Graph Parameters - Nodes: {num_nodes}, p: {p}, eta: {eta}, style: {style}\")\n",
    "            print(f\"Naive Loss: {naive_loss_before}\")\n",
    "            print(f\"Simple Loss: {simple_loss_before}\")\n",
    "            print(f\"Ratio Loss: {ratio_loss_before}\")\n",
    "            print(f\"Kendall Tau Loss: {kendall_tau_before}\")\n",
    "            print(f\"Elapsed Time: {elapsed_time:.4f} seconds\")\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"Nodes\": num_nodes,\n",
    "                \"p\": p,\n",
    "                \"eta\": eta,\n",
    "                \"Style\": style,\n",
    "                \"Naive Loss\": naive_loss_before.item(),\n",
    "                \"Simple Loss\": simple_loss_before.item(),\n",
    "                \"Ratio Loss\": ratio_loss_before.item(),\n",
    "                \"Kendall Tau Loss\": kendall_tau_before,\n",
    "                \"Elapsed Time\": elapsed_time\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"synthetic_experiment_results.csv\", index=False)\n",
    "print(\"Experiment completed. Results saved to synthetic_experiment_results.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381c339-ce6e-4167-8fcf-e5eccb6b3488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38f7d7-bc9b-4f1a-b39d-18c328997894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def optimize_scores_with_gnn(graph, weights, scores, num_epochs=1000, lr=0.01, perturbation_scale=0.1, patience=10):\n",
    "    \"\"\"\n",
    "    Uses a Graph Neural Network (GNN) to optimize initial scores while ensuring constraints on loss values.\n",
    "    Implements momentum-based optimization, adaptive perturbation, and simulated annealing for escaping local minima.\n",
    "    \n",
    "    Args:\n",
    "        graph (dict): Adjacency list with (neighbor, edge_id) pairs.\n",
    "        weights (dict): Dictionary mapping (start, end, edge_id) to weights.\n",
    "        scores (list): Initial scores for teams.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate.\n",
    "        perturbation_scale (float): Scale of noise to escape local minima.\n",
    "        patience (int): Number of epochs to wait before applying stronger perturbation.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimized scores (only if all three losses improve, else returns input scores).\n",
    "        dict: Dictionary containing naive loss, simple loss, and ratio loss.\n",
    "    \"\"\"\n",
    "    n = len(scores)\n",
    "    adjacency_matrix = graph_to_adjacency_matrix(graph, weights, n)\n",
    "    \n",
    "    scores = torch.FloatTensor(scores).view(-1, 1)\n",
    "    \n",
    "    edge_index = []\n",
    "    edge_weights = []\n",
    "    for u in graph:\n",
    "        for v, edge_id in graph[u]:\n",
    "            edge_index.append([u, v])\n",
    "            edge_weights.append(weights.get((u, v, edge_id), 1.0))\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
    "    \n",
    "    # Modify the model class to ensure it takes initial scores\n",
    "    class ScoreGNN(torch.nn.Module):\n",
    "        def __init__(self, num_nodes, initial_scores):\n",
    "            super(ScoreGNN, self).__init__()\n",
    "            self.conv1 = GATConv(1, 128, heads=8, dropout=0.1)\n",
    "            self.conv2 = GATConv(1024, 512, heads=8, dropout=0.1)\n",
    "            self.conv3 = GATConv(4096, 2048, heads=4, dropout=0.1)\n",
    "            self.conv4 = GATConv(8192, 1024, heads=4, dropout=0.1)\n",
    "            self.conv5 = GATConv(4096, 512, heads=4, dropout=0.1)\n",
    "            self.conv6 = GATConv(2048, 256, heads=2, dropout=0.1)\n",
    "            self.conv7 = GATConv(512, 128, heads=2, dropout=0.1)\n",
    "            self.conv8 = GATConv(256, 1, heads=1, dropout=0.1)\n",
    "            \n",
    "            # **Initialize bias using input scores**\n",
    "            self.initial_scores = torch.nn.Parameter(initial_scores.clone(), requires_grad=False)\n",
    "            self.bias = torch.nn.Parameter(initial_scores.clone())  \n",
    "    \n",
    "        def forward(self, x, edge_index, edge_weight):\n",
    "            x = F.elu(self.conv1(x, edge_index, edge_weight))\n",
    "            x = F.elu(self.conv2(x, edge_index, edge_weight))\n",
    "            x = F.elu(self.conv3(x, edge_index, edge_weight))\n",
    "            x = F.elu(self.conv4(x, edge_index, edge_weight))\n",
    "            x = F.elu(self.conv5(x, edge_index, edge_weight))\n",
    "            x = F.elu(self.conv6(x, edge_index, edge_weight))\n",
    "            x = F.elu(self.conv7(x, edge_index, edge_weight))\n",
    "            x = self.conv8(x, edge_index, edge_weight)\n",
    "            \n",
    "            # **Incorporate initial scores into final output**\n",
    "            return x + self.bias + self.initial_scores  # Keeps model anchored to initial scores\n",
    "\n",
    "\n",
    "# Modify Model Initialization\n",
    "\n",
    "\n",
    "# Use the given scores as input features to the neural network\n",
    "    # Modify Model Initialization\n",
    "    model = ScoreGNN(n,optimized_scores)  # Ensure model is initialized first\n",
    "    \n",
    "    # Use the given scores as input features to the neural network\n",
    "    new_scores = model(scores, edge_index, edge_weights)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    initial_naive_loss = calculate_upset_loss(adjacency_matrix, scores, style=\"naive\")\n",
    "    initial_simple_loss = calculate_upset_loss(adjacency_matrix, scores, style=\"simple\")\n",
    "    initial_ratio_loss = calculate_upset_loss(adjacency_matrix, scores, style=\"ratio\")\n",
    "    \n",
    "    best_scores = scores.clone()\n",
    "    best_losses = {\n",
    "        \"naive_loss\": initial_naive_loss.item(),\n",
    "        \"simple_loss\": initial_simple_loss.item(),\n",
    "        \"ratio_loss\": initial_ratio_loss.item()\n",
    "    }\n",
    "    improvement_epochs = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        new_scores = model(scores, edge_index, edge_weights)\n",
    "        \n",
    "        naive_loss = calculate_upset_loss(adjacency_matrix, new_scores, style=\"naive\")\n",
    "        simple_loss = calculate_upset_loss(adjacency_matrix, new_scores, style=\"simple\")\n",
    "        ratio_loss = calculate_upset_loss(adjacency_matrix, new_scores, style=\"ratio\")\n",
    "        \n",
    "        if naive_loss < initial_naive_loss and simple_loss < initial_simple_loss and ratio_loss < initial_ratio_loss:\n",
    "            best_scores = new_scores.clone()\n",
    "            best_losses = {\n",
    "                \"naive_loss\": naive_loss.item(),\n",
    "                \"simple_loss\": simple_loss.item(),\n",
    "                \"ratio_loss\": ratio_loss.item()\n",
    "            }\n",
    "            improvement_epochs += 1\n",
    "        \n",
    "        penalty = torch.relu(naive_loss - initial_naive_loss) * 10 + torch.relu(simple_loss - initial_simple_loss) * 10\n",
    "        loss = ratio_loss + penalty\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Naive Loss={naive_loss:.4f}, Simple Loss={simple_loss:.4f}, Ratio Loss={ratio_loss:.4f}\")\n",
    "    \n",
    "    print(f\"Improvement Epochs: {improvement_epochs}\")\n",
    "    print(f\"Final Naive Loss: {best_losses['naive_loss']}\")\n",
    "    print(f\"Final Simple Loss: {best_losses['simple_loss']}\")\n",
    "    print(f\"Final Ratio Loss: {best_losses['ratio_loss']}\")\n",
    "    \n",
    "    return best_scores if all(best_losses[k] < v.item() for k, v in zip(best_losses.keys(), [initial_naive_loss, initial_simple_loss, initial_ratio_loss])) else scores.clone()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(optimize_scores_with_gnn(init_graph,init_weights,optimized_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d14514-c86a-492e-976d-0e4da4ec8679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171ffe2-1cbb-40fe-a125-965f7389f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "!pip install torch_geometric\n",
    "from scipy.sparse import csr_matrix  # Deprecated\n",
    "\n",
    "\n",
    "# Open and read the .pk file\n",
    "with open(\"ERO/p5K5N350eta0styleuniformtrials2train_r80test_r10seed40.pk\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Print or inspect the data\n",
    "print(data)\n",
    "\n",
    "def extract_edges_from_data(data):\n",
    "    \"\"\"\n",
    "    Extracts directed edges from the given data format.\n",
    "\n",
    "    Args:\n",
    "        data: An object containing 'A' (a sparse matrix representation of edges).\n",
    "\n",
    "    Returns:\n",
    "        edges (list): List of tuples representing directed edges (start, end, weight, edge_id).\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    edge_id = 0  # Unique identifier for each edge\n",
    "\n",
    "    # Extract row (start) and column (end) indices of nonzero elements\n",
    "    row_indices, col_indices = data.A.nonzero()\n",
    "\n",
    "    # Extract corresponding weights\n",
    "    weights = data.A.data\n",
    "\n",
    "    # Construct edge tuples\n",
    "    for start, end, weight in zip(row_indices, col_indices, weights):\n",
    "        edges.append((start, end, weight, edge_id))\n",
    "        edge_id += 1\n",
    "\n",
    "    return edges\n",
    "\n",
    "print(\"data.A=\",data.A)\n",
    "print(extract_edges_from_data(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f444c-3584-4ca2-bbc3-a3bb8274e4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e50de-7e15-466d-83fa-234450de8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mwfas_synthetic(data):\n",
    "    \"\"\"\n",
    "    Main function to find Minimum Weighted Feedback Arc Set (MWFAS) in a graph with parallel edges.\n",
    "    \n",
    "    :param file_path: Path to the file containing the graph.\n",
    "    :return: A dictionary with metrics, updated graph, removed edges, and their weights.\n",
    "    \"\"\"\n",
    "    # Read the graph and initialize its structure\n",
    "    edges = extract_edges_from_data(data)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    graph, weights = initialize_graph(edges)\n",
    "\n",
    "    # Original graph statistics\n",
    "    total_edges = len(edges)\n",
    "    total_weight = sum(w for _, _, w, _ in edges)\n",
    "\n",
    "    # Phase 1: Reduce cycles\n",
    "    removed_edges, removed_weights = find_cycles_and_reduce(graph, weights, n)\n",
    "\n",
    "    # Phase 2: Re-add edges (if applicable)\n",
    "    readded_edges, remaining_removed_edges = check_and_readd_edges(graph, removed_edges, n)\n",
    "\n",
    "    # Compute final metrics\n",
    "    num_removed_edges = len(remaining_removed_edges)\n",
    "    total_removed_weight = sum(removed_weights.get(edge, 0) for edge in remaining_removed_edges)\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        \"total_edges\": total_edges,\n",
    "        \"total_weight\": total_weight,\n",
    "        \"num_removed_edges\": num_removed_edges,\n",
    "        \"removed_weight\": total_removed_weight,\n",
    "        \"final_graph\": graph,\n",
    "        \"removed_edges\": remaining_removed_edges,\n",
    "        \"removed_weights\": {edge: removed_weights.get(edge, 0) for edge in remaining_removed_edges},\n",
    "    }\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "folder_path = \"ERO\"  # Path to your folder\n",
    "file_paths = glob.glob(f\"{folder_path}/*\")  # Gets all files in the folder\n",
    "\n",
    "print(file_paths) \n",
    "\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "def extract_properties(file_name):\n",
    "    \"\"\"\n",
    "    Extracts properties from the given file name.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the file (including path) to extract properties from.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing extracted properties and their values.\n",
    "    \"\"\"\n",
    "    # Remove \"ERO/\" prefix and \".pk\" suffix\n",
    "    clean_name = os.path.basename(file_name).replace(\".pk\", \"\").replace(\"ERO/\", \"\")\n",
    "\n",
    "    properties = {}\n",
    "\n",
    "    # **Step 1: Extract \"style\" and remove it from the string**\n",
    "    style_match = re.search(r\"style([a-zA-Z]+)trials\", clean_name)\n",
    "    if style_match:\n",
    "        properties[\"style\"] = style_match.group(1)  # Extracts \"gamma\" in \"stylegamma\"\n",
    "        clean_name = clean_name.replace(f\"style{properties['style']}trials\", \"trials\")  # Remove from string\n",
    "\n",
    "    # **Step 2: Extract all remaining properties and their values**\n",
    "    parts = re.findall(r'[a-zA-Z_]+|\\d+', clean_name)  # Allows capturing \"train_r\" and \"test_r\"\n",
    "\n",
    "    i = 0\n",
    "    while i < len(parts):\n",
    "        key = parts[i]\n",
    "\n",
    "        # Ensure there is a number after the key\n",
    "        if i + 1 < len(parts) and parts[i + 1].isdigit():\n",
    "            properties[key] = int(parts[i + 1])\n",
    "            i += 2  # Move past key-value pair\n",
    "        else:\n",
    "            i += 1  # Move to the next part if no valid pair found\n",
    "\n",
    "    return properties\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Iterate through each input file\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    start_time = time.time()\n",
    "    print(file_path)\n",
    "    print(extract_properties(file_path))\n",
    "    # Step 1: Read the graph and initialize structures\n",
    "    edges = extract_edges_from_data(data)\n",
    "    if len(edges)>4000:\n",
    "        continue\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    print(f\"Processing file: {file_path} - Number of nodes: {n}, Number of edges: {len(edges)}\")\n",
    "\n",
    "    init_graph, init_weights = initialize_graph(edges)\n",
    "#    init_graph, init_weights =update_edge_weights(init_graph,init_weights)\n",
    "#    print(init_weights)\n",
    "    # Step 2: Ensure the graph is a DAG by removing cycles\n",
    "    result = mwfas_synthetic(data)\n",
    "    new_graph = result['final_graph']\n",
    "    new_weights = {key: value for key, value in init_weights.items() if key not in result['removed_weights']}\n",
    "\n",
    "    # Step 3: Compute rankings for the vertices using the modified graph\n",
    "    final_rankings = compute_vertex_rankings(new_graph, new_weights, n)\n",
    "\n",
    "    # Step 4: Evaluate upset losses before optimization\n",
    "    scores = torch.FloatTensor(final_rankings).view(-1, 1)\n",
    "    adjacency_matrix = graph_to_adjacency_matrix(init_graph, init_weights, n)\n",
    "\n",
    "    print(\"Evaluating losses before optimization...\")\n",
    "    naive_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='naive')\n",
    "    simple_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='simple')\n",
    "    ratio_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='ratio')\n",
    "    final_rankings=[x+1 for x in final_rankings]\n",
    "    print(final_rankings)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Before Optimization:\")\n",
    "    print(f\"Naive Loss: {naive_loss_before}\")\n",
    "    print(f\"Simple Loss: {simple_loss_before}\")\n",
    "    print(f\"Ratio Loss: {ratio_loss_before}\")\n",
    "    print(f\"Elapsed Time: {elapsed_time:.4f} seconds\")\n",
    "#   Step 5: Perform optimization\n",
    "#     optimized_scores = minimize_ratio_loss(adjacency_matrix, scores)\n",
    "# #    optimized_scores=scores\n",
    "#     optimized_scores = torch.FloatTensor(optimized_scores).view(-1, 1)  # Ensure Torch Tensor format\n",
    "\n",
    "#  #   Step 6: Evaluate upset losses after optimization\n",
    "#     print(\"Evaluating losses after optimization...\")\n",
    "#     naive_loss_after = calculate_upset_loss(adjacency_matrix, optimized_scores, style='naive')\n",
    "#     simple_loss_after = calculate_upset_loss(adjacency_matrix, optimized_scores, style='simple')\n",
    "#     ratio_loss_after = calculate_upset_loss(adjacency_matrix, optimized_scores, style='ratio')\n",
    "\n",
    "#    # end_time = time.time()\n",
    "#    # elapsed_time = end_time - start_time\n",
    "\n",
    "#     # Step 7: Print final results\n",
    "#     print(\"After Optimization:\")\n",
    "#     print(f\"Naive Loss: {naive_loss_after}\")\n",
    "#     print(f\"Simple Loss: {simple_loss_after}\")\n",
    "#     print(f\"Ratio Loss: {ratio_loss_after}\")\n",
    "  # print(f\"Elapsed Time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d9e60-8023-40bb-9071-14ca23f5ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd692d53-f974-4f68-b104-ea89317cfa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through each input file\n",
    "for file_path in file_paths:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Read the graph and initialize structures\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    print(f\"Processing file: {file_path} - Number of nodes: {n}, Number of edges: {len(edges)}\")\n",
    "\n",
    "    init_graph, init_weights = initialize_graph(edges)\n",
    "\n",
    "    # Step 2: Ensure the graph is a DAG by removing cycles\n",
    "    result = mwfas2(file_path)\n",
    "    new_graph = result['final_graph']\n",
    "    new_weights = {key: value for key, value in init_weights.items() if key not in result['removed_weights']}\n",
    "\n",
    "    # Step 3: Compute rankings for the vertices using the modified graph\n",
    "    final_rankings = compute_vertex_rankings(new_graph, new_weights, n)\n",
    "    print(\"final ranking=\",final_rankings)\n",
    "    # Step 4: Evaluate upset losses before optimization\n",
    "    scores = torch.FloatTensor(final_rankings).view(-1, 1)\n",
    "    adjacency_matrix = graph_to_adjacency_matrix(init_graph, init_weights, n)\n",
    "\n",
    "    print(\"Evaluating losses before optimization...\")\n",
    "    naive_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='naive')\n",
    "    simple_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='simple')\n",
    "    ratio_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='ratio')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Before Optimization:\")\n",
    "    print(f\"Naive Loss: {naive_loss_before}\")\n",
    "    print(f\"Simple Loss: {simple_loss_before}\")\n",
    "    print(f\"Ratio Loss: {ratio_loss_before}\")\n",
    "    print(f\"Elapsed Time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21703be-eeec-47c4-84fe-538f65ab3dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d6378-70ce-4715-b76d-743a87c1b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def berger_shor_fas(file_path):\n",
    "    \"\"\"\n",
    "    Implements the Berger-Shor algorithm for finding a feedback arc set (FAS).\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing the graph.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains the final DAG, removed edges, and related metrics.\n",
    "    \"\"\"\n",
    "    # Step 1: Read the graph\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    graph, weights = initialize_graph(edges)\n",
    "\n",
    "    # Step 2: Compute the feedback arc set\n",
    "    feedback_arc_set = []\n",
    "    removed_weights = {}\n",
    "\n",
    "    while graph:\n",
    "        min_node = None\n",
    "        min_weight = float('inf')\n",
    "\n",
    "        for node in list(graph.keys()):\n",
    "            in_edges = [(u, node, eid) for u in graph for v, eid in graph[u] if v == node]\n",
    "            out_edges = [(node, v, eid) for v, eid in graph[node]]\n",
    "\n",
    "            in_weight = sum(weights.get(edge, 0) for edge in in_edges)\n",
    "            out_weight = sum(weights.get(edge, 0) for edge in out_edges)\n",
    "\n",
    "            total_weight = in_weight + out_weight\n",
    "\n",
    "#            print(f\"Node: {node}, In-Weight: {in_weight}, Out-Weight: {out_weight}, Total Weight: {total_weight}\")\n",
    "\n",
    "            if total_weight < min_weight:\n",
    "                min_weight = total_weight\n",
    "                min_node = node\n",
    "\n",
    "#        print(f\"Selected Min Node: {min_node}, Min Weight: {min_weight}\")\n",
    "\n",
    "        if min_node is not None:\n",
    "            for u, v, eid in [(u, min_node, eid) for u in graph for v, eid in graph[u] if v == min_node]:\n",
    "                feedback_arc_set.append((u, v, eid))\n",
    "                removed_weights[(u, v, eid)] = weights.get((u, v, eid), 0)\n",
    "\n",
    "            graph.pop(min_node, None)\n",
    "\n",
    "            for u in list(graph.keys()):\n",
    "                graph[u] = [(v, eid) for v, eid in graph[u] if v != min_node]\n",
    "\n",
    "        if not graph:  # Ensure termination for edge cases\n",
    "            break\n",
    "\n",
    "    # Step 3: Construct the final graph\n",
    "    final_graph = defaultdict(list)\n",
    "    for u, adj in initialize_graph(edges)[0].items():\n",
    "        for v, eid in adj:\n",
    "            if (u, v, eid) not in feedback_arc_set:\n",
    "                final_graph[u].append((v, eid))\n",
    "\n",
    "#    print(\"Final Feedback Arc Set:\", feedback_arc_set)\n",
    "\n",
    "    # Validate the final graph is a DAG\n",
    "    if not is_dag(final_graph):\n",
    "        print(\"Error: The final graph is not a DAG!\")\n",
    "    else:\n",
    "        print(\"The final graph is a DAG.\")\n",
    "\n",
    "    # Print number of edges in the final graph\n",
    "    final_edge_count = sum(len(adj) for adj in final_graph.values())\n",
    "    print(f\"Number of edges in the final graph: {final_edge_count}\")\n",
    "\n",
    "    return {\n",
    "        \"final_graph\": final_graph,\n",
    "        \"removed_edges\": feedback_arc_set,\n",
    "        \"removed_weights\": removed_weights,\n",
    "        \"total_removed_weight\": sum(removed_weights.values()),\n",
    "    }\n",
    "\n",
    "def greedy_fas(file_path):\n",
    "    \"\"\"\n",
    "    Implements a greedy algorithm for finding a feedback arc set (FAS).\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing the graph.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains the final DAG, removed edges, and related metrics.\n",
    "    \"\"\"\n",
    "    # Step 1: Read the graph\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    graph, weights = initialize_graph(edges)\n",
    "\n",
    "    # Step 2: Compute the feedback arc set\n",
    "    feedback_arc_set = []\n",
    "    removed_weights = {}\n",
    "\n",
    "    while graph:\n",
    "        max_node = None\n",
    "        max_difference = float('-inf')\n",
    "\n",
    "        for node in list(graph.keys()):\n",
    "            in_edges = [(u, node, eid) for u in graph for v, eid in graph[u] if v == node]\n",
    "            out_edges = [(node, v, eid) for v, eid in graph[node]]\n",
    "\n",
    "            in_weight = sum(weights.get(edge, 0) for edge in in_edges)\n",
    "            out_weight = sum(weights.get(edge, 0) for edge in out_edges)\n",
    "\n",
    "            difference = out_weight - in_weight\n",
    "\n",
    "   #         print(f\"Node: {node}, In-Weight: {in_weight}, Out-Weight: {out_weight}, Difference: {difference}\")\n",
    "\n",
    "            if difference > max_difference:\n",
    "                max_difference = difference\n",
    "                max_node = node\n",
    "\n",
    "   #     print(f\"Selected Max Node: {max_node}, Max Difference: {max_difference}\")\n",
    "\n",
    "        if max_node is not None:\n",
    "            for u, v, eid in [(u, max_node, eid) for u in graph for v, eid in graph[u] if v == max_node]:\n",
    "                feedback_arc_set.append((u, v, eid))\n",
    "                removed_weights[(u, v, eid)] = weights.get((u, v, eid), 0)\n",
    "\n",
    "            graph.pop(max_node, None)\n",
    "\n",
    "            for u in list(graph.keys()):\n",
    "                graph[u] = [(v, eid) for v, eid in graph[u] if v != max_node]\n",
    "\n",
    "        if not graph:  # Ensure termination for edge cases\n",
    "            break\n",
    "\n",
    "    # Step 3: Construct the final graph\n",
    "    final_graph = defaultdict(list)\n",
    "    for u, adj in initialize_graph(edges)[0].items():\n",
    "        for v, eid in adj:\n",
    "            if (u, v, eid) not in feedback_arc_set:\n",
    "                final_graph[u].append((v, eid))\n",
    "\n",
    "#    print(\"Final Feedback Arc Set:\", feedback_arc_set)\n",
    "\n",
    "    # Validate the final graph is a DAG\n",
    "    if not is_dag(final_graph):\n",
    "        print(\"Error: The final graph is not a DAG!\")\n",
    "    else:\n",
    "        print(\"The final graph is a DAG.\")\n",
    "\n",
    "    # Print number of edges in the final graph\n",
    "    final_edge_count = sum(len(adj) for adj in final_graph.values())\n",
    "    print(f\"Number of edges in the final graph: {final_edge_count}\")\n",
    "\n",
    "    return {\n",
    "        \"final_graph\": final_graph,\n",
    "        \"removed_edges\": feedback_arc_set,\n",
    "        \"removed_weights\": removed_weights,\n",
    "        \"total_removed_weight\": sum(removed_weights.values()),\n",
    "    }\n",
    "\n",
    "def is_dag(graph):\n",
    "    \"\"\"\n",
    "    Checks if a graph is a Directed Acyclic Graph (DAG).\n",
    "\n",
    "    Args:\n",
    "        graph (dict): The graph represented as an adjacency list.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the graph is a DAG, False otherwise.\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    stack = set()\n",
    "\n",
    "    def dfs(node):\n",
    "        if node in stack:  # Cycle detected\n",
    "            return False\n",
    "        if node in visited:\n",
    "            return True\n",
    "        visited.add(node)\n",
    "        stack.add(node)\n",
    "        for neighbor, _ in graph.get(node, []):\n",
    "            if not dfs(neighbor):\n",
    "                return False\n",
    "        stack.remove(node)\n",
    "        return True\n",
    "\n",
    "    for node in graph:\n",
    "        if not dfs(node):\n",
    "            return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881443a-c02b-434f-800f-9e5d5d429355",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through each input file\n",
    "for file_path in file_paths:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Read the graph and initialize structures\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    print(f\"Processing file: {file_path} - Number of nodes: {n}, Number of edges: {len(edges)}\")\n",
    "\n",
    "    init_graph, init_weights = initialize_graph(edges)\n",
    "\n",
    "    # Step 2: Ensure the graph is a DAG by removing cycles\n",
    "    result = greedy_fas(file_path)\n",
    "    new_graph = result['final_graph']\n",
    "    new_weights = {key: value for key, value in init_weights.items() if key not in result['removed_weights']}\n",
    "\n",
    "    # Step 3: Compute rankings for the vertices using the modified graph\n",
    "    final_rankings = compute_vertex_rankings(new_graph, new_weights, n)\n",
    "    print(\"final ranking=\",final_rankings)\n",
    "    # Step 4: Evaluate upset losses before optimization\n",
    "    scores = torch.FloatTensor(final_rankings).view(-1, 1)\n",
    "    adjacency_matrix = graph_to_adjacency_matrix(init_graph, init_weights, n)\n",
    "\n",
    "    print(\"Evaluating losses before optimization...\")\n",
    "    naive_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='naive')\n",
    "    simple_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='simple')\n",
    "    ratio_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='ratio')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Before Optimization:\")\n",
    "    print(f\"Naive Loss: {naive_loss_before}\")\n",
    "    print(f\"Simple Loss: {simple_loss_before}\")\n",
    "    print(f\"Ratio Loss: {ratio_loss_before}\")\n",
    "    print(f\"Elapsed Time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f7154-8bb8-47f3-8326-ff52dadef477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d7d3d-dad1-4ae7-9f5d-ea138b6be78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28f460-56cc-401b-93a0-b8b795cc619a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44889cc-3b19-49b3-9ce3-08b7bd930827",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through each input file\n",
    "for file_path in file_paths:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Read the graph and initialize structures\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _, _ in edges) + 1\n",
    "    print(f\"Processing file: {file_path} - Number of nodes: {n}, Number of edges: {len(edges)}\")\n",
    "\n",
    "    init_graph, init_weights = initialize_graph(edges)\n",
    "\n",
    "    # Step 2: Ensure the graph is a DAG by removing cycles\n",
    "    result = berger_shor_fas(file_path)\n",
    "    new_graph = result['final_graph']\n",
    "    new_weights = {key: value for key, value in init_weights.items() if key not in result['removed_weights']}\n",
    "\n",
    "    # Step 3: Compute rankings for the vertices using the modified graph\n",
    "    final_rankings = compute_vertex_rankings(new_graph, new_weights, n)\n",
    "    print(\"final ranking=\",final_rankings)\n",
    "    # Step 4: Evaluate upset losses before optimization\n",
    "    scores = torch.FloatTensor(final_rankings).view(-1, 1)\n",
    "    adjacency_matrix = graph_to_adjacency_matrix(init_graph, init_weights, n)\n",
    "\n",
    "    print(\"Evaluating losses before optimization...\")\n",
    "    naive_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='naive')\n",
    "    simple_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='simple')\n",
    "    ratio_loss_before = calculate_upset_loss(adjacency_matrix, scores, style='ratio')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Before Optimization:\")\n",
    "    print(f\"Naive Loss: {naive_loss_before}\")\n",
    "    print(f\"Simple Loss: {simple_loss_before}\")\n",
    "    print(f\"Ratio Loss: {ratio_loss_before}\")\n",
    "    print(f\"Elapsed Time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782e6b5-522b-4aec-ac12-96aa6947e83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c737627-0f69-4afe-b779-6a187921b2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45c268-57f6-4a92-b3a4-b9bb20d30b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37111c44-1af3-4f85-a47a-91fb20d3c6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d3f9ab-d5a8-4a40-92b1-994c6a1ba41d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
