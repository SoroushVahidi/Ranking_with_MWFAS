{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60769118-7444-47ac-8af5-4dd96e7fbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_graph(file_path):\n",
    "    \"\"\"\n",
    "    Reads a weighted directed graph from a file. Each line contains three values:\n",
    "    start vertex, end vertex, and edge weight.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing the graph.\n",
    "\n",
    "    Returns:\n",
    "        edges (list): List of tuples representing directed edges (start, end, weight).\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line:  # Skip empty lines\n",
    "                    start, end, weight = map(float, line.split())\n",
    "                    edges.append((int(start), int(end), weight))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading graph: {e}\")\n",
    "    return edges\n",
    "\n",
    "def initialize_graph(edges):\n",
    "    \"\"\"Converts edge list to adjacency list and weights dictionary.\"\"\"\n",
    "    graph = defaultdict(list)\n",
    "    weights = {}\n",
    "    for u, v, w in edges:\n",
    "        graph[u].append(v)\n",
    "        weights[(u, v)] = w\n",
    "    return graph, weights\n",
    "\n",
    "def find_cycles_and_reduce(graph, weights, n):\n",
    "    \"\"\"Phase 1: Find cycles and reduce weights using a copy.\"\"\"\n",
    "    weights_copy = weights.copy()  # Work with a copy of weights\n",
    "    removed_edges = set()\n",
    "    removed_weights = {}\n",
    "\n",
    "    while True:\n",
    "        cycle = find_cycle(graph,  n)\n",
    "        if not cycle:  # No cycle found\n",
    "            break\n",
    "\n",
    "        # Ensure all edges in the cycle exist in the weights dictionary\n",
    "        cycle = [(u, v) for u, v in cycle if (u, v) in weights_copy]\n",
    "\n",
    "        if not cycle:  # If no valid cycle exists, continue\n",
    "            continue\n",
    "\n",
    "        min_weight = min(weights_copy[(u, v)] for u, v in cycle)\n",
    "\n",
    "        for u, v in cycle:\n",
    "            weights_copy[(u, v)] -= min_weight\n",
    "            if weights_copy[(u, v)] <= 0:\n",
    "                # Ensure the edge is in the graph before removing\n",
    "                if v in graph[u]:\n",
    "                    graph[u].remove(v)\n",
    "                    removed_edges.add((u, v))\n",
    "                    removed_weights[(u, v)] = weights[(u, v)]\n",
    "\n",
    "    return removed_edges, removed_weights\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def find_cycle(graph, n):\n",
    "    \"\"\"Detect a cycle in the graph using DFS and return the cycle as a list of edges.\"\"\"\n",
    "    visited = [False] * n\n",
    "    stack = [False] * n\n",
    "    parent = [-1] * n\n",
    "\n",
    "    def dfs(v):\n",
    "        visited[v] = True\n",
    "        stack[v] = True\n",
    "        for neighbor in graph[v]:\n",
    "            if not visited[neighbor]:\n",
    "                parent[neighbor] = v\n",
    "                cycle = dfs(neighbor)\n",
    "                if cycle:\n",
    "                    return cycle\n",
    "            elif stack[neighbor]:\n",
    "                # Found a cycle, reconstruct it\n",
    "                cycle = []\n",
    "                current = v\n",
    "                while current != neighbor:\n",
    "                    cycle.append((parent[current], current))\n",
    "                    current = parent[current]\n",
    "                cycle.append((parent[neighbor], neighbor))\n",
    "                return cycle\n",
    "        stack[v] = False\n",
    "        return None\n",
    "\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            cycle = dfs(i)\n",
    "            if cycle:\n",
    "                return cycle\n",
    "    return None\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def find_minimum_weight_cycle(graph, weights, n):\n",
    "    \"\"\"\n",
    "    Find the minimum weight cycle in the graph using Floyd-Warshall.\n",
    "\n",
    "    :param graph: Adjacency list representation of the graph.\n",
    "    :param weights: Dictionary of edge weights.\n",
    "    :param n: Total number of vertices in the graph.\n",
    "    :return: List of edges representing the minimum weight cycle.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize distance and predecessor matrices\n",
    "    dist = np.full((n, n), float('inf'))\n",
    "    pred = [[-1 for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "    # Fill in the distances based on edge weights\n",
    "    for u in range(n):\n",
    "        dist[u][u] = 0\n",
    "        if u in graph:\n",
    "            for v in graph[u]:\n",
    "                dist[u][v] = weights.get((u, v), float('inf'))\n",
    "                pred[u][v] = u\n",
    "\n",
    "    # Step 2: Run Floyd-Warshall algorithm\n",
    "    for k in range(n):\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if dist[i][j] > dist[i][k] + dist[k][j]:\n",
    "                    dist[i][j] = dist[i][k] + dist[k][j]\n",
    "                    pred[i][j] = pred[k][j]\n",
    "\n",
    "    # Step 3: Find the minimum weight cycle\n",
    "    min_cycle_weight = float('inf')\n",
    "    cycle = []\n",
    "\n",
    "    for u in range(n):\n",
    "        for v in range(n):\n",
    "            if u != v and dist[u][v] < float('inf') and dist[v][u] < float('inf'):\n",
    "                cycle_weight = dist[u][v] + dist[v][u]\n",
    "                if cycle_weight < min_cycle_weight:\n",
    "                    min_cycle_weight = cycle_weight\n",
    "                    # Reconstruct the cycle\n",
    "                    cycle = []\n",
    "                    # Trace path from u to v\n",
    "                    current = v\n",
    "                    while current != u:\n",
    "                        cycle.append((pred[u][current], current))\n",
    "                        current = pred[u][current]\n",
    "                    # Trace path from v back to u\n",
    "                    current = u\n",
    "                    while current != v:\n",
    "                        cycle.append((pred[v][current], current))\n",
    "                        current = pred[v][current]\n",
    "\n",
    "    # Return the minimum weight cycle if found\n",
    "    if min_cycle_weight == float('inf'):\n",
    "        return None\n",
    "    else:\n",
    "        return cycle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_and_readd_edges(graph, removed_edges, n):\n",
    "    \"\"\"Phase 2: Check and re-add edges if they do not create a cycle.\"\"\"\n",
    "    \n",
    "    def has_path(start, end, graph):\n",
    "        \"\"\"Helper function to check if there is a path from start to end using DFS.\"\"\"\n",
    "        visited = [False] * n\n",
    "        stack = [start]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            if node == end:\n",
    "                return True\n",
    "            if not visited[node]:\n",
    "                visited[node] = True\n",
    "                stack.extend(graph[node])\n",
    "        return False\n",
    "\n",
    "    readded_edges = set()\n",
    "    f = sorted(list(removed_edges), reverse=True)\n",
    "\n",
    "    for u, v in f:\n",
    "        if not has_path(v, u, graph):  # Only re-add if it doesn't create a cycle\n",
    "            graph[u].append(v)\n",
    "            readded_edges.add((u, v))\n",
    "\n",
    "    return removed_edges - readded_edges\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "        self.rank = [0] * n\n",
    "\n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])  # Path compression\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y):\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "        if root_x != root_y:\n",
    "            if self.rank[root_x] > self.rank[root_y]:\n",
    "                self.parent[root_y] = root_x\n",
    "            elif self.rank[root_x] < self.rank[root_y]:\n",
    "                self.parent[root_y] = root_y\n",
    "            else:\n",
    "                self.parent[root_y] = root_x\n",
    "                self.rank[root_x] += 1\n",
    "\n",
    "def mwfas(file_path):\n",
    "    \"\"\"\n",
    "    Main function to find Minimum Weighted Feedback Arc Set (MWFAS).\n",
    "    :param file_path: Path to the file containing the graph.\n",
    "    :return: A dictionary with metrics, updated graph, removed edges, and their weights.\n",
    "    \"\"\"\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _ in edges) + 1\n",
    "    graph, weights = initialize_graph(edges)\n",
    "\n",
    "    # Original graph statistics\n",
    "    total_edges = len(edges)\n",
    "    total_weight = sum(w for _, _, w in edges)\n",
    "\n",
    "    # Phase 1: Reduce cycles\n",
    "  #  print(\"Before Phase 1:\")\n",
    "  #  print(\"Graph:\", graph)\n",
    "  #  print(\"Weights:\", weights)\n",
    "    \n",
    "    removed_edges, removed_weights = find_cycles_and_reduce(graph, weights, n)\n",
    "    \n",
    "   # print(\"After Phase 1:\")\n",
    "   # print(\"Removed edges:\", removed_edges)\n",
    "   # print(\"Removed weights:\", removed_weights)\n",
    "\n",
    "    # Phase 2: Re-add edges (if applicable)\n",
    "    removed_edges = check_and_readd_edges(graph, removed_edges, n)\n",
    "\n",
    "    # Compute final metrics\n",
    "    num_removed_edges = len(removed_edges)\n",
    "    total_removed_weight = sum(removed_weights.get(edge, 0) for edge in removed_edges)\n",
    "\n",
    "#    print(\"Final Results:\")\n",
    "#    print(\"Removed edges:\", removed_edges)\n",
    "#    print(\"Weights of removed edges:\", {edge: removed_weights.get(edge, 0) for edge in removed_edges})\n",
    "#    print(\"Final graph:\", graph)\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        \"total_edges\": total_edges,\n",
    "        \"total_weight\": total_weight,\n",
    "        \"num_removed_edges\": num_removed_edges,\n",
    "        \"removed_weight\": total_removed_weight,\n",
    "        \"final_graph\": graph,\n",
    "        \"removed_edges\": removed_edges,\n",
    "        \"removed_weights\": {edge: removed_weights.get(edge, 0) for edge in removed_edges},\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5efbb96d-66cb-4ff9-a7d9-6e46488f403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After feedback arc set removal\n",
    "def compute_vertex_rankings(graph, weights, n):\n",
    " #   print(\"in the input dag to topol sort \", \"graph is \",graph, \"and weights are \",weights)\n",
    "    \"\"\"\n",
    "    Compute rankings for the vertices in a DAG.\n",
    "    :param graph: Adjacency list of the DAG.\n",
    "    :param weights: Dictionary of edge weights.\n",
    "    :param n: Total number of vertices in the graph.\n",
    "    :return: A list of rankings for the vertices.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate in-degrees\n",
    "    in_degree = [0] * n\n",
    "    for u in graph:\n",
    "        for v in graph[u]:\n",
    "            in_degree[v] += 1\n",
    "\n",
    "    # Step 2: Perform topological sort using a min-heap\n",
    "    from heapq import heappop, heappush\n",
    "    min_heap = []\n",
    "    for i in range(n):\n",
    "        if in_degree[i] == 0:\n",
    "            heappush(min_heap, i)\n",
    "\n",
    "    topological_order = []\n",
    "    while min_heap:\n",
    "        current = heappop(min_heap)\n",
    "        topological_order.append(current)\n",
    "        for neighbor in graph[current]:\n",
    "            in_degree[neighbor] -= 1\n",
    "            if in_degree[neighbor] == 0:\n",
    "                heappush(min_heap, neighbor)\n",
    "\n",
    "    # Step 3: Calculate outgoing edge weight sums for all vertices\n",
    "    outgoing_weights = {v: 0 for v in range(n)}\n",
    "    incoming_weights = {v: 0 for v in range(n)}\n",
    "\n",
    "    for u in graph:\n",
    "        for v in graph[u]:\n",
    "            outgoing_weights[u] += weights.get((u, v), 0)\n",
    "            incoming_weights[v] += weights.get((u, v), 0)\n",
    "\n",
    "\n",
    "    # Step 4: Assign rankings\n",
    "    rankings = [-1] * n\n",
    "    current_rank = 0\n",
    "    for vertex in topological_order:\n",
    "        rankings[vertex] = current_rank\n",
    "        current_rank += 1\n",
    "\n",
    "    # Break ties for vertices with the same ranking based on outgoing edge weights\n",
    "    tied_vertices = sorted(\n",
    "        [(rankings[v], -(outgoing_weights[v] - incoming_weights[v]) / \n",
    "          (outgoing_weights[v] + incoming_weights[v] if outgoing_weights[v] + incoming_weights[v] > 0 else 1), v)\n",
    "         for v in range(n)],\n",
    "        key=lambda x: (x[0], x[1])\n",
    "    )\n",
    "\n",
    "    scores = [0] * n\n",
    "    for final_rank, (_, _, vertex) in enumerate(tied_vertices):\n",
    "        scores[vertex]=-final_rank\n",
    "\n",
    "    print(scores)\n",
    "    return scores\n",
    "\n",
    "# Ensure graph is a DAG after feedback arc set removal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0465ba07-e038-43fc-8b77-f7d5f98b2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_updated_scores(adjacency_matrix, scores, epsilon=1e-8):\n",
    "    import cvxpy as cp\n",
    "    import numpy as np\n",
    "\n",
    "    n = len(scores)\n",
    "\n",
    "    # Convert inputs to numpy\n",
    "    adjacency_matrix_np = adjacency_matrix.numpy()\n",
    "    scores_np = scores.numpy().flatten()\n",
    "\n",
    "    # Define optimization variables\n",
    "    updated_scores = cp.Variable(n)\n",
    "    R = cp.Variable((n, n))  # Auxiliary variable for ratios\n",
    "\n",
    "    # Compute M (skew-symmetric pairwise matrix)\n",
    "    M = adjacency_matrix_np - adjacency_matrix_np.T\n",
    "\n",
    "    # Edge mask\n",
    "    edge_mask = (adjacency_matrix_np + adjacency_matrix_np.T > 0).astype(float)\n",
    "\n",
    "    # Objective: Minimize squared difference between M and R\n",
    "    objective = cp.sum_squares(cp.multiply(edge_mask, M - R))\n",
    "\n",
    "    # Constraints for R_ij\n",
    "    constraints = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if edge_mask[i, j] > 0:\n",
    "                constraints.append(R[i, j] * (updated_scores[i] + updated_scores[j] + epsilon) == updated_scores[i] - updated_scores[j])\n",
    "\n",
    "    # Order-preserving constraints\n",
    "    constraints += [\n",
    "        updated_scores[i] <= updated_scores[j]\n",
    "        for i in range(n)\n",
    "        for j in range(n)\n",
    "        if scores_np[i] <= scores_np[j]\n",
    "    ]\n",
    "\n",
    "    # Solve the problem\n",
    "    problem = cp.Problem(cp.Minimize(objective), constraints)\n",
    "    problem.solve()\n",
    "\n",
    "    return updated_scores.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "970aa5dc-cb8b-4d04-a90a-973d74396090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_adjacency_matrix(graph, weights, n):\n",
    "    \"\"\"\n",
    "    Converts a graph represented as an adjacency list with weights to an adjacency matrix.\n",
    "\n",
    "    :param graph: Adjacency list of the graph.\n",
    "    :param weights: Dictionary of edge weights where the key is a tuple (u, v) and the value is the weight.\n",
    "    :param n: Number of vertices in the graph.\n",
    "    :return: A torch.FloatTensor adjacency matrix with weights.\n",
    "    \"\"\"\n",
    "    adjacency_matrix = torch.zeros((n, n))\n",
    "    for u in graph:\n",
    "        for v in graph[u]:\n",
    "            adjacency_matrix[u, v] = weights[(u, v)] # Weighted edge from u to v\n",
    "  #  print(\"adjacency matrix= \",adjacency_matrix)\n",
    " #   print(adjacency_matrix)\n",
    "    return adjacency_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e70cf-ef80-4e9a-adb4-39e31e3e1b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09a3b680-b16e-4ab5-a9b7-7e2faa7766ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def reorder_floats(x):\n",
    "    n = len(x)\n",
    "    random_floats = np.random.uniform(0,  2*n/3, n)\n",
    "    y = np.zeros(n)\n",
    "    for idx, val in enumerate(np.argsort(x)):\n",
    "        y[val] = sorted(random_floats)[idx]\n",
    "    return y\n",
    "\n",
    "def calculate_upset_loss(adjacency_matrix, scores, style='ratio', margin=0.01):\n",
    "    \"\"\"\n",
    "    Calculate the upset loss for the graph rankings using adjacency matrix and scores.\n",
    "\n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
    "    :param style: Type of upset loss ('naive', 'simple', 'ratio', or 'margin').\n",
    "    :param margin: Margin for margin loss (default: 0.01).\n",
    "    :return: Torch FloatTensor upset loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-8  # For numerical stability\n",
    "\n",
    "    # Ensure scores are 2D\n",
    "    if scores.ndim == 1:\n",
    "        scores = scores.view(-1, 1)\n",
    "\n",
    "    # Skew-symmetric pairwise comparison matrix (M)\n",
    "    M1 = adjacency_matrix - adjacency_matrix.T\n",
    "\n",
    "    # Normalize scores to [0, 1] range\n",
    "    normalized_scores = scores\n",
    "\n",
    "    # Pairwise score differences (T)\n",
    "    T1 = normalized_scores - normalized_scores.T\n",
    "\n",
    "    # Edge mask: Only consider meaningful edges (where M != 0)\n",
    "    edge_mask = M1 != 0\n",
    "\n",
    "    if style == 'ratio':\n",
    "        min_upset = float('inf')  # Initialize with a large value\n",
    "        \n",
    "        for _ in range(40):\n",
    "            # Generate reordered scores using reorder_floats\n",
    "            if _==0:\n",
    "                reordered_scores=scores\n",
    "            else:\n",
    "                reordered_scores = torch.FloatTensor(reorder_floats(scores.flatten().tolist()))\n",
    "            reordered_scores = reordered_scores.view(-1, 1)\n",
    "\n",
    "            # Compute T2 for normalized scores\n",
    "            T2 = reordered_scores + reordered_scores.T + epsilon\n",
    "            T = torch.div(T1, T2)\n",
    "            M2 = adjacency_matrix + adjacency_matrix.T + epsilon\n",
    "            M3 = torch.div(M1, M2)  # Normalize the adjacency matrix\n",
    "            \n",
    "            # Compute ratio-based upset loss for this iteration\n",
    "            powers = torch.pow((M3 - T)[edge_mask], 2)\n",
    "            upset_loss = torch.sum(powers) / torch.sum(edge_mask)\n",
    "\n",
    "            # Track the minimum upset loss\n",
    "            min_upset = min(min_upset, upset_loss.item())\n",
    "        \n",
    "        return torch.tensor(min_upset)\n",
    "\n",
    "    elif style == 'naive':\n",
    "        upset = torch.sum(torch.sign(T1[edge_mask]) != torch.sign(M1[edge_mask])) / torch.sum(edge_mask)\n",
    "\n",
    "    elif style == 'simple':\n",
    "        upset = torch.mean((torch.sign(T1[edge_mask]) - torch.sign(M1[edge_mask]))**2)\n",
    "\n",
    "    elif style == 'margin':\n",
    "        upset = torch.mean(torch.nn.functional.relu(-M1[edge_mask] * (T1[edge_mask] - margin)))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported style: {style}\")\n",
    "\n",
    "    return upset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfb48e27-720b-4562-9a4b-0c7b314f97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_ratio_upset_loss(adjacency_matrix, scores, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Compute the ratio upset loss for the graph rankings using adjacency matrix and scores.\n",
    "\n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param scores: Torch FloatTensor ranking scores (n x 1).\n",
    "    :param epsilon: Small value for numerical stability (default: 1e-8).\n",
    "    :return: Torch FloatTensor ratio upset loss value.\n",
    "    \"\"\"\n",
    "    # Ensure scores are 2D\n",
    "    if scores.ndim == 1:\n",
    "        scores = scores.view(-1, 1)\n",
    "\n",
    "    # Skew-symmetric pairwise comparison matrix (M)\n",
    "    M1 = adjacency_matrix - adjacency_matrix.T\n",
    "\n",
    "    # Pairwise score differences (T1)\n",
    "    T1 = scores - scores.T\n",
    "\n",
    "    # Edge mask: Only consider meaningful edges (where M1 != 0)\n",
    "    edge_mask = M1 != 0\n",
    "\n",
    "    # Compute T2 for normalized scores\n",
    "    T2 = scores + scores.T + epsilon\n",
    "    T = torch.div(T1, T2)\n",
    "\n",
    "    # Normalize M1 using adjacency matrix\n",
    "    M2 = adjacency_matrix + adjacency_matrix.T + epsilon\n",
    "    M3 = torch.div(M1, M2)  # Normalize the adjacency matrix\n",
    "\n",
    "    # Compute ratio upset loss\n",
    "    powers = torch.pow((M3 - T)[edge_mask], 2)\n",
    "    upset_loss = torch.sum(powers) / torch.sum(edge_mask)\n",
    "\n",
    "    return upset_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27b336e9-7412-4d1a-8a2f-8fa7f37d5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "def minimize_ratio_upset_loss(adjacency_matrix, scores, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Find the scores that minimize the ratio upset loss for a given adjacency matrix.\n",
    "\n",
    "    :param adjacency_matrix: Torch FloatTensor adjacency matrix (n x n).\n",
    "    :param epsilon: Small value for numerical stability (default: 1e-8).\n",
    "    :return: Tuple (optimal_scores, minimized_loss)\n",
    "    \"\"\"\n",
    "    n = adjacency_matrix.shape[0]\n",
    "\n",
    "    # Objective function to minimize\n",
    "    def objective_function(scores):\n",
    "        # Convert scores to Torch FloatTensor\n",
    "        scores = torch.tensor(scores, dtype=torch.float32)\n",
    "        # Compute ratio upset loss\n",
    "        return compute_ratio_upset_loss(adjacency_matrix, scores, epsilon)\n",
    "\n",
    "    # Initial guess for scores (uniform values)\n",
    "    initial_guess = scores\n",
    "\n",
    "    # Bounds to ensure scores remain positive\n",
    "    bounds = [(1e-6, None)] * n  # Lower bound is 1e-6 to avoid division by zero\n",
    "\n",
    "    # Minimize the objective function\n",
    "    result = differential_evolution(\n",
    "    objective_function,\n",
    "    bounds=bounds,\n",
    "    strategy='best1bin',\n",
    "    maxiter=1000,\n",
    "    tol=1e-9\n",
    ")\n",
    "\n",
    "    if not result.success:\n",
    "        raise ValueError(f\"Optimization failed: {result.message}\")\n",
    "\n",
    "    # Optimal scores and minimized loss\n",
    "    optimal_scores = result.x\n",
    "    minimized_loss = result.fun\n",
    "\n",
    "    return optimal_scores, minimized_loss\n",
    "\n",
    "# Example usage\n",
    "# adjacency_matrix = torch.FloatTensor([\n",
    "#     [0, 1, 1, 0],\n",
    "#     [1, 0, 1, 1],\n",
    "#     [1, 1, 0, 1],\n",
    "#     [0, 1, 1, 0]\n",
    "# ])\n",
    "\n",
    "# optimal_scores, minimized_loss = minimize_ratio_upset_loss(adjacency_matrix)\n",
    "# print(\"Optimal Scores:\", optimal_scores)\n",
    "# print(\"Minimized Ratio Upset Loss:\", minimized_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2567d-9114-4688-98f5-85c6b93a1527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68495d94-ad4d-43b2-9333-c84dc62fd752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c6b21d9-6337-4113-8cdf-bdc54cd095c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_upset_losses(file_path, rankings):\n",
    "    \"\"\"\n",
    "    Evaluate upset losses (naive, simple, ratio, margin) for a graph and given rankings.\n",
    "\n",
    "    :param file_path: Path to the graph file.\n",
    "    :param rankings: List of rankings for the vertices.\n",
    "    \"\"\"\n",
    "    # Step 1: Prepare Graph and Adjacency Matrix\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _ in edges) + 1\n",
    "    graph, weights = initialize_graph(edges)\n",
    "    adjacency_matrix = graph_to_adjacency_matrix(graph, weights, n)\n",
    "\n",
    "    # Step 2: Convert Rankings to Scores Tensor\n",
    "    scores = torch.FloatTensor(rankings).view(-1, 1)\n",
    "\n",
    "    # Step 3: Calculate Upset Losses\n",
    "    naive_loss = calculate_upset_loss(adjacency_matrix, scores, style='naive').item()\n",
    "    simple_loss = calculate_upset_loss(adjacency_matrix, scores, style='simple').item()\n",
    "    ratio_loss = calculate_upset_loss(adjacency_matrix, scores, style='ratio').item()\n",
    "    margin_loss = calculate_upset_loss(adjacency_matrix, scores, style='margin').item()\n",
    "\n",
    "    # Step 4: Print Results\n",
    "    print(\"Upset Losses for the Graph Rankings:\")\n",
    "    print(f\"Naive Upset Loss: {naive_loss:.4f}\")\n",
    "    print(f\"Simple Upset Loss: {simple_loss:.4f}\")\n",
    "    print(f\"Differentiable Upset Loss (Ratio): {ratio_loss:.4f}\")\n",
    "    print(f\"Upset Margin Loss: {margin_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e96b02a-635c-40c9-9e43-26cb60a76680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 1994adj.txt - Number of nodes: 301, Number of edges: 3144\n",
      "[-60, -97, -184, -26, -41, -27, 0, -201, -273, -24, -1, -19, -271, -248, -108, -243, -226, -65, -299, -121, -185, -63, -133, -113, -5, -52, -148, -269, -118, -160, -109, -31, -190, -83, -112, -2, -38, -32, -59, -151, -56, -181, -163, -149, -45, -78, -40, -3, -101, -93, -265, -15, -223, -285, -170, -48, -176, -283, -263, -241, -39, -95, -229, -272, -68, -25, -128, -164, -136, -57, -77, -46, -114, -111, -188, -53, -13, -123, -99, -12, -6, -250, -140, -240, -98, -30, -82, -161, -153, -80, -150, -107, -47, -49, -130, -4, -7, -138, -100, -67, -8, -175, -35, -192, -200, -252, -85, -55, -84, -139, -74, -152, -116, -274, -66, -54, -137, -132, -86, -179, -205, -76, -159, -58, -105, -242, -22, -9, -258, -194, -228, -64, -10, -28, -61, -122, -262, -251, -62, -36, -102, -33, -11, -69, -220, -154, -75, -50, -42, -79, -18, -14, -212, -43, -141, -203, -214, -115, -117, -17, -16, -253, -177, -234, -70, -182, -279, -120, -224, -197, -71, -20, -247, -286, -208, -275, -21, -124, -165, -94, -239, -147, -29, -266, -51, -202, -110, -183, -23, -217, -207, -287, -178, -215, -44, -103, -72, -225, -162, -186, -204, -90, -104, -237, -155, -119, -92, -144, -198, -34, -142, -106, -166, -88, -96, -298, -37, -73, -290, -284, -222, -145, -218, -199, -171, -246, -209, -81, -87, -210, -143, -156, -282, -281, -216, -254, -125, -233, -126, -227, -193, -268, -206, -134, -89, -91, -211, -213, -187, -180, -167, -189, -195, -264, -221, -131, -256, -230, -255, -174, -157, -260, -127, -259, -196, -168, -158, -135, -276, -238, -191, -257, -245, -129, -249, -146, -267, -169, -261, -300, -244, -270, -277, -219, -235, -278, -172, -280, -231, -296, -232, -291, -288, -173, -236, -289, -292, -293, -294, -295, -297]\n",
      "naive loss= tensor(0.1345)\n",
      "simplee loss= tensor(0.5380)\n",
      "ratio loss= tensor(0.5212)\n",
      "elapsed time= 1.2568340301513672\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# List of input files\n",
    "file_paths = [\"1994adj.txt\"]\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Iterate through each input file\n",
    "for file_path in file_paths:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Read the graph and initialize structures\n",
    "    edges = read_graph(file_path)\n",
    "    n = max(max(u, v) for u, v, _ in edges) + 1\n",
    "    print(f\"Processing file: {file_path} - Number of nodes: {n}, Number of edges: {len(edges)}\")\n",
    "\n",
    "    init_graph, init_weights = initialize_graph(edges)\n",
    "\n",
    "    # Step 2: Ensure the graph is a DAG by removing cycles\n",
    "    result = mwfas(file_path)\n",
    "    new_graph = result['final_graph']\n",
    "    new_weights = {key: value for key, value in init_weights.items() if key not in result['removed_weights']}\n",
    "\n",
    "    # Step 3: Compute rankings for the vertices using the modified graph\n",
    "    final_rankings = compute_vertex_rankings(new_graph, new_weights, n)\n",
    "\n",
    "    # Step 4: Evaluate upset losses\n",
    "    scores = torch.FloatTensor(final_rankings).view(-1, 1)\n",
    "    adjacency_matrix = graph_to_adjacency_matrix(init_graph, init_weights, n)\n",
    "\n",
    "    naive_loss = calculate_upset_loss(adjacency_matrix, scores, style='naive')  # No .item()\n",
    "    simple_loss = calculate_upset_loss(adjacency_matrix, scores, style='simple')  # No .item()\n",
    "    ratio_loss = calculate_upset_loss(adjacency_matrix, scores, style='ratio')  # No .item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"naive loss=\",naive_loss)\n",
    "    print(\"simplee loss=\",simple_loss)\n",
    "    print(\"ratio loss=\",ratio_loss)\n",
    "    print(\"elapsed time=\",elapsed_time)\n",
    "\n",
    "    # Store results in a list\n",
    "    # results.append({\n",
    "    #     \"File\": \"Basketball_\"+str(file_path[:9]),\n",
    "    #     \"Nodes\": n,\n",
    "    #     \"Edges\": len(edges),\n",
    "    #     \"Naive Loss\": round(naive_loss.item(),2),\n",
    "    #     \"Simple Loss\": round(simple_loss.item(),2),\n",
    "    #     \"Ratio Loss\": round(ratio_loss.item(),2),\n",
    "    #     \"Elapsed Time (s)\": round(elapsed_time,2)\n",
    "    # })\n",
    "    # print\n",
    "    # print(\"simple loss=\",simple_loss.item())\n",
    "\n",
    "# Write results to an Excel file\n",
    "#output_file = \"graph_analysis_results.xlsx\"\n",
    "#df = pd.DataFrame(results)\n",
    "#df.to_excel(output_file, index=False)\n",
    "\n",
    "#print(f\"Results written to {output_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d9e60-8023-40bb-9071-14ca23f5ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd692d53-f974-4f68-b104-ea89317cfa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21703be-eeec-47c4-84fe-538f65ab3dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
